2024-12-03 22:19:59,195 - extractor - INFO - ---------Processing feature 4/815---------
2024-12-03 22:20:33,132 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个 linux 内核的专家。给你一个 linux 内核特性的描述以及其对应的若干 commit 的标题和提交信息，你的任务是为这个特性写一个多方面的扩写版描述。你的扩写需包含以下几个方面：1. 该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)；2. 为了让开发者理解该特性，需要哪些具体的，与这个特性直接相关的背景知识 (background_knowledge)；3. 该特性被实现前后，linux 内核发生了怎样的变化 (kernel_changes)；4. 你对该特性的综合理解 (comprehensive_understanding)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}]
        },
        "output": {
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
        }
    ],
    "input": {
        "feature_description": "parallelize hugetlb page init on boot",
        "commits": [{"commit_subject": "hugetlb: have CONFIG_HUGETLBFS select CONFIG_PADATA", "commit_message": "Allow hugetlb use padata_do_multithreaded for parallel initialization. \nSelect CONFIG_PADATA in this case.\n\nLink: https://lkml.kernel.org/r/20240222140422.393911-7-gang.li@linux.dev\nSigned-off-by: Gang Li <ligang.bdlg@bytedance.com>\nTested-by: David Rientjes <rientjes@google.com>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nTested-by: Paul E. McKenney <paulmck@kernel.org>\nAcked-by: Daniel Jordan <daniel.m.jordan@oracle.com>\nCc: Alexey Dobriyan <adobriyan@gmail.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Jane Chu <jane.chu@oracle.com>\nCc: Mike Kravetz <mike.kravetz@oracle.com>\nCc: Randy Dunlap <rdunlap@infradead.org>\nCc: Steffen Klassert <steffen.klassert@secunet.com>\nCc: Tim Chen <tim.c.chen@linux.intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "hugetlb: parallelize 2M hugetlb allocation and initialization", "commit_message": "By distributing both the allocation and the initialization tasks across\nmultiple threads, the initialization of 2M hugetlb will be faster, thereby\nimproving the boot speed.\n\nHere are some test results:\n      test case        no patch(ms)   patched(ms)   saved\n ------------------- -------------- ------------- --------\n  256c2T(4 node) 2M           3336          1051   68.52%\n  128c1T(2 node) 2M           1943           716   63.15%\n\nLink: https://lkml.kernel.org/r/20240222140422.393911-8-gang.li@linux.dev\nSigned-off-by: Gang Li <ligang.bdlg@bytedance.com>\nTested-by: David Rientjes <rientjes@google.com>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nCc: Alexey Dobriyan <adobriyan@gmail.com>\nCc: Daniel Jordan <daniel.m.jordan@oracle.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Jane Chu <jane.chu@oracle.com>\nCc: Mike Kravetz <mike.kravetz@oracle.com>\nCc: Paul E. McKenney <paulmck@kernel.org>\nCc: Randy Dunlap <rdunlap@infradead.org>\nCc: Steffen Klassert <steffen.klassert@secunet.com>\nCc: Tim Chen <tim.c.chen@linux.intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "hugetlb: split hugetlb_hstate_alloc_pages", "commit_message": "1G and 2M huge pages have different allocation and initialization logic,\nwhich leads to subtle differences in parallelization.  Therefore, it is\nappropriate to split hugetlb_hstate_alloc_pages into gigantic and\nnon-gigantic.\n\nThis patch has no functional changes.\n\nLink: https://lkml.kernel.org/r/20240222140422.393911-3-gang.li@linux.dev\nSigned-off-by: Gang Li <ligang.bdlg@bytedance.com>\nTested-by: David Rientjes <rientjes@google.com>\nReviewed-by: Tim Chen <tim.c.chen@linux.intel.com>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nCc: Alexey Dobriyan <adobriyan@gmail.com>\nCc: Daniel Jordan <daniel.m.jordan@oracle.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Jane Chu <jane.chu@oracle.com>\nCc: Mike Kravetz <mike.kravetz@oracle.com>\nCc: Paul E. McKenney <paulmck@kernel.org>\nCc: Randy Dunlap <rdunlap@infradead.org>\nCc: Steffen Klassert <steffen.klassert@secunet.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "hugetlb: pass *next_nid_to_alloc directly to for_each_node_mask_to_alloc", "commit_message": "With parallelization of hugetlb allocation across different threads, each\nthread works on a differnet node to allocate pages from, instead of all\nallocating from a common node h->next_nid_to_alloc.  To address this, it's\nnecessary to assign a separate next_nid_to_alloc for each thread.\n\nConsequently, the hstate_next_node_to_alloc and\nfor_each_node_mask_to_alloc have been modified to directly accept a\n*next_nid_to_alloc parameter, ensuring thread-specific allocation and\navoiding concurrent access issues.\n\nLink: https://lkml.kernel.org/r/20240222140422.393911-4-gang.li@linux.dev\nSigned-off-by: Gang Li <ligang.bdlg@bytedance.com>\nTested-by: David Rientjes <rientjes@google.com>\nReviewed-by: Tim Chen <tim.c.chen@linux.intel.com>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nCc: Alexey Dobriyan <adobriyan@gmail.com>\nCc: Daniel Jordan <daniel.m.jordan@oracle.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Jane Chu <jane.chu@oracle.com>\nCc: Mike Kravetz <mike.kravetz@oracle.com>\nCc: Paul E. McKenney <paulmck@kernel.org>\nCc: Randy Dunlap <rdunlap@infradead.org>\nCc: Steffen Klassert <steffen.klassert@secunet.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "hugetlb: code clean for hugetlb_hstate_alloc_pages", "commit_message": "Patch series \"hugetlb: parallelize hugetlb page init on boot\", v6.\n\nIntroduction\n------------\nHugetlb initialization during boot takes up a considerable amount of time.\nFor instance, on a 2TB system, initializing 1,800 1GB huge pages takes\n1-2 seconds out of 10 seconds.  Initializing 11,776 1GB pages on a 12TB\nIntel host takes more than 1 minute[1].  This is a noteworthy figure.\n\nInspired by [2] and [3], hugetlb initialization can also be accelerated\nthrough parallelization.  Kernel already has infrastructure like\npadata_do_multithreaded, this patch uses it to achieve effective results\nby minimal modifications.\n\n[1] https://lore.kernel.org/all/783f8bac-55b8-5b95-eb6a-11a583675000@google.com/\n[2] https://lore.kernel.org/all/20200527173608.2885243-1-daniel.m.jordan@oracle.com/\n[3] https://lore.kernel.org/all/20230906112605.2286994-1-usama.arif@bytedance.com/\n[4] https://lore.kernel.org/all/76becfc1-e609-e3e8-2966-4053143170b6@google.com/\n\nmax_threads\n-----------\nThis patch use `padata_do_multithreaded` like this:\n\n```\njob.max_threads\t= num_node_state(N_MEMORY) * multiplier;\npadata_do_multithreaded(&job);\n```\n\nTo fully utilize the CPU, the number of parallel threads needs to be\ncarefully considered.  `max_threads = num_node_state(N_MEMORY)` does not\nfully utilize the CPU, so we need to multiply it by a multiplier.\n\nTests below indicate that a multiplier of 2 significantly improves\nperformance, and although larger values also provide improvements, the\ngains are marginal.\n\n  multiplier     1       2       3       4       5\n ------------ ------- ------- ------- ------- -------\n  256G 2node   358ms   215ms   157ms   134ms   126ms\n  2T   4node   979ms   679ms   543ms   489ms   481ms\n  50G  2node   71ms    44ms    37ms    30ms    31ms\n\nTherefore, choosing 2 as the multiplier strikes a good balance between\nenhancing parallel processing capabilities and maintaining efficient\nresource management.\n\nTest result\n-----------\n      test case       no patch(ms)   patched(ms)   saved\n ------------------- -------------- ------------- --------\n  256c2T(4 node) 1G           4745          2024   57.34%\n  128c1T(2 node) 1G           3358          1712   49.02%\n     12T         1G          77000         18300   76.23%\n\n  256c2T(4 node) 2M           3336          1051   68.52%\n  128c1T(2 node) 2M           1943           716   63.15%\n\n\nThis patch (of 8):\n\nThe readability of `hugetlb_hstate_alloc_pages` is poor.  By cleaning the\ncode, its readability can be improved, facilitating future modifications.\n\nThis patch extracts two functions to reduce the complexity of\n`hugetlb_hstate_alloc_pages` and has no functional changes.\n\n- hugetlb_hstate_alloc_pages_node_specific() to handle iterates through\n  each online node and performs allocation if necessary.\n- hugetlb_hstate_alloc_pages_report() report error during allocation.\n  And the value of h->max_huge_pages is updated accordingly.\n\nLink: https://lkml.kernel.org/r/20240222140422.393911-1-gang.li@linux.dev\nLink: https://lkml.kernel.org/r/20240222140422.393911-2-gang.li@linux.dev\nSigned-off-by: Gang Li <ligang.bdlg@bytedance.com>\nTested-by: David Rientjes <rientjes@google.com>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nReviewed-by: Tim Chen <tim.c.chen@linux.intel.com>\nCc: Daniel Jordan <daniel.m.jordan@oracle.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Jane Chu <jane.chu@oracle.com>\nCc: Paul E. McKenney <paulmck@kernel.org>\nCc: Randy Dunlap <rdunlap@infradead.org>\nCc: Steffen Klassert <steffen.klassert@secunet.com>\nCc: Alexey Dobriyan <adobriyan@gmail.com>\nCc: Mike Kravetz <mike.kravetz@oracle.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "hugetlb: parallelize 1G hugetlb initialization", "commit_message": "Optimizing the initialization speed of 1G huge pages through\nparallelization.\n\n1G hugetlbs are allocated from bootmem, a process that is already very\nfast and does not currently require optimization.  Therefore, we focus on\nparallelizing only the initialization phase in `gather_bootmem_prealloc`.\n\nHere are some test results:\n      test case       no patch(ms)   patched(ms)   saved\n ------------------- -------------- ------------- --------\n  256c2T(4 node) 1G           4745          2024   57.34%\n  128c1T(2 node) 1G           3358          1712   49.02%\n     12T         1G          77000         18300   76.23%\n\n[akpm@linux-foundation.org: s/initialied/initialized/, per Alexey]\nLink: https://lkml.kernel.org/r/20240222140422.393911-9-gang.li@linux.dev\nSigned-off-by: Gang Li <ligang.bdlg@bytedance.com>\nTested-by: David Rientjes <rientjes@google.com>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nCc: Alexey Dobriyan <adobriyan@gmail.com>\nCc: Daniel Jordan <daniel.m.jordan@oracle.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Jane Chu <jane.chu@oracle.com>\nCc: Mike Kravetz <mike.kravetz@oracle.com>\nCc: Paul E. McKenney <paulmck@kernel.org>\nCc: Randy Dunlap <rdunlap@infradead.org>\nCc: Steffen Klassert <steffen.klassert@secunet.com>\nCc: Tim Chen <tim.c.chen@linux.intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "padata: downgrade padata_do_multithreaded to serial execution for non-SMP", "commit_message": "hugetlb parallelization depends on PADATA, and PADATA depends on SMP.\n\nPADATA consists of two distinct functionality: One part is\npadata_do_multithreaded which disregards order and simply divides tasks\ninto several groups for parallel execution.  Hugetlb init parallelization\ndepends on padata_do_multithreaded.\n\nThe other part is composed of a set of APIs that, while handling data in\nan out-of-order parallel manner, can eventually return the data with\nordered sequence.  Currently Only `crypto/pcrypt.c` use them.\n\nAll users of PADATA of non-SMP case currently only use\npadata_do_multithreaded.  It is easy to implement a serial one in\ninclude/linux/padata.h.  And it is not necessary to implement another\nfunctionality unless the only user of crypto/pcrypt.c does not depend on\nSMP in the future.\n\nLink: https://lkml.kernel.org/r/20240222140422.393911-6-gang.li@linux.dev\nSigned-off-by: Gang Li <ligang.bdlg@bytedance.com>\nTested-by: Paul E. McKenney <paulmck@kernel.org>\nAcked-by: Daniel Jordan <daniel.m.jordan@oracle.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: David Rientjes <rientjes@google.com>\nCc: Jane Chu <jane.chu@oracle.com>\nCc: Muchun Song <muchun.song@linux.dev>\nCc: Randy Dunlap <rdunlap@infradead.org>\nCc: Steffen Klassert <steffen.klassert@secunet.com>\nCc: Tim Chen <tim.c.chen@linux.intel.com>\nCc: Alexey Dobriyan <adobriyan@gmail.com>\nCc: Mike Kravetz <mike.kravetz@oracle.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}]
    }
}

2024-12-03 22:21:06,443 - extractor - DEBUG - deepseek response:
ChatCompletion(id='69274ca8-7e50-4eb7-9c95-74addb1b4a84', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "proper_nouns": [\n        {\n            "HUGETLBFS": "HUGETLBFS is a file system in the Linux kernel that allows applications to use huge pages. Huge pages are larger memory pages (typically 2MB or 1GB) that can improve memory performance by reducing the overhead associated with managing smaller pages (usually 4KB)."\n        },\n        {\n            "PADATA": "PADATA, or Parallel Data, is a framework in the Linux kernel designed to facilitate parallel processing of tasks. It allows for the distribution of tasks across multiple CPUs or threads, which can significantly speed up operations that are computationally intensive or time-consuming."\n        },\n        {\n            "bootmem": "Bootmem is a memory allocator used during the early stages of the Linux kernel boot process. It is responsible for allocating memory for the initial setup of the system, including the allocation of huge pages during the initialization of the HUGETLBFS."\n        },\n        {\n            "SMP (Symmetric Multi-Processing)": "SMP is a multiprocessing architecture where two or more identical processors are connected to a single shared main memory. The PADATA framework relies on SMP to distribute tasks across multiple processors, enabling parallel execution."\n        }\n    ],\n    "background_knowledge": [\n        "The initialization of huge pages during the boot process can be a time-consuming task, especially on systems with large amounts of memory. This is because each huge page needs to be allocated and initialized before it can be used by applications.",\n        "Parallelizing the initialization of huge pages can significantly reduce the boot time by distributing the workload across multiple CPUs or threads. This approach leverages the PADATA framework to achieve parallel execution."\n    ],\n    "kernel_changes": [\n        "Before this feature, the initialization of huge pages was a serial process, meaning it was performed sequentially on a single CPU. This could lead to long boot times, especially on systems with large amounts of memory.",\n        "With the introduction of parallelization, the initialization process for huge pages is now distributed across multiple CPUs or threads. This allows for faster completion of the task, reducing the overall boot time.",\n        "The implementation involves several key changes, including the selection of CONFIG_PADATA to enable the use of the PADATA framework, the splitting of hugetlb_hstate_alloc_pages into separate functions for 1G and 2M huge pages, and the modification of hstate_next_node_to_alloc to support thread-specific allocation."\n    ],\n    "comprehensive_understanding": "This feature aims to improve the boot performance of Linux systems by parallelizing the initialization of huge pages during the boot process. By leveraging the PADATA framework, the task of allocating and initializing huge pages is distributed across multiple CPUs or threads, significantly reducing the time required for this operation. This not only speeds up the boot process but also improves the overall efficiency of memory management in the kernel."\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733235633, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=631, prompt_tokens=5702, total_tokens=6333, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=5696, prompt_cache_miss_tokens=6))
2024-12-03 22:21:06,443 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个命名实体抽取 (NER) 任务的专家，同时也是 linux 内核的专家。给你一个 linux 内核特性的描述 (feature_description)、对应的若干 commit 的标题和提交信息 (commits)、该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)、为了让开发者理解该特性所需的与这个特性直接相关的背景知识 (background_knowledge)、该特性被实现前后，linux 内核发生的变化 (kernel_changes)、对该特性的综合理解 (comprehensive_understanding)。你的任务是：根据这些信息提取出在 linux 内核中涉及到的和这个特性相关的概念实体 (concepts)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}],
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": [
                "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
            ]
        },
        "output": {
            "concepts": ["Copy-on-Write (CoW)", "mTHP", "folio", "memory reclamation", "memory availability", "anti-fragmentation"]
        }
    ],
    "input": {
        "feature_description": "parallelize hugetlb page init on boot",
        "commits": [{"commit_subject": "hugetlb: have CONFIG_HUGETLBFS select CONFIG_PADATA", "commit_message": "Allow hugetlb use padata_do_multithreaded for parallel initialization. \nSelect CONFIG_PADATA in this case.\n\nLink: https://lkml.kernel.org/r/20240222140422.393911-7-gang.li@linux.dev\nSigned-off-by: Gang Li <ligang.bdlg@bytedance.com>\nTested-by: David Rientjes <rientjes@google.com>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nTested-by: Paul E. McKenney <paulmck@kernel.org>\nAcked-by: Daniel Jordan <daniel.m.jordan@oracle.com>\nCc: Alexey Dobriyan <adobriyan@gmail.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Jane Chu <jane.chu@oracle.com>\nCc: Mike Kravetz <mike.kravetz@oracle.com>\nCc: Randy Dunlap <rdunlap@infradead.org>\nCc: Steffen Klassert <steffen.klassert@secunet.com>\nCc: Tim Chen <tim.c.chen@linux.intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "hugetlb: parallelize 2M hugetlb allocation and initialization", "commit_message": "By distributing both the allocation and the initialization tasks across\nmultiple threads, the initialization of 2M hugetlb will be faster, thereby\nimproving the boot speed.\n\nHere are some test results:\n      test case        no patch(ms)   patched(ms)   saved\n ------------------- -------------- ------------- --------\n  256c2T(4 node) 2M           3336          1051   68.52%\n  128c1T(2 node) 2M           1943           716   63.15%\n\nLink: https://lkml.kernel.org/r/20240222140422.393911-8-gang.li@linux.dev\nSigned-off-by: Gang Li <ligang.bdlg@bytedance.com>\nTested-by: David Rientjes <rientjes@google.com>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nCc: Alexey Dobriyan <adobriyan@gmail.com>\nCc: Daniel Jordan <daniel.m.jordan@oracle.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Jane Chu <jane.chu@oracle.com>\nCc: Mike Kravetz <mike.kravetz@oracle.com>\nCc: Paul E. McKenney <paulmck@kernel.org>\nCc: Randy Dunlap <rdunlap@infradead.org>\nCc: Steffen Klassert <steffen.klassert@secunet.com>\nCc: Tim Chen <tim.c.chen@linux.intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "hugetlb: split hugetlb_hstate_alloc_pages", "commit_message": "1G and 2M huge pages have different allocation and initialization logic,\nwhich leads to subtle differences in parallelization.  Therefore, it is\nappropriate to split hugetlb_hstate_alloc_pages into gigantic and\nnon-gigantic.\n\nThis patch has no functional changes.\n\nLink: https://lkml.kernel.org/r/20240222140422.393911-3-gang.li@linux.dev\nSigned-off-by: Gang Li <ligang.bdlg@bytedance.com>\nTested-by: David Rientjes <rientjes@google.com>\nReviewed-by: Tim Chen <tim.c.chen@linux.intel.com>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nCc: Alexey Dobriyan <adobriyan@gmail.com>\nCc: Daniel Jordan <daniel.m.jordan@oracle.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Jane Chu <jane.chu@oracle.com>\nCc: Mike Kravetz <mike.kravetz@oracle.com>\nCc: Paul E. McKenney <paulmck@kernel.org>\nCc: Randy Dunlap <rdunlap@infradead.org>\nCc: Steffen Klassert <steffen.klassert@secunet.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "hugetlb: pass *next_nid_to_alloc directly to for_each_node_mask_to_alloc", "commit_message": "With parallelization of hugetlb allocation across different threads, each\nthread works on a differnet node to allocate pages from, instead of all\nallocating from a common node h->next_nid_to_alloc.  To address this, it's\nnecessary to assign a separate next_nid_to_alloc for each thread.\n\nConsequently, the hstate_next_node_to_alloc and\nfor_each_node_mask_to_alloc have been modified to directly accept a\n*next_nid_to_alloc parameter, ensuring thread-specific allocation and\navoiding concurrent access issues.\n\nLink: https://lkml.kernel.org/r/20240222140422.393911-4-gang.li@linux.dev\nSigned-off-by: Gang Li <ligang.bdlg@bytedance.com>\nTested-by: David Rientjes <rientjes@google.com>\nReviewed-by: Tim Chen <tim.c.chen@linux.intel.com>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nCc: Alexey Dobriyan <adobriyan@gmail.com>\nCc: Daniel Jordan <daniel.m.jordan@oracle.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Jane Chu <jane.chu@oracle.com>\nCc: Mike Kravetz <mike.kravetz@oracle.com>\nCc: Paul E. McKenney <paulmck@kernel.org>\nCc: Randy Dunlap <rdunlap@infradead.org>\nCc: Steffen Klassert <steffen.klassert@secunet.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "hugetlb: code clean for hugetlb_hstate_alloc_pages", "commit_message": "Patch series \"hugetlb: parallelize hugetlb page init on boot\", v6.\n\nIntroduction\n------------\nHugetlb initialization during boot takes up a considerable amount of time.\nFor instance, on a 2TB system, initializing 1,800 1GB huge pages takes\n1-2 seconds out of 10 seconds.  Initializing 11,776 1GB pages on a 12TB\nIntel host takes more than 1 minute[1].  This is a noteworthy figure.\n\nInspired by [2] and [3], hugetlb initialization can also be accelerated\nthrough parallelization.  Kernel already has infrastructure like\npadata_do_multithreaded, this patch uses it to achieve effective results\nby minimal modifications.\n\n[1] https://lore.kernel.org/all/783f8bac-55b8-5b95-eb6a-11a583675000@google.com/\n[2] https://lore.kernel.org/all/20200527173608.2885243-1-daniel.m.jordan@oracle.com/\n[3] https://lore.kernel.org/all/20230906112605.2286994-1-usama.arif@bytedance.com/\n[4] https://lore.kernel.org/all/76becfc1-e609-e3e8-2966-4053143170b6@google.com/\n\nmax_threads\n-----------\nThis patch use `padata_do_multithreaded` like this:\n\n```\njob.max_threads\t= num_node_state(N_MEMORY) * multiplier;\npadata_do_multithreaded(&job);\n```\n\nTo fully utilize the CPU, the number of parallel threads needs to be\ncarefully considered.  `max_threads = num_node_state(N_MEMORY)` does not\nfully utilize the CPU, so we need to multiply it by a multiplier.\n\nTests below indicate that a multiplier of 2 significantly improves\nperformance, and although larger values also provide improvements, the\ngains are marginal.\n\n  multiplier     1       2       3       4       5\n ------------ ------- ------- ------- ------- -------\n  256G 2node   358ms   215ms   157ms   134ms   126ms\n  2T   4node   979ms   679ms   543ms   489ms   481ms\n  50G  2node   71ms    44ms    37ms    30ms    31ms\n\nTherefore, choosing 2 as the multiplier strikes a good balance between\nenhancing parallel processing capabilities and maintaining efficient\nresource management.\n\nTest result\n-----------\n      test case       no patch(ms)   patched(ms)   saved\n ------------------- -------------- ------------- --------\n  256c2T(4 node) 1G           4745          2024   57.34%\n  128c1T(2 node) 1G           3358          1712   49.02%\n     12T         1G          77000         18300   76.23%\n\n  256c2T(4 node) 2M           3336          1051   68.52%\n  128c1T(2 node) 2M           1943           716   63.15%\n\n\nThis patch (of 8):\n\nThe readability of `hugetlb_hstate_alloc_pages` is poor.  By cleaning the\ncode, its readability can be improved, facilitating future modifications.\n\nThis patch extracts two functions to reduce the complexity of\n`hugetlb_hstate_alloc_pages` and has no functional changes.\n\n- hugetlb_hstate_alloc_pages_node_specific() to handle iterates through\n  each online node and performs allocation if necessary.\n- hugetlb_hstate_alloc_pages_report() report error during allocation.\n  And the value of h->max_huge_pages is updated accordingly.\n\nLink: https://lkml.kernel.org/r/20240222140422.393911-1-gang.li@linux.dev\nLink: https://lkml.kernel.org/r/20240222140422.393911-2-gang.li@linux.dev\nSigned-off-by: Gang Li <ligang.bdlg@bytedance.com>\nTested-by: David Rientjes <rientjes@google.com>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nReviewed-by: Tim Chen <tim.c.chen@linux.intel.com>\nCc: Daniel Jordan <daniel.m.jordan@oracle.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Jane Chu <jane.chu@oracle.com>\nCc: Paul E. McKenney <paulmck@kernel.org>\nCc: Randy Dunlap <rdunlap@infradead.org>\nCc: Steffen Klassert <steffen.klassert@secunet.com>\nCc: Alexey Dobriyan <adobriyan@gmail.com>\nCc: Mike Kravetz <mike.kravetz@oracle.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "hugetlb: parallelize 1G hugetlb initialization", "commit_message": "Optimizing the initialization speed of 1G huge pages through\nparallelization.\n\n1G hugetlbs are allocated from bootmem, a process that is already very\nfast and does not currently require optimization.  Therefore, we focus on\nparallelizing only the initialization phase in `gather_bootmem_prealloc`.\n\nHere are some test results:\n      test case       no patch(ms)   patched(ms)   saved\n ------------------- -------------- ------------- --------\n  256c2T(4 node) 1G           4745          2024   57.34%\n  128c1T(2 node) 1G           3358          1712   49.02%\n     12T         1G          77000         18300   76.23%\n\n[akpm@linux-foundation.org: s/initialied/initialized/, per Alexey]\nLink: https://lkml.kernel.org/r/20240222140422.393911-9-gang.li@linux.dev\nSigned-off-by: Gang Li <ligang.bdlg@bytedance.com>\nTested-by: David Rientjes <rientjes@google.com>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nCc: Alexey Dobriyan <adobriyan@gmail.com>\nCc: Daniel Jordan <daniel.m.jordan@oracle.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Jane Chu <jane.chu@oracle.com>\nCc: Mike Kravetz <mike.kravetz@oracle.com>\nCc: Paul E. McKenney <paulmck@kernel.org>\nCc: Randy Dunlap <rdunlap@infradead.org>\nCc: Steffen Klassert <steffen.klassert@secunet.com>\nCc: Tim Chen <tim.c.chen@linux.intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "padata: downgrade padata_do_multithreaded to serial execution for non-SMP", "commit_message": "hugetlb parallelization depends on PADATA, and PADATA depends on SMP.\n\nPADATA consists of two distinct functionality: One part is\npadata_do_multithreaded which disregards order and simply divides tasks\ninto several groups for parallel execution.  Hugetlb init parallelization\ndepends on padata_do_multithreaded.\n\nThe other part is composed of a set of APIs that, while handling data in\nan out-of-order parallel manner, can eventually return the data with\nordered sequence.  Currently Only `crypto/pcrypt.c` use them.\n\nAll users of PADATA of non-SMP case currently only use\npadata_do_multithreaded.  It is easy to implement a serial one in\ninclude/linux/padata.h.  And it is not necessary to implement another\nfunctionality unless the only user of crypto/pcrypt.c does not depend on\nSMP in the future.\n\nLink: https://lkml.kernel.org/r/20240222140422.393911-6-gang.li@linux.dev\nSigned-off-by: Gang Li <ligang.bdlg@bytedance.com>\nTested-by: Paul E. McKenney <paulmck@kernel.org>\nAcked-by: Daniel Jordan <daniel.m.jordan@oracle.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: David Rientjes <rientjes@google.com>\nCc: Jane Chu <jane.chu@oracle.com>\nCc: Muchun Song <muchun.song@linux.dev>\nCc: Randy Dunlap <rdunlap@infradead.org>\nCc: Steffen Klassert <steffen.klassert@secunet.com>\nCc: Tim Chen <tim.c.chen@linux.intel.com>\nCc: Alexey Dobriyan <adobriyan@gmail.com>\nCc: Mike Kravetz <mike.kravetz@oracle.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}],
        "proper_nouns": [{"HUGETLBFS": "HUGETLBFS is a file system in the Linux kernel that allows applications to use huge pages. Huge pages are larger memory pages (typically 2MB or 1GB) that can improve memory performance by reducing the overhead associated with managing smaller pages (usually 4KB)."}, {"PADATA": "PADATA, or Parallel Data, is a framework in the Linux kernel designed to facilitate parallel processing of tasks. It allows for the distribution of tasks across multiple CPUs or threads, which can significantly speed up operations that are computationally intensive or time-consuming."}, {"bootmem": "Bootmem is a memory allocator used during the early stages of the Linux kernel boot process. It is responsible for allocating memory for the initial setup of the system, including the allocation of huge pages during the initialization of the HUGETLBFS."}, {"SMP (Symmetric Multi-Processing)": "SMP is a multiprocessing architecture where two or more identical processors are connected to a single shared main memory. The PADATA framework relies on SMP to distribute tasks across multiple processors, enabling parallel execution."}],
        "background_knowledge": ["The initialization of huge pages during the boot process can be a time-consuming task, especially on systems with large amounts of memory. This is because each huge page needs to be allocated and initialized before it can be used by applications.", "Parallelizing the initialization of huge pages can significantly reduce the boot time by distributing the workload across multiple CPUs or threads. This approach leverages the PADATA framework to achieve parallel execution."],
        "kernel_changes": ["Before this feature, the initialization of huge pages was a serial process, meaning it was performed sequentially on a single CPU. This could lead to long boot times, especially on systems with large amounts of memory.", "With the introduction of parallelization, the initialization process for huge pages is now distributed across multiple CPUs or threads. This allows for faster completion of the task, reducing the overall boot time.", "The implementation involves several key changes, including the selection of CONFIG_PADATA to enable the use of the PADATA framework, the splitting of hugetlb_hstate_alloc_pages into separate functions for 1G and 2M huge pages, and the modification of hstate_next_node_to_alloc to support thread-specific allocation."],
        "comprehensive_understanding": "This feature aims to improve the boot performance of Linux systems by parallelizing the initialization of huge pages during the boot process. By leveraging the PADATA framework, the task of allocating and initializing huge pages is distributed across multiple CPUs or threads, significantly reducing the time required for this operation. This not only speeds up the boot process but also improves the overall efficiency of memory management in the kernel."
    }
}

2024-12-03 22:21:10,383 - extractor - DEBUG - deepseek response:
ChatCompletion(id='15f9a2e2-3c47-4a36-8e33-c36c30b6f175', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "concepts": ["HUGETLBFS", "PADATA", "bootmem", "SMP (Symmetric Multi-Processing)", "huge pages", "parallelization", "memory management", "boot performance", "multithreading", "CPU utilization"]\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733235666, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=63, prompt_tokens=6334, total_tokens=6397, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=5760, prompt_cache_miss_tokens=574))
2024-12-03 22:21:10,384 - extractor - DEBUG - feature_extracted of feature_id=33236:
{
    "feature_id": 33236,
    "feature_description": "parallelize hugetlb page init on boot",
    "version": "6.9",
    "commits": [
        {
            "commit_subject": "hugetlb: have CONFIG_HUGETLBFS select CONFIG_PADATA",
            "commit_message": "Allow hugetlb use padata_do_multithreaded for parallel initialization. \nSelect CONFIG_PADATA in this case.\n\nLink: https://lkml.kernel.org/r/20240222140422.393911-7-gang.li@linux.dev\nSigned-off-by: Gang Li <ligang.bdlg@bytedance.com>\nTested-by: David Rientjes <rientjes@google.com>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nTested-by: Paul E. McKenney <paulmck@kernel.org>\nAcked-by: Daniel Jordan <daniel.m.jordan@oracle.com>\nCc: Alexey Dobriyan <adobriyan@gmail.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Jane Chu <jane.chu@oracle.com>\nCc: Mike Kravetz <mike.kravetz@oracle.com>\nCc: Randy Dunlap <rdunlap@infradead.org>\nCc: Steffen Klassert <steffen.klassert@secunet.com>\nCc: Tim Chen <tim.c.chen@linux.intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "hugetlb: parallelize 2M hugetlb allocation and initialization",
            "commit_message": "By distributing both the allocation and the initialization tasks across\nmultiple threads, the initialization of 2M hugetlb will be faster, thereby\nimproving the boot speed.\n\nHere are some test results:\n      test case        no patch(ms)   patched(ms)   saved\n ------------------- -------------- ------------- --------\n  256c2T(4 node) 2M           3336          1051   68.52%\n  128c1T(2 node) 2M           1943           716   63.15%\n\nLink: https://lkml.kernel.org/r/20240222140422.393911-8-gang.li@linux.dev\nSigned-off-by: Gang Li <ligang.bdlg@bytedance.com>\nTested-by: David Rientjes <rientjes@google.com>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nCc: Alexey Dobriyan <adobriyan@gmail.com>\nCc: Daniel Jordan <daniel.m.jordan@oracle.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Jane Chu <jane.chu@oracle.com>\nCc: Mike Kravetz <mike.kravetz@oracle.com>\nCc: Paul E. McKenney <paulmck@kernel.org>\nCc: Randy Dunlap <rdunlap@infradead.org>\nCc: Steffen Klassert <steffen.klassert@secunet.com>\nCc: Tim Chen <tim.c.chen@linux.intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "hugetlb: split hugetlb_hstate_alloc_pages",
            "commit_message": "1G and 2M huge pages have different allocation and initialization logic,\nwhich leads to subtle differences in parallelization.  Therefore, it is\nappropriate to split hugetlb_hstate_alloc_pages into gigantic and\nnon-gigantic.\n\nThis patch has no functional changes.\n\nLink: https://lkml.kernel.org/r/20240222140422.393911-3-gang.li@linux.dev\nSigned-off-by: Gang Li <ligang.bdlg@bytedance.com>\nTested-by: David Rientjes <rientjes@google.com>\nReviewed-by: Tim Chen <tim.c.chen@linux.intel.com>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nCc: Alexey Dobriyan <adobriyan@gmail.com>\nCc: Daniel Jordan <daniel.m.jordan@oracle.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Jane Chu <jane.chu@oracle.com>\nCc: Mike Kravetz <mike.kravetz@oracle.com>\nCc: Paul E. McKenney <paulmck@kernel.org>\nCc: Randy Dunlap <rdunlap@infradead.org>\nCc: Steffen Klassert <steffen.klassert@secunet.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "hugetlb: pass *next_nid_to_alloc directly to for_each_node_mask_to_alloc",
            "commit_message": "With parallelization of hugetlb allocation across different threads, each\nthread works on a differnet node to allocate pages from, instead of all\nallocating from a common node h->next_nid_to_alloc.  To address this, it's\nnecessary to assign a separate next_nid_to_alloc for each thread.\n\nConsequently, the hstate_next_node_to_alloc and\nfor_each_node_mask_to_alloc have been modified to directly accept a\n*next_nid_to_alloc parameter, ensuring thread-specific allocation and\navoiding concurrent access issues.\n\nLink: https://lkml.kernel.org/r/20240222140422.393911-4-gang.li@linux.dev\nSigned-off-by: Gang Li <ligang.bdlg@bytedance.com>\nTested-by: David Rientjes <rientjes@google.com>\nReviewed-by: Tim Chen <tim.c.chen@linux.intel.com>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nCc: Alexey Dobriyan <adobriyan@gmail.com>\nCc: Daniel Jordan <daniel.m.jordan@oracle.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Jane Chu <jane.chu@oracle.com>\nCc: Mike Kravetz <mike.kravetz@oracle.com>\nCc: Paul E. McKenney <paulmck@kernel.org>\nCc: Randy Dunlap <rdunlap@infradead.org>\nCc: Steffen Klassert <steffen.klassert@secunet.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "hugetlb: code clean for hugetlb_hstate_alloc_pages",
            "commit_message": "Patch series \"hugetlb: parallelize hugetlb page init on boot\", v6.\n\nIntroduction\n------------\nHugetlb initialization during boot takes up a considerable amount of time.\nFor instance, on a 2TB system, initializing 1,800 1GB huge pages takes\n1-2 seconds out of 10 seconds.  Initializing 11,776 1GB pages on a 12TB\nIntel host takes more than 1 minute[1].  This is a noteworthy figure.\n\nInspired by [2] and [3], hugetlb initialization can also be accelerated\nthrough parallelization.  Kernel already has infrastructure like\npadata_do_multithreaded, this patch uses it to achieve effective results\nby minimal modifications.\n\n[1] https://lore.kernel.org/all/783f8bac-55b8-5b95-eb6a-11a583675000@google.com/\n[2] https://lore.kernel.org/all/20200527173608.2885243-1-daniel.m.jordan@oracle.com/\n[3] https://lore.kernel.org/all/20230906112605.2286994-1-usama.arif@bytedance.com/\n[4] https://lore.kernel.org/all/76becfc1-e609-e3e8-2966-4053143170b6@google.com/\n\nmax_threads\n-----------\nThis patch use `padata_do_multithreaded` like this:\n\n```\njob.max_threads\t= num_node_state(N_MEMORY) * multiplier;\npadata_do_multithreaded(&job);\n```\n\nTo fully utilize the CPU, the number of parallel threads needs to be\ncarefully considered.  `max_threads = num_node_state(N_MEMORY)` does not\nfully utilize the CPU, so we need to multiply it by a multiplier.\n\nTests below indicate that a multiplier of 2 significantly improves\nperformance, and although larger values also provide improvements, the\ngains are marginal.\n\n  multiplier     1       2       3       4       5\n ------------ ------- ------- ------- ------- -------\n  256G 2node   358ms   215ms   157ms   134ms   126ms\n  2T   4node   979ms   679ms   543ms   489ms   481ms\n  50G  2node   71ms    44ms    37ms    30ms    31ms\n\nTherefore, choosing 2 as the multiplier strikes a good balance between\nenhancing parallel processing capabilities and maintaining efficient\nresource management.\n\nTest result\n-----------\n      test case       no patch(ms)   patched(ms)   saved\n ------------------- -------------- ------------- --------\n  256c2T(4 node) 1G           4745          2024   57.34%\n  128c1T(2 node) 1G           3358          1712   49.02%\n     12T         1G          77000         18300   76.23%\n\n  256c2T(4 node) 2M           3336          1051   68.52%\n  128c1T(2 node) 2M           1943           716   63.15%\n\n\nThis patch (of 8):\n\nThe readability of `hugetlb_hstate_alloc_pages` is poor.  By cleaning the\ncode, its readability can be improved, facilitating future modifications.\n\nThis patch extracts two functions to reduce the complexity of\n`hugetlb_hstate_alloc_pages` and has no functional changes.\n\n- hugetlb_hstate_alloc_pages_node_specific() to handle iterates through\n  each online node and performs allocation if necessary.\n- hugetlb_hstate_alloc_pages_report() report error during allocation.\n  And the value of h->max_huge_pages is updated accordingly.\n\nLink: https://lkml.kernel.org/r/20240222140422.393911-1-gang.li@linux.dev\nLink: https://lkml.kernel.org/r/20240222140422.393911-2-gang.li@linux.dev\nSigned-off-by: Gang Li <ligang.bdlg@bytedance.com>\nTested-by: David Rientjes <rientjes@google.com>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nReviewed-by: Tim Chen <tim.c.chen@linux.intel.com>\nCc: Daniel Jordan <daniel.m.jordan@oracle.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Jane Chu <jane.chu@oracle.com>\nCc: Paul E. McKenney <paulmck@kernel.org>\nCc: Randy Dunlap <rdunlap@infradead.org>\nCc: Steffen Klassert <steffen.klassert@secunet.com>\nCc: Alexey Dobriyan <adobriyan@gmail.com>\nCc: Mike Kravetz <mike.kravetz@oracle.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "hugetlb: parallelize 1G hugetlb initialization",
            "commit_message": "Optimizing the initialization speed of 1G huge pages through\nparallelization.\n\n1G hugetlbs are allocated from bootmem, a process that is already very\nfast and does not currently require optimization.  Therefore, we focus on\nparallelizing only the initialization phase in `gather_bootmem_prealloc`.\n\nHere are some test results:\n      test case       no patch(ms)   patched(ms)   saved\n ------------------- -------------- ------------- --------\n  256c2T(4 node) 1G           4745          2024   57.34%\n  128c1T(2 node) 1G           3358          1712   49.02%\n     12T         1G          77000         18300   76.23%\n\n[akpm@linux-foundation.org: s/initialied/initialized/, per Alexey]\nLink: https://lkml.kernel.org/r/20240222140422.393911-9-gang.li@linux.dev\nSigned-off-by: Gang Li <ligang.bdlg@bytedance.com>\nTested-by: David Rientjes <rientjes@google.com>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nCc: Alexey Dobriyan <adobriyan@gmail.com>\nCc: Daniel Jordan <daniel.m.jordan@oracle.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Jane Chu <jane.chu@oracle.com>\nCc: Mike Kravetz <mike.kravetz@oracle.com>\nCc: Paul E. McKenney <paulmck@kernel.org>\nCc: Randy Dunlap <rdunlap@infradead.org>\nCc: Steffen Klassert <steffen.klassert@secunet.com>\nCc: Tim Chen <tim.c.chen@linux.intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "padata: downgrade padata_do_multithreaded to serial execution for non-SMP",
            "commit_message": "hugetlb parallelization depends on PADATA, and PADATA depends on SMP.\n\nPADATA consists of two distinct functionality: One part is\npadata_do_multithreaded which disregards order and simply divides tasks\ninto several groups for parallel execution.  Hugetlb init parallelization\ndepends on padata_do_multithreaded.\n\nThe other part is composed of a set of APIs that, while handling data in\nan out-of-order parallel manner, can eventually return the data with\nordered sequence.  Currently Only `crypto/pcrypt.c` use them.\n\nAll users of PADATA of non-SMP case currently only use\npadata_do_multithreaded.  It is easy to implement a serial one in\ninclude/linux/padata.h.  And it is not necessary to implement another\nfunctionality unless the only user of crypto/pcrypt.c does not depend on\nSMP in the future.\n\nLink: https://lkml.kernel.org/r/20240222140422.393911-6-gang.li@linux.dev\nSigned-off-by: Gang Li <ligang.bdlg@bytedance.com>\nTested-by: Paul E. McKenney <paulmck@kernel.org>\nAcked-by: Daniel Jordan <daniel.m.jordan@oracle.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: David Rientjes <rientjes@google.com>\nCc: Jane Chu <jane.chu@oracle.com>\nCc: Muchun Song <muchun.song@linux.dev>\nCc: Randy Dunlap <rdunlap@infradead.org>\nCc: Steffen Klassert <steffen.klassert@secunet.com>\nCc: Tim Chen <tim.c.chen@linux.intel.com>\nCc: Alexey Dobriyan <adobriyan@gmail.com>\nCc: Mike Kravetz <mike.kravetz@oracle.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        }
    ],
    "proper_nouns": [
        {
            "HUGETLBFS": "HUGETLBFS is a file system in the Linux kernel that allows applications to use huge pages. Huge pages are larger memory pages (typically 2MB or 1GB) that can improve memory performance by reducing the overhead associated with managing smaller pages (usually 4KB)."
        },
        {
            "PADATA": "PADATA, or Parallel Data, is a framework in the Linux kernel designed to facilitate parallel processing of tasks. It allows for the distribution of tasks across multiple CPUs or threads, which can significantly speed up operations that are computationally intensive or time-consuming."
        },
        {
            "bootmem": "Bootmem is a memory allocator used during the early stages of the Linux kernel boot process. It is responsible for allocating memory for the initial setup of the system, including the allocation of huge pages during the initialization of the HUGETLBFS."
        },
        {
            "SMP (Symmetric Multi-Processing)": "SMP is a multiprocessing architecture where two or more identical processors are connected to a single shared main memory. The PADATA framework relies on SMP to distribute tasks across multiple processors, enabling parallel execution."
        }
    ],
    "background_knowledge": [
        "The initialization of huge pages during the boot process can be a time-consuming task, especially on systems with large amounts of memory. This is because each huge page needs to be allocated and initialized before it can be used by applications.",
        "Parallelizing the initialization of huge pages can significantly reduce the boot time by distributing the workload across multiple CPUs or threads. This approach leverages the PADATA framework to achieve parallel execution."
    ],
    "kernel_changes": [
        "Before this feature, the initialization of huge pages was a serial process, meaning it was performed sequentially on a single CPU. This could lead to long boot times, especially on systems with large amounts of memory.",
        "With the introduction of parallelization, the initialization process for huge pages is now distributed across multiple CPUs or threads. This allows for faster completion of the task, reducing the overall boot time.",
        "The implementation involves several key changes, including the selection of CONFIG_PADATA to enable the use of the PADATA framework, the splitting of hugetlb_hstate_alloc_pages into separate functions for 1G and 2M huge pages, and the modification of hstate_next_node_to_alloc to support thread-specific allocation."
    ],
    "comprehensive_understanding": "This feature aims to improve the boot performance of Linux systems by parallelizing the initialization of huge pages during the boot process. By leveraging the PADATA framework, the task of allocating and initializing huge pages is distributed across multiple CPUs or threads, significantly reducing the time required for this operation. This not only speeds up the boot process but also improves the overall efficiency of memory management in the kernel.",
    "concepts": [
        "HUGETLBFS",
        "PADATA",
        "bootmem",
        "SMP (Symmetric Multi-Processing)",
        "huge pages",
        "parallelization",
        "memory management",
        "boot performance",
        "multithreading",
        "CPU utilization"
    ]
}
2024-12-03 22:21:10,384 - extractor - INFO - ---------Processing feature 5/815---------
2024-12-03 22:21:48,245 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个 linux 内核的专家。给你一个 linux 内核特性的描述以及其对应的若干 commit 的标题和提交信息，你的任务是为这个特性写一个多方面的扩写版描述。你的扩写需包含以下几个方面：1. 该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)；2. 为了让开发者理解该特性，需要哪些具体的，与这个特性直接相关的背景知识 (background_knowledge)；3. 该特性被实现前后，linux 内核发生了怎样的变化 (kernel_changes)；4. 你对该特性的综合理解 (comprehensive_understanding)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}]
        },
        "output": {
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
        }
    ],
    "input": {
        "feature_description": "PageFlags cleanups",
        "commits": [{"commit_subject": "mm: constify testing page/folio flags", "commit_message": "Now that dump_page() takes a const argument, we can constify all the page\nflag tests.\n\nLink: https://lkml.kernel.org/r/20240227192337.757313-7-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nReviewed-by: David Hildenbrand <david@redhat.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: constify more page/folio tests", "commit_message": "Constify the flag tests that aren't automatically generated and the tests\nthat look like flag tests but are more complicated.\n\nLink: https://lkml.kernel.org/r/20240227192337.757313-8-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nReviewed-by: David Hildenbrand <david@redhat.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: remove PageWaiters, PageSetWaiters and PageClearWaiters", "commit_message": "All callers have been converted to use folios.  This was the only user of\nPF_ONLY_HEAD, so remove that too.\n\nLink: https://lkml.kernel.org/r/20240227192337.757313-3-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nReviewed-by: David Hildenbrand <david@redhat.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: remove PageYoung and PageIdle definitions", "commit_message": "All callers have been converted to use folios, so remove the various\nset/clear/test functions defined on pages.\n\nLink: https://lkml.kernel.org/r/20240227192337.757313-4-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nReviewed-by: David Hildenbrand <david@redhat.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: remove cast from page_to_nid()", "commit_message": "Now that PF_POISONED_CHECK() can take a const argument, we can drop\nthe cast.\n\nLink: https://lkml.kernel.org/r/20240227192337.757313-9-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nReviewed-by: David Hildenbrand <david@redhat.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: separate out FOLIO_FLAGS from PAGEFLAGS", "commit_message": "Patch series \"PageFlags cleanups\".\n\nWe have now successfully removed all of the uses of some of the PageFlags\nfrom the kernel, but there's nothing to stop somebody reintroducing them. \nBy splitting out FOLIO_FLAGS from PAGEFLAGS, we can stop defining the old\nflags; and we do that in some of the later patches.\n\nAfter doing this, I realised that dump_page() was living dangerously; we\ncould end up calling folio_test_foo() on a pointer which no longer pointed\nto a folio (as dump_page() is not necessarily called when the caller has a\nreference to the page).  So I fixed that up.\n\nAnd then I realised that this was the key to making dump_page() take a\nconst argument, which means we can constify the page flags testing, which\nmeans we can remove more cast-away-the-const bad code.\n\nAnd here's where I ended up.\n\n\nThis patch (of 8):\n\nWe've progressed far enough with the folio transition that some flags are\nnow no longer checked on pages, but only on folios.  To prevent new users\nappearing, prepare to only define the folio versions of the flag\ntest/set/clear.\n\nLink: https://lkml.kernel.org/r/20240227192337.757313-1-willy@infradead.org\nLink: https://lkml.kernel.org/r/20240227192337.757313-2-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nReviewed-by: David Hildenbrand <david@redhat.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: make dump_page() take a const argument", "commit_message": "Now that __dump_page() takes a const argument, we can make dump_page()\ntake a const struct page too.\n\nLink: https://lkml.kernel.org/r/20240227192337.757313-6-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nReviewed-by: David Hildenbrand <david@redhat.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: add __dump_folio()", "commit_message": "Turn __dump_page() into a wrapper around __dump_folio().  Snapshot the\npage & folio into a stack variable so we don't hit BUG_ON() if an\nallocation is freed under us and what was a folio pointer becomes a\npointer to a tail page.\n\n[willy@infradead.org: fix build issue]\n  Link: https://lkml.kernel.org/r/ZeAKCyTn_xS3O9cE@casper.infradead.org\n[willy@infradead.org: fix __dump_folio]\n  Link: https://lkml.kernel.org/r/ZeJJegP8zM7S9GTy@casper.infradead.org\n[willy@infradead.org: fix pointer confusion]\n  Link: https://lkml.kernel.org/r/ZeYa00ixxC4k1ot-@casper.infradead.org\n[akpm@linux-foundation.org: s/printk/pr_warn/]\nLink: https://lkml.kernel.org/r/20240227192337.757313-5-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}]
    }
}

2024-12-03 22:22:30,363 - extractor - DEBUG - deepseek response:
ChatCompletion(id='0951e971-b920-40af-b25c-2051582c9731', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "proper_nouns": [\n        {\n            "PageFlags": "PageFlags are a set of flags associated with each memory page in the Linux kernel. These flags are used to indicate the state and characteristics of a page, such as whether it is locked, dirty, or reserved. They play a crucial role in memory management and are essential for various operations like page allocation, swapping, and file system operations."\n        },\n        {\n            "folio": "In the Linux kernel, a \'folio\' is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages."\n        },\n        {\n            "dump_page()": "dump_page() is a function in the Linux kernel used for debugging purposes. It prints detailed information about a given memory page, including its flags, mapping, and other relevant attributes. This function is particularly useful for diagnosing memory-related issues and understanding the state of memory pages during runtime."\n        }\n    ],\n    "background_knowledge": [\n        "The PageFlags in the Linux kernel are used to manage and track the state of memory pages. These flags are essential for various memory management operations, including page allocation, swapping, and file system operations.",\n        "The introduction of \'folios\' in the Linux kernel aims to improve the efficiency of memory management, especially for large files and operations that span multiple pages. Folios provide a higher-level abstraction that simplifies the management of memory pages.",\n        "The dump_page() function is a debugging tool that provides detailed information about a memory page, which is crucial for diagnosing memory-related issues and understanding the state of memory pages during runtime."\n    ],\n    "kernel_changes": [\n        "Before the \'PageFlags cleanups\' feature, the Linux kernel used a mix of page-specific flags and folio-specific flags, which could lead to inconsistencies and inefficiencies in memory management. The feature introduced several changes to streamline and optimize the use of PageFlags:",\n        "1. Constification of page/folio flag tests: This change ensures that the flag tests are performed in a consistent and efficient manner, reducing the risk of errors and improving code readability.",\n        "2. Removal of obsolete PageFlags: Several obsolete PageFlags, such as PageWaiters, PageSetWaiters, and PageClearWaiters, were removed as they were no longer necessary with the introduction of folios.",\n        "3. Separation of FOLIO_FLAGS from PAGEFLAGS: By separating folio-specific flags from page-specific flags, the kernel prevents the reintroduction of obsolete flags and ensures that only relevant flags are used for each type of memory management operation.",\n        "4. Making dump_page() take a const argument: This change enhances the safety and reliability of the dump_page() function by ensuring that it operates on a constant reference to a memory page, preventing unintended modifications."\n    ],\n    "comprehensive_understanding": "The \'PageFlags cleanups\' feature in the Linux kernel represents a significant improvement in the management and efficiency of memory pages. By constifying page/folio flag tests, removing obsolete PageFlags, separating FOLIO_FLAGS from PAGEFLAGS, and enhancing the dump_page() function, the feature ensures that memory management operations are performed in a consistent, efficient, and reliable manner. These changes not only improve the performance of the kernel but also enhance its maintainability and robustness, making it easier for developers to diagnose and resolve memory-related issues."\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733235708, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=774, prompt_tokens=3349, total_tokens=4123, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=3328, prompt_cache_miss_tokens=21))
2024-12-03 22:22:30,364 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个命名实体抽取 (NER) 任务的专家，同时也是 linux 内核的专家。给你一个 linux 内核特性的描述 (feature_description)、对应的若干 commit 的标题和提交信息 (commits)、该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)、为了让开发者理解该特性所需的与这个特性直接相关的背景知识 (background_knowledge)、该特性被实现前后，linux 内核发生的变化 (kernel_changes)、对该特性的综合理解 (comprehensive_understanding)。你的任务是：根据这些信息提取出在 linux 内核中涉及到的和这个特性相关的概念实体 (concepts)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}],
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": [
                "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
            ]
        },
        "output": {
            "concepts": ["Copy-on-Write (CoW)", "mTHP", "folio", "memory reclamation", "memory availability", "anti-fragmentation"]
        }
    ],
    "input": {
        "feature_description": "PageFlags cleanups",
        "commits": [{"commit_subject": "mm: constify testing page/folio flags", "commit_message": "Now that dump_page() takes a const argument, we can constify all the page\nflag tests.\n\nLink: https://lkml.kernel.org/r/20240227192337.757313-7-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nReviewed-by: David Hildenbrand <david@redhat.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: constify more page/folio tests", "commit_message": "Constify the flag tests that aren't automatically generated and the tests\nthat look like flag tests but are more complicated.\n\nLink: https://lkml.kernel.org/r/20240227192337.757313-8-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nReviewed-by: David Hildenbrand <david@redhat.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: remove PageWaiters, PageSetWaiters and PageClearWaiters", "commit_message": "All callers have been converted to use folios.  This was the only user of\nPF_ONLY_HEAD, so remove that too.\n\nLink: https://lkml.kernel.org/r/20240227192337.757313-3-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nReviewed-by: David Hildenbrand <david@redhat.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: remove PageYoung and PageIdle definitions", "commit_message": "All callers have been converted to use folios, so remove the various\nset/clear/test functions defined on pages.\n\nLink: https://lkml.kernel.org/r/20240227192337.757313-4-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nReviewed-by: David Hildenbrand <david@redhat.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: remove cast from page_to_nid()", "commit_message": "Now that PF_POISONED_CHECK() can take a const argument, we can drop\nthe cast.\n\nLink: https://lkml.kernel.org/r/20240227192337.757313-9-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nReviewed-by: David Hildenbrand <david@redhat.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: separate out FOLIO_FLAGS from PAGEFLAGS", "commit_message": "Patch series \"PageFlags cleanups\".\n\nWe have now successfully removed all of the uses of some of the PageFlags\nfrom the kernel, but there's nothing to stop somebody reintroducing them. \nBy splitting out FOLIO_FLAGS from PAGEFLAGS, we can stop defining the old\nflags; and we do that in some of the later patches.\n\nAfter doing this, I realised that dump_page() was living dangerously; we\ncould end up calling folio_test_foo() on a pointer which no longer pointed\nto a folio (as dump_page() is not necessarily called when the caller has a\nreference to the page).  So I fixed that up.\n\nAnd then I realised that this was the key to making dump_page() take a\nconst argument, which means we can constify the page flags testing, which\nmeans we can remove more cast-away-the-const bad code.\n\nAnd here's where I ended up.\n\n\nThis patch (of 8):\n\nWe've progressed far enough with the folio transition that some flags are\nnow no longer checked on pages, but only on folios.  To prevent new users\nappearing, prepare to only define the folio versions of the flag\ntest/set/clear.\n\nLink: https://lkml.kernel.org/r/20240227192337.757313-1-willy@infradead.org\nLink: https://lkml.kernel.org/r/20240227192337.757313-2-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nReviewed-by: David Hildenbrand <david@redhat.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: make dump_page() take a const argument", "commit_message": "Now that __dump_page() takes a const argument, we can make dump_page()\ntake a const struct page too.\n\nLink: https://lkml.kernel.org/r/20240227192337.757313-6-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nReviewed-by: David Hildenbrand <david@redhat.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: add __dump_folio()", "commit_message": "Turn __dump_page() into a wrapper around __dump_folio().  Snapshot the\npage & folio into a stack variable so we don't hit BUG_ON() if an\nallocation is freed under us and what was a folio pointer becomes a\npointer to a tail page.\n\n[willy@infradead.org: fix build issue]\n  Link: https://lkml.kernel.org/r/ZeAKCyTn_xS3O9cE@casper.infradead.org\n[willy@infradead.org: fix __dump_folio]\n  Link: https://lkml.kernel.org/r/ZeJJegP8zM7S9GTy@casper.infradead.org\n[willy@infradead.org: fix pointer confusion]\n  Link: https://lkml.kernel.org/r/ZeYa00ixxC4k1ot-@casper.infradead.org\n[akpm@linux-foundation.org: s/printk/pr_warn/]\nLink: https://lkml.kernel.org/r/20240227192337.757313-5-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}],
        "proper_nouns": [{"PageFlags": "PageFlags are a set of flags associated with each memory page in the Linux kernel. These flags are used to indicate the state and characteristics of a page, such as whether it is locked, dirty, or reserved. They play a crucial role in memory management and are essential for various operations like page allocation, swapping, and file system operations."}, {"folio": "In the Linux kernel, a 'folio' is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages."}, {"dump_page()": "dump_page() is a function in the Linux kernel used for debugging purposes. It prints detailed information about a given memory page, including its flags, mapping, and other relevant attributes. This function is particularly useful for diagnosing memory-related issues and understanding the state of memory pages during runtime."}],
        "background_knowledge": ["The PageFlags in the Linux kernel are used to manage and track the state of memory pages. These flags are essential for various memory management operations, including page allocation, swapping, and file system operations.", "The introduction of 'folios' in the Linux kernel aims to improve the efficiency of memory management, especially for large files and operations that span multiple pages. Folios provide a higher-level abstraction that simplifies the management of memory pages.", "The dump_page() function is a debugging tool that provides detailed information about a memory page, which is crucial for diagnosing memory-related issues and understanding the state of memory pages during runtime."],
        "kernel_changes": ["Before the 'PageFlags cleanups' feature, the Linux kernel used a mix of page-specific flags and folio-specific flags, which could lead to inconsistencies and inefficiencies in memory management. The feature introduced several changes to streamline and optimize the use of PageFlags:", "1. Constification of page/folio flag tests: This change ensures that the flag tests are performed in a consistent and efficient manner, reducing the risk of errors and improving code readability.", "2. Removal of obsolete PageFlags: Several obsolete PageFlags, such as PageWaiters, PageSetWaiters, and PageClearWaiters, were removed as they were no longer necessary with the introduction of folios.", "3. Separation of FOLIO_FLAGS from PAGEFLAGS: By separating folio-specific flags from page-specific flags, the kernel prevents the reintroduction of obsolete flags and ensures that only relevant flags are used for each type of memory management operation.", "4. Making dump_page() take a const argument: This change enhances the safety and reliability of the dump_page() function by ensuring that it operates on a constant reference to a memory page, preventing unintended modifications."],
        "comprehensive_understanding": "The 'PageFlags cleanups' feature in the Linux kernel represents a significant improvement in the management and efficiency of memory pages. By constifying page/folio flag tests, removing obsolete PageFlags, separating FOLIO_FLAGS from PAGEFLAGS, and enhancing the dump_page() function, the feature ensures that memory management operations are performed in a consistent, efficient, and reliable manner. These changes not only improve the performance of the kernel but also enhance its maintainability and robustness, making it easier for developers to diagnose and resolve memory-related issues."
    }
}

2024-12-03 22:22:33,876 - extractor - DEBUG - deepseek response:
ChatCompletion(id='ce5494af-caa5-4594-9adc-a5e9d16c9189', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "concepts": ["PageFlags", "folio", "dump_page()", "memory management", "flag tests", "memory pages", "FOLIO_FLAGS", "PAGEFLAGS"]\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733235750, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=50, prompt_tokens=4125, total_tokens=4175, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=3392, prompt_cache_miss_tokens=733))
2024-12-03 22:22:33,876 - extractor - DEBUG - feature_extracted of feature_id=33235:
{
    "feature_id": 33235,
    "feature_description": "PageFlags cleanups",
    "version": "6.9",
    "commits": [
        {
            "commit_subject": "mm: constify testing page/folio flags",
            "commit_message": "Now that dump_page() takes a const argument, we can constify all the page\nflag tests.\n\nLink: https://lkml.kernel.org/r/20240227192337.757313-7-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nReviewed-by: David Hildenbrand <david@redhat.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm: constify more page/folio tests",
            "commit_message": "Constify the flag tests that aren't automatically generated and the tests\nthat look like flag tests but are more complicated.\n\nLink: https://lkml.kernel.org/r/20240227192337.757313-8-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nReviewed-by: David Hildenbrand <david@redhat.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm: remove PageWaiters, PageSetWaiters and PageClearWaiters",
            "commit_message": "All callers have been converted to use folios.  This was the only user of\nPF_ONLY_HEAD, so remove that too.\n\nLink: https://lkml.kernel.org/r/20240227192337.757313-3-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nReviewed-by: David Hildenbrand <david@redhat.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm: remove PageYoung and PageIdle definitions",
            "commit_message": "All callers have been converted to use folios, so remove the various\nset/clear/test functions defined on pages.\n\nLink: https://lkml.kernel.org/r/20240227192337.757313-4-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nReviewed-by: David Hildenbrand <david@redhat.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm: remove cast from page_to_nid()",
            "commit_message": "Now that PF_POISONED_CHECK() can take a const argument, we can drop\nthe cast.\n\nLink: https://lkml.kernel.org/r/20240227192337.757313-9-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nReviewed-by: David Hildenbrand <david@redhat.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm: separate out FOLIO_FLAGS from PAGEFLAGS",
            "commit_message": "Patch series \"PageFlags cleanups\".\n\nWe have now successfully removed all of the uses of some of the PageFlags\nfrom the kernel, but there's nothing to stop somebody reintroducing them. \nBy splitting out FOLIO_FLAGS from PAGEFLAGS, we can stop defining the old\nflags; and we do that in some of the later patches.\n\nAfter doing this, I realised that dump_page() was living dangerously; we\ncould end up calling folio_test_foo() on a pointer which no longer pointed\nto a folio (as dump_page() is not necessarily called when the caller has a\nreference to the page).  So I fixed that up.\n\nAnd then I realised that this was the key to making dump_page() take a\nconst argument, which means we can constify the page flags testing, which\nmeans we can remove more cast-away-the-const bad code.\n\nAnd here's where I ended up.\n\n\nThis patch (of 8):\n\nWe've progressed far enough with the folio transition that some flags are\nnow no longer checked on pages, but only on folios.  To prevent new users\nappearing, prepare to only define the folio versions of the flag\ntest/set/clear.\n\nLink: https://lkml.kernel.org/r/20240227192337.757313-1-willy@infradead.org\nLink: https://lkml.kernel.org/r/20240227192337.757313-2-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nReviewed-by: David Hildenbrand <david@redhat.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm: make dump_page() take a const argument",
            "commit_message": "Now that __dump_page() takes a const argument, we can make dump_page()\ntake a const struct page too.\n\nLink: https://lkml.kernel.org/r/20240227192337.757313-6-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nReviewed-by: David Hildenbrand <david@redhat.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm: add __dump_folio()",
            "commit_message": "Turn __dump_page() into a wrapper around __dump_folio().  Snapshot the\npage & folio into a stack variable so we don't hit BUG_ON() if an\nallocation is freed under us and what was a folio pointer becomes a\npointer to a tail page.\n\n[willy@infradead.org: fix build issue]\n  Link: https://lkml.kernel.org/r/ZeAKCyTn_xS3O9cE@casper.infradead.org\n[willy@infradead.org: fix __dump_folio]\n  Link: https://lkml.kernel.org/r/ZeJJegP8zM7S9GTy@casper.infradead.org\n[willy@infradead.org: fix pointer confusion]\n  Link: https://lkml.kernel.org/r/ZeYa00ixxC4k1ot-@casper.infradead.org\n[akpm@linux-foundation.org: s/printk/pr_warn/]\nLink: https://lkml.kernel.org/r/20240227192337.757313-5-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        }
    ],
    "proper_nouns": [
        {
            "PageFlags": "PageFlags are a set of flags associated with each memory page in the Linux kernel. These flags are used to indicate the state and characteristics of a page, such as whether it is locked, dirty, or reserved. They play a crucial role in memory management and are essential for various operations like page allocation, swapping, and file system operations."
        },
        {
            "folio": "In the Linux kernel, a 'folio' is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages."
        },
        {
            "dump_page()": "dump_page() is a function in the Linux kernel used for debugging purposes. It prints detailed information about a given memory page, including its flags, mapping, and other relevant attributes. This function is particularly useful for diagnosing memory-related issues and understanding the state of memory pages during runtime."
        }
    ],
    "background_knowledge": [
        "The PageFlags in the Linux kernel are used to manage and track the state of memory pages. These flags are essential for various memory management operations, including page allocation, swapping, and file system operations.",
        "The introduction of 'folios' in the Linux kernel aims to improve the efficiency of memory management, especially for large files and operations that span multiple pages. Folios provide a higher-level abstraction that simplifies the management of memory pages.",
        "The dump_page() function is a debugging tool that provides detailed information about a memory page, which is crucial for diagnosing memory-related issues and understanding the state of memory pages during runtime."
    ],
    "kernel_changes": [
        "Before the 'PageFlags cleanups' feature, the Linux kernel used a mix of page-specific flags and folio-specific flags, which could lead to inconsistencies and inefficiencies in memory management. The feature introduced several changes to streamline and optimize the use of PageFlags:",
        "1. Constification of page/folio flag tests: This change ensures that the flag tests are performed in a consistent and efficient manner, reducing the risk of errors and improving code readability.",
        "2. Removal of obsolete PageFlags: Several obsolete PageFlags, such as PageWaiters, PageSetWaiters, and PageClearWaiters, were removed as they were no longer necessary with the introduction of folios.",
        "3. Separation of FOLIO_FLAGS from PAGEFLAGS: By separating folio-specific flags from page-specific flags, the kernel prevents the reintroduction of obsolete flags and ensures that only relevant flags are used for each type of memory management operation.",
        "4. Making dump_page() take a const argument: This change enhances the safety and reliability of the dump_page() function by ensuring that it operates on a constant reference to a memory page, preventing unintended modifications."
    ],
    "comprehensive_understanding": "The 'PageFlags cleanups' feature in the Linux kernel represents a significant improvement in the management and efficiency of memory pages. By constifying page/folio flag tests, removing obsolete PageFlags, separating FOLIO_FLAGS from PAGEFLAGS, and enhancing the dump_page() function, the feature ensures that memory management operations are performed in a consistent, efficient, and reliable manner. These changes not only improve the performance of the kernel but also enhance its maintainability and robustness, making it easier for developers to diagnose and resolve memory-related issues.",
    "concepts": [
        "PageFlags",
        "folio",
        "dump_page()",
        "memory management",
        "flag tests",
        "memory pages",
        "FOLIO_FLAGS",
        "PAGEFLAGS"
    ]
}
2024-12-03 22:22:33,877 - extractor - INFO - ---------Processing feature 6/815---------
2024-12-03 22:23:54,164 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个 linux 内核的专家。给你一个 linux 内核特性的描述以及其对应的若干 commit 的标题和提交信息，你的任务是为这个特性写一个多方面的扩写版描述。你的扩写需包含以下几个方面：1. 该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)；2. 为了让开发者理解该特性，需要哪些具体的，与这个特性直接相关的背景知识 (background_knowledge)；3. 该特性被实现前后，linux 内核发生了怎样的变化 (kernel_changes)；4. 你对该特性的综合理解 (comprehensive_understanding)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}]
        },
        "output": {
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
        }
    ],
    "input": {
        "feature_description": "Rearrange batched folio freeing",
        "commits": [{"commit_subject": "memcg: remove mem_cgroup_uncharge_list()", "commit_message": "All users have been converted to mem_cgroup_uncharge_folios() so we can\nremove this API.\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-14-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: remove lru_to_page()", "commit_message": "The last user was removed over a year ago; remove the definition.\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-16-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: use a folio in __collapse_huge_page_copy_succeeded()", "commit_message": "These pages are all chained together through the lru list, so we know\nthey're folios.  Use the folio APIs to save three hidden calls to\ncompound_head().\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-18-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: remove use of folio list from folios_put()", "commit_message": "Instead of putting the interesting folios on a list, delete the\nuninteresting one from the folio_batch.\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-7-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: use free_unref_folios() in put_pages_list()", "commit_message": "Break up the list of folios into batches here so that the folios are more\nlikely to be cache hot when doing the rest of the processing.\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-8-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: convert free_pages_and_swap_cache() to use folios_put()", "commit_message": "Process the pages in batch-sized quantities instead of all-at-once.\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-17-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: free folios in a batch in shrink_folio_list()", "commit_message": "Use free_unref_page_batch() to free the folios.  This may increase the\nnumber of IPIs from calling try_to_unmap_flush() more often, but that's\ngoing to be very workload-dependent.  It may even reduce the number of\nIPIs as we now batch-free large folios instead of freeing them one at a\ntime.\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-12-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Mel Gorman <mgorman@suse.de>\nCc: David Hildenbrand <david@redhat.com>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: convert free_unref_page_list() to use folios", "commit_message": "Most of its callees are not yet ready to accept a folio, but we know all\nof the pages passed in are actually folios because they're linked through\n->lru.\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-3-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: free folios directly in move_folios_to_lru()", "commit_message": "The few folios which can't be moved to the LRU list (because their\nrefcount dropped to zero) used to be returned to the caller to dispose of.\nMake this simpler to call by freeing the folios directly through\nfree_unref_folios().\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-13-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: remove free_unref_page_list()", "commit_message": "All callers now use free_unref_folios() so we can delete this function.\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-15-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: allow non-hugetlb large folios to be batch processed", "commit_message": "Hugetlb folios still get special treatment, but normal large folios can\nnow be freed by free_unref_folios().  This should have a reasonable\nperformance impact, TBD.\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-11-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: add free_unref_folios()", "commit_message": "Iterate over a folio_batch rather than a linked list.  This is easier for\nthe CPU to prefetch and has a batch count naturally built in so we don't\nneed to track it.  Again, this lowers the maximum lock hold time from\n32 folios to 15, but I do not expect this to have a significant effect.\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-4-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: make folios_put() the basis of release_pages()", "commit_message": "Patch series \"Rearrange batched folio freeing\", v3.\n\nOther than the obvious \"remove calls to compound_head\" changes, the\nfundamental belief here is that iterating a linked list is much slower\nthan iterating an array (5-15x slower in my testing).  There's also an\nassociated belief that since we iterate the batch of folios three times,\nwe do better when the array is small (ie 15 entries) than we do with a\nbatch that is hundreds of entries long, which only gives us the\nopportunity for the first pages to fall out of cache by the time we get to\nthe end.\n\nIt is possible we should increase the size of folio_batch.  Hopefully the\nbots let us know if this introduces any performance regressions.\n\n\nThis patch (of 3):\n\nBy making release_pages() call folios_put(), we can get rid of the calls\nto compound_head() for the callers that already know they have folios.  We\ncan also get rid of the lock_batch tracking as we know the size of the\nbatch is limited by folio_batch.  This does reduce the maximum number of\npages for which the lruvec lock is held, from SWAP_CLUSTER_MAX (32) to\nPAGEVEC_SIZE (15).  I do not expect this to make a significant difference,\nbut if it does, we can increase PAGEVEC_SIZE to 31.\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-1-willy@infradead.org\nLink: https://lkml.kernel.org/r/20240227174254.710559-2-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: use __page_cache_release() in folios_put()", "commit_message": "Pass a pointer to the lruvec so we can take advantage of the\nfolio_lruvec_relock_irqsave().  Adjust the calling convention of\nfolio_lruvec_relock_irqsave() to suit and add a page_cache_release()\nwrapper.\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-9-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "memcg: add mem_cgroup_uncharge_folios()", "commit_message": "Almost identical to mem_cgroup_uncharge_list(), except it takes a\nfolio_batch instead of a list_head.\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-6-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: use folios_put() in __folio_batch_release()", "commit_message": "There's no need to indirect through release_pages() and iterate over this\nbatch of folios an extra time; we can just use the batch that we have.\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-5-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: convert free_swap_cache() to take a folio", "commit_message": "All but one caller already has a folio, so convert\nfree_page_and_swap_cache() to have a folio and remove the call to\npage_folio().\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-19-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nReviewed-by: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: handle large folios in free_unref_folios()", "commit_message": "Call folio_undo_large_rmappable() if needed.  free_unref_page_prepare()\ndestroys the ability to call folio_order(), so stash the order in\nfolio->private for the benefit of the second loop.\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-10-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}]
    }
}

2024-12-03 22:24:38,185 - extractor - DEBUG - deepseek response:
ChatCompletion(id='4ae87372-f54f-4b4c-a7a3-ecff59c3751c', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "proper_nouns": [\n        {\n            "folio": "In the Linux kernel, a \'folio\' is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages."\n        },\n        {\n            "folio_batch": "A \'folio_batch\' is a mechanism used in the Linux kernel to handle a batch of folios, which are higher-level abstractions of memory pages. This batching approach is designed to optimize memory management operations by processing multiple folios together, thereby reducing the overhead associated with individual page operations."\n        },\n        {\n            "LRU (Least Recently Used) list": "The \'LRU list\' is a data structure in the Linux kernel used for managing memory pages based on their usage patterns. Pages that are least recently used are moved to the end of the list, making it easier to reclaim memory by evicting these less frequently used pages first."\n        },\n        {\n            "IPI (Inter-Processor Interrupt)": "An \'IPI\' is a mechanism used in multiprocessor systems to send an interrupt to another processor. In the context of memory management, IPIs are used to synchronize operations across different processors, ensuring that memory operations are consistent and coherent across the system."\n        }\n    ],\n    "background_knowledge": [\n        "The Linux kernel manages memory through various data structures and algorithms, including the use of folios and folio_batches to optimize memory management operations. One of the key challenges in memory management is efficiently reclaiming memory, which involves identifying and freeing unused or less frequently used pages. The LRU list is a critical component in this process, as it helps in identifying which pages can be safely reclaimed.",\n        "Batch processing of folios is an optimization technique that aims to reduce the overhead associated with individual page operations. By processing multiple folios together, the kernel can achieve better performance and reduce the number of IPIs required for synchronization across multiple processors."\n    ],\n    "kernel_changes": [\n        "Before the implementation of the \'Rearrange batched folio freeing\' feature, the Linux kernel processed folios individually, which could lead to inefficiencies, especially when dealing with large numbers of pages. The introduction of folio_batches allowed for the batch processing of folios, which improved the efficiency of memory management operations.",\n        "The feature introduced several changes to the kernel, including the removal of obsolete APIs like \'mem_cgroup_uncharge_list()\' and \'lru_to_page()\', which were no longer necessary due to the new batch processing mechanisms. Additionally, the kernel was updated to use folio-based APIs more extensively, such as \'free_unref_folios()\' and \'folios_put()\', which were designed to handle folios in batches.",\n        "The changes also involved optimizing the handling of large folios, ensuring that they could be processed efficiently within the batch framework. This included modifications to functions like \'free_unref_folios()\' to handle large folios appropriately, thereby improving the overall performance of memory reclamation and management."\n    ],\n    "comprehensive_understanding": "The \'Rearrange batched folio freeing\' feature is a significant optimization in the Linux kernel\'s memory management system. By introducing the concept of folio_batches and batch processing of folios, the feature aims to reduce the overhead associated with individual page operations and improve the efficiency of memory reclamation. This is achieved through a series of changes that include the removal of obsolete APIs, the introduction of new folio-based APIs, and the optimization of large folio handling. The result is a more efficient and streamlined memory management system that can better handle the demands of modern workloads, particularly those involving large files and extensive memory usage."\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733235834, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=835, prompt_tokens=5663, total_tokens=6498, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=5632, prompt_cache_miss_tokens=31))
2024-12-03 22:24:38,186 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个命名实体抽取 (NER) 任务的专家，同时也是 linux 内核的专家。给你一个 linux 内核特性的描述 (feature_description)、对应的若干 commit 的标题和提交信息 (commits)、该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)、为了让开发者理解该特性所需的与这个特性直接相关的背景知识 (background_knowledge)、该特性被实现前后，linux 内核发生的变化 (kernel_changes)、对该特性的综合理解 (comprehensive_understanding)。你的任务是：根据这些信息提取出在 linux 内核中涉及到的和这个特性相关的概念实体 (concepts)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}],
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": [
                "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
            ]
        },
        "output": {
            "concepts": ["Copy-on-Write (CoW)", "mTHP", "folio", "memory reclamation", "memory availability", "anti-fragmentation"]
        }
    ],
    "input": {
        "feature_description": "Rearrange batched folio freeing",
        "commits": [{"commit_subject": "memcg: remove mem_cgroup_uncharge_list()", "commit_message": "All users have been converted to mem_cgroup_uncharge_folios() so we can\nremove this API.\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-14-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: remove lru_to_page()", "commit_message": "The last user was removed over a year ago; remove the definition.\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-16-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: use a folio in __collapse_huge_page_copy_succeeded()", "commit_message": "These pages are all chained together through the lru list, so we know\nthey're folios.  Use the folio APIs to save three hidden calls to\ncompound_head().\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-18-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: remove use of folio list from folios_put()", "commit_message": "Instead of putting the interesting folios on a list, delete the\nuninteresting one from the folio_batch.\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-7-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: use free_unref_folios() in put_pages_list()", "commit_message": "Break up the list of folios into batches here so that the folios are more\nlikely to be cache hot when doing the rest of the processing.\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-8-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: convert free_pages_and_swap_cache() to use folios_put()", "commit_message": "Process the pages in batch-sized quantities instead of all-at-once.\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-17-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: free folios in a batch in shrink_folio_list()", "commit_message": "Use free_unref_page_batch() to free the folios.  This may increase the\nnumber of IPIs from calling try_to_unmap_flush() more often, but that's\ngoing to be very workload-dependent.  It may even reduce the number of\nIPIs as we now batch-free large folios instead of freeing them one at a\ntime.\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-12-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Mel Gorman <mgorman@suse.de>\nCc: David Hildenbrand <david@redhat.com>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: convert free_unref_page_list() to use folios", "commit_message": "Most of its callees are not yet ready to accept a folio, but we know all\nof the pages passed in are actually folios because they're linked through\n->lru.\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-3-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: free folios directly in move_folios_to_lru()", "commit_message": "The few folios which can't be moved to the LRU list (because their\nrefcount dropped to zero) used to be returned to the caller to dispose of.\nMake this simpler to call by freeing the folios directly through\nfree_unref_folios().\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-13-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: remove free_unref_page_list()", "commit_message": "All callers now use free_unref_folios() so we can delete this function.\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-15-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: allow non-hugetlb large folios to be batch processed", "commit_message": "Hugetlb folios still get special treatment, but normal large folios can\nnow be freed by free_unref_folios().  This should have a reasonable\nperformance impact, TBD.\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-11-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: add free_unref_folios()", "commit_message": "Iterate over a folio_batch rather than a linked list.  This is easier for\nthe CPU to prefetch and has a batch count naturally built in so we don't\nneed to track it.  Again, this lowers the maximum lock hold time from\n32 folios to 15, but I do not expect this to have a significant effect.\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-4-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: make folios_put() the basis of release_pages()", "commit_message": "Patch series \"Rearrange batched folio freeing\", v3.\n\nOther than the obvious \"remove calls to compound_head\" changes, the\nfundamental belief here is that iterating a linked list is much slower\nthan iterating an array (5-15x slower in my testing).  There's also an\nassociated belief that since we iterate the batch of folios three times,\nwe do better when the array is small (ie 15 entries) than we do with a\nbatch that is hundreds of entries long, which only gives us the\nopportunity for the first pages to fall out of cache by the time we get to\nthe end.\n\nIt is possible we should increase the size of folio_batch.  Hopefully the\nbots let us know if this introduces any performance regressions.\n\n\nThis patch (of 3):\n\nBy making release_pages() call folios_put(), we can get rid of the calls\nto compound_head() for the callers that already know they have folios.  We\ncan also get rid of the lock_batch tracking as we know the size of the\nbatch is limited by folio_batch.  This does reduce the maximum number of\npages for which the lruvec lock is held, from SWAP_CLUSTER_MAX (32) to\nPAGEVEC_SIZE (15).  I do not expect this to make a significant difference,\nbut if it does, we can increase PAGEVEC_SIZE to 31.\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-1-willy@infradead.org\nLink: https://lkml.kernel.org/r/20240227174254.710559-2-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: use __page_cache_release() in folios_put()", "commit_message": "Pass a pointer to the lruvec so we can take advantage of the\nfolio_lruvec_relock_irqsave().  Adjust the calling convention of\nfolio_lruvec_relock_irqsave() to suit and add a page_cache_release()\nwrapper.\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-9-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "memcg: add mem_cgroup_uncharge_folios()", "commit_message": "Almost identical to mem_cgroup_uncharge_list(), except it takes a\nfolio_batch instead of a list_head.\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-6-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: use folios_put() in __folio_batch_release()", "commit_message": "There's no need to indirect through release_pages() and iterate over this\nbatch of folios an extra time; we can just use the batch that we have.\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-5-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: convert free_swap_cache() to take a folio", "commit_message": "All but one caller already has a folio, so convert\nfree_page_and_swap_cache() to have a folio and remove the call to\npage_folio().\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-19-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nReviewed-by: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: handle large folios in free_unref_folios()", "commit_message": "Call folio_undo_large_rmappable() if needed.  free_unref_page_prepare()\ndestroys the ability to call folio_order(), so stash the order in\nfolio->private for the benefit of the second loop.\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-10-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}],
        "proper_nouns": [{"folio": "In the Linux kernel, a 'folio' is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages."}, {"folio_batch": "A 'folio_batch' is a mechanism used in the Linux kernel to handle a batch of folios, which are higher-level abstractions of memory pages. This batching approach is designed to optimize memory management operations by processing multiple folios together, thereby reducing the overhead associated with individual page operations."}, {"LRU (Least Recently Used) list": "The 'LRU list' is a data structure in the Linux kernel used for managing memory pages based on their usage patterns. Pages that are least recently used are moved to the end of the list, making it easier to reclaim memory by evicting these less frequently used pages first."}, {"IPI (Inter-Processor Interrupt)": "An 'IPI' is a mechanism used in multiprocessor systems to send an interrupt to another processor. In the context of memory management, IPIs are used to synchronize operations across different processors, ensuring that memory operations are consistent and coherent across the system."}],
        "background_knowledge": ["The Linux kernel manages memory through various data structures and algorithms, including the use of folios and folio_batches to optimize memory management operations. One of the key challenges in memory management is efficiently reclaiming memory, which involves identifying and freeing unused or less frequently used pages. The LRU list is a critical component in this process, as it helps in identifying which pages can be safely reclaimed.", "Batch processing of folios is an optimization technique that aims to reduce the overhead associated with individual page operations. By processing multiple folios together, the kernel can achieve better performance and reduce the number of IPIs required for synchronization across multiple processors."],
        "kernel_changes": ["Before the implementation of the 'Rearrange batched folio freeing' feature, the Linux kernel processed folios individually, which could lead to inefficiencies, especially when dealing with large numbers of pages. The introduction of folio_batches allowed for the batch processing of folios, which improved the efficiency of memory management operations.", "The feature introduced several changes to the kernel, including the removal of obsolete APIs like 'mem_cgroup_uncharge_list()' and 'lru_to_page()', which were no longer necessary due to the new batch processing mechanisms. Additionally, the kernel was updated to use folio-based APIs more extensively, such as 'free_unref_folios()' and 'folios_put()', which were designed to handle folios in batches.", "The changes also involved optimizing the handling of large folios, ensuring that they could be processed efficiently within the batch framework. This included modifications to functions like 'free_unref_folios()' to handle large folios appropriately, thereby improving the overall performance of memory reclamation and management."],
        "comprehensive_understanding": "The 'Rearrange batched folio freeing' feature is a significant optimization in the Linux kernel's memory management system. By introducing the concept of folio_batches and batch processing of folios, the feature aims to reduce the overhead associated with individual page operations and improve the efficiency of memory reclamation. This is achieved through a series of changes that include the removal of obsolete APIs, the introduction of new folio-based APIs, and the optimization of large folio handling. The result is a more efficient and streamlined memory management system that can better handle the demands of modern workloads, particularly those involving large files and extensive memory usage."
    }
}

2024-12-03 22:24:42,443 - extractor - DEBUG - deepseek response:
ChatCompletion(id='9d9dc69d-6287-4986-af8c-406d0ecd421c', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "concepts": ["folio", "folio_batch", "LRU (Least Recently Used) list", "IPI (Inter-Processor Interrupt)", "memory reclamation", "batch processing", "memory management", "memory efficiency", "large folios"]\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733235878, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=63, prompt_tokens=6499, total_tokens=6562, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=5760, prompt_cache_miss_tokens=739))
2024-12-03 22:24:42,443 - extractor - DEBUG - feature_extracted of feature_id=33234:
{
    "feature_id": 33234,
    "feature_description": "Rearrange batched folio freeing",
    "version": "6.9",
    "commits": [
        {
            "commit_subject": "memcg: remove mem_cgroup_uncharge_list()",
            "commit_message": "All users have been converted to mem_cgroup_uncharge_folios() so we can\nremove this API.\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-14-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm: remove lru_to_page()",
            "commit_message": "The last user was removed over a year ago; remove the definition.\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-16-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm: use a folio in __collapse_huge_page_copy_succeeded()",
            "commit_message": "These pages are all chained together through the lru list, so we know\nthey're folios.  Use the folio APIs to save three hidden calls to\ncompound_head().\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-18-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm: remove use of folio list from folios_put()",
            "commit_message": "Instead of putting the interesting folios on a list, delete the\nuninteresting one from the folio_batch.\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-7-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm: use free_unref_folios() in put_pages_list()",
            "commit_message": "Break up the list of folios into batches here so that the folios are more\nlikely to be cache hot when doing the rest of the processing.\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-8-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm: convert free_pages_and_swap_cache() to use folios_put()",
            "commit_message": "Process the pages in batch-sized quantities instead of all-at-once.\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-17-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm: free folios in a batch in shrink_folio_list()",
            "commit_message": "Use free_unref_page_batch() to free the folios.  This may increase the\nnumber of IPIs from calling try_to_unmap_flush() more often, but that's\ngoing to be very workload-dependent.  It may even reduce the number of\nIPIs as we now batch-free large folios instead of freeing them one at a\ntime.\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-12-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Mel Gorman <mgorman@suse.de>\nCc: David Hildenbrand <david@redhat.com>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm: convert free_unref_page_list() to use folios",
            "commit_message": "Most of its callees are not yet ready to accept a folio, but we know all\nof the pages passed in are actually folios because they're linked through\n->lru.\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-3-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm: free folios directly in move_folios_to_lru()",
            "commit_message": "The few folios which can't be moved to the LRU list (because their\nrefcount dropped to zero) used to be returned to the caller to dispose of.\nMake this simpler to call by freeing the folios directly through\nfree_unref_folios().\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-13-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm: remove free_unref_page_list()",
            "commit_message": "All callers now use free_unref_folios() so we can delete this function.\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-15-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm: allow non-hugetlb large folios to be batch processed",
            "commit_message": "Hugetlb folios still get special treatment, but normal large folios can\nnow be freed by free_unref_folios().  This should have a reasonable\nperformance impact, TBD.\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-11-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm: add free_unref_folios()",
            "commit_message": "Iterate over a folio_batch rather than a linked list.  This is easier for\nthe CPU to prefetch and has a batch count naturally built in so we don't\nneed to track it.  Again, this lowers the maximum lock hold time from\n32 folios to 15, but I do not expect this to have a significant effect.\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-4-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm: make folios_put() the basis of release_pages()",
            "commit_message": "Patch series \"Rearrange batched folio freeing\", v3.\n\nOther than the obvious \"remove calls to compound_head\" changes, the\nfundamental belief here is that iterating a linked list is much slower\nthan iterating an array (5-15x slower in my testing).  There's also an\nassociated belief that since we iterate the batch of folios three times,\nwe do better when the array is small (ie 15 entries) than we do with a\nbatch that is hundreds of entries long, which only gives us the\nopportunity for the first pages to fall out of cache by the time we get to\nthe end.\n\nIt is possible we should increase the size of folio_batch.  Hopefully the\nbots let us know if this introduces any performance regressions.\n\n\nThis patch (of 3):\n\nBy making release_pages() call folios_put(), we can get rid of the calls\nto compound_head() for the callers that already know they have folios.  We\ncan also get rid of the lock_batch tracking as we know the size of the\nbatch is limited by folio_batch.  This does reduce the maximum number of\npages for which the lruvec lock is held, from SWAP_CLUSTER_MAX (32) to\nPAGEVEC_SIZE (15).  I do not expect this to make a significant difference,\nbut if it does, we can increase PAGEVEC_SIZE to 31.\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-1-willy@infradead.org\nLink: https://lkml.kernel.org/r/20240227174254.710559-2-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm: use __page_cache_release() in folios_put()",
            "commit_message": "Pass a pointer to the lruvec so we can take advantage of the\nfolio_lruvec_relock_irqsave().  Adjust the calling convention of\nfolio_lruvec_relock_irqsave() to suit and add a page_cache_release()\nwrapper.\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-9-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "memcg: add mem_cgroup_uncharge_folios()",
            "commit_message": "Almost identical to mem_cgroup_uncharge_list(), except it takes a\nfolio_batch instead of a list_head.\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-6-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm: use folios_put() in __folio_batch_release()",
            "commit_message": "There's no need to indirect through release_pages() and iterate over this\nbatch of folios an extra time; we can just use the batch that we have.\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-5-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm: convert free_swap_cache() to take a folio",
            "commit_message": "All but one caller already has a folio, so convert\nfree_page_and_swap_cache() to have a folio and remove the call to\npage_folio().\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-19-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nReviewed-by: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm: handle large folios in free_unref_folios()",
            "commit_message": "Call folio_undo_large_rmappable() if needed.  free_unref_page_prepare()\ndestroys the ability to call folio_order(), so stash the order in\nfolio->private for the benefit of the second loop.\n\nLink: https://lkml.kernel.org/r/20240227174254.710559-10-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nCc: Mel Gorman <mgorman@suse.de>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        }
    ],
    "proper_nouns": [
        {
            "folio": "In the Linux kernel, a 'folio' is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages."
        },
        {
            "folio_batch": "A 'folio_batch' is a mechanism used in the Linux kernel to handle a batch of folios, which are higher-level abstractions of memory pages. This batching approach is designed to optimize memory management operations by processing multiple folios together, thereby reducing the overhead associated with individual page operations."
        },
        {
            "LRU (Least Recently Used) list": "The 'LRU list' is a data structure in the Linux kernel used for managing memory pages based on their usage patterns. Pages that are least recently used are moved to the end of the list, making it easier to reclaim memory by evicting these less frequently used pages first."
        },
        {
            "IPI (Inter-Processor Interrupt)": "An 'IPI' is a mechanism used in multiprocessor systems to send an interrupt to another processor. In the context of memory management, IPIs are used to synchronize operations across different processors, ensuring that memory operations are consistent and coherent across the system."
        }
    ],
    "background_knowledge": [
        "The Linux kernel manages memory through various data structures and algorithms, including the use of folios and folio_batches to optimize memory management operations. One of the key challenges in memory management is efficiently reclaiming memory, which involves identifying and freeing unused or less frequently used pages. The LRU list is a critical component in this process, as it helps in identifying which pages can be safely reclaimed.",
        "Batch processing of folios is an optimization technique that aims to reduce the overhead associated with individual page operations. By processing multiple folios together, the kernel can achieve better performance and reduce the number of IPIs required for synchronization across multiple processors."
    ],
    "kernel_changes": [
        "Before the implementation of the 'Rearrange batched folio freeing' feature, the Linux kernel processed folios individually, which could lead to inefficiencies, especially when dealing with large numbers of pages. The introduction of folio_batches allowed for the batch processing of folios, which improved the efficiency of memory management operations.",
        "The feature introduced several changes to the kernel, including the removal of obsolete APIs like 'mem_cgroup_uncharge_list()' and 'lru_to_page()', which were no longer necessary due to the new batch processing mechanisms. Additionally, the kernel was updated to use folio-based APIs more extensively, such as 'free_unref_folios()' and 'folios_put()', which were designed to handle folios in batches.",
        "The changes also involved optimizing the handling of large folios, ensuring that they could be processed efficiently within the batch framework. This included modifications to functions like 'free_unref_folios()' to handle large folios appropriately, thereby improving the overall performance of memory reclamation and management."
    ],
    "comprehensive_understanding": "The 'Rearrange batched folio freeing' feature is a significant optimization in the Linux kernel's memory management system. By introducing the concept of folio_batches and batch processing of folios, the feature aims to reduce the overhead associated with individual page operations and improve the efficiency of memory reclamation. This is achieved through a series of changes that include the removal of obsolete APIs, the introduction of new folio-based APIs, and the optimization of large folio handling. The result is a more efficient and streamlined memory management system that can better handle the demands of modern workloads, particularly those involving large files and extensive memory usage.",
    "concepts": [
        "folio",
        "folio_batch",
        "LRU (Least Recently Used) list",
        "IPI (Inter-Processor Interrupt)",
        "memory reclamation",
        "batch processing",
        "memory management",
        "memory efficiency",
        "large folios"
    ]
}
2024-12-03 22:24:42,445 - extractor - INFO - ---------Processing feature 7/815---------
2024-12-03 22:25:10,456 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个 linux 内核的专家。给你一个 linux 内核特性的描述以及其对应的若干 commit 的标题和提交信息，你的任务是为这个特性写一个多方面的扩写版描述。你的扩写需包含以下几个方面：1. 该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)；2. 为了让开发者理解该特性，需要哪些具体的，与这个特性直接相关的背景知识 (background_knowledge)；3. 该特性被实现前后，linux 内核发生了怎样的变化 (kernel_changes)；4. 你对该特性的综合理解 (comprehensive_understanding)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}]
        },
        "output": {
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
        }
    ],
    "input": {
        "feature_description": "Split a folio to any lower order folios",
        "commits": [{"commit_subject": "mm: thp: split huge page to any lower order pages", "commit_message": "To split a THP to any lower order pages, we need to reform THPs on\nsubpages at given order and add page refcount based on the new page order.\nAlso we need to reinitialize page_deferred_list after removing the page\nfrom the split_queue, otherwise a subsequent split will see list\ncorruption when checking the page_deferred_list again.\n\nNote: Anonymous order-1 folio is not supported because _deferred_list,\nwhich is used by partially mapped folios, is stored in subpage 2 and an\norder-1 folio only has subpage 0 and 1.  File-backed order-1 folios are\nfine, since they do not use _deferred_list.\n\n[ziy@nvidia.com: fixup per discussion with Ryan]\n  Link: https://lkml.kernel.org/r/494F48CD-1F0F-4CAD-884E-6D48F40AF990@nvidia.com\nLink: https://lkml.kernel.org/r/20240226205534.1603748-8-zi.yan@sent.com\nSigned-off-by: Zi Yan <ziy@nvidia.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Hugh Dickins <hughd@google.com>\nCc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>\nCc: Luis Chamberlain <mcgrof@kernel.org>\nCc: \"Matthew Wilcox (Oracle)\" <willy@infradead.org>\nCc: Michal Koutny <mkoutny@suse.com>\nCc: Roman Gushchin <roman.gushchin@linux.dev>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Yu Zhao <yuzhao@google.com>\nCc: Zach O'Keefe <zokeefe@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: support order-1 folios in the page cache", "commit_message": "Folios of order 1 have no space to store the deferred list.  This is not a\nproblem for the page cache as file-backed folios are never placed on the\ndeferred list.  All we need to do is prevent the core MM from touching the\ndeferred list for order 1 folios and remove the code which prevented us\nfrom allocating order 1 folios.\n\nLink: https://lore.kernel.org/linux-mm/90344ea7-4eec-47ee-5996-0c22f42d6a6a@google.com/\nLink: https://lkml.kernel.org/r/20240226205534.1603748-3-zi.yan@sent.com\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nSigned-off-by: Zi Yan <ziy@nvidia.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Hugh Dickins <hughd@google.com>\nCc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>\nCc: Luis Chamberlain <mcgrof@kernel.org>\nCc: Michal Koutny <mkoutny@suse.com>\nCc: Roman Gushchin <roman.gushchin@linux.dev>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Yu Zhao <yuzhao@google.com>\nCc: Zach O'Keefe <zokeefe@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/memcg: use order instead of nr in split_page_memcg()", "commit_message": "We do not have non power of two pages, using nr is error prone if nr is\nnot power-of-two.  Use page order instead.\n\nLink: https://lkml.kernel.org/r/20240226205534.1603748-4-zi.yan@sent.com\nSigned-off-by: Zi Yan <ziy@nvidia.com>\nAcked-by: David Hildenbrand <david@redhat.com>\nCc: Hugh Dickins <hughd@google.com>\nCc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>\nCc: Luis Chamberlain <mcgrof@kernel.org>\nCc: \"Matthew Wilcox (Oracle)\" <willy@infradead.org>\nCc: Michal Koutny <mkoutny@suse.com>\nCc: Roman Gushchin <roman.gushchin@linux.dev>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Yu Zhao <yuzhao@google.com>\nCc: Zach O'Keefe <zokeefe@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/huge_memory: only split PMD mapping when necessary in unmap_folio()", "commit_message": "Patch series \"Split a folio to any lower order folios\", v5.\n\nFile folio supports any order and multi-size THP is upstreamed[1], so both\nfile and anonymous folios can be >0 order.  Currently, split_huge_page()\nonly splits a huge page to order-0 pages, but splitting to orders higher\nthan 0 might better utilize large folios, if done properly.  In addition,\nLarge Block Sizes in XFS support would benefit from it during truncate[2].\nThis patchset adds support for splitting a large folio to any lower order\nfolios.\n\nIn addition to this implementation of split_huge_page_to_list_to_order(),\na possible optimization could be splitting a large folio to arbitrary\nsmaller folios instead of a single order.  As both Hugh and Ryan pointed\nout [3,5] that split to a single order might not be optimal, an order-9\nfolio might be better split into 1 order-8, 1 order-7, ..., 1 order-1, and\n2 order-0 folios, depending on subsequent folio operations.  Leave this as\nfuture work.\n\n[1] https://lore.kernel.org/all/20231207161211.2374093-1-ryan.roberts@arm.com/\n[2] https://lore.kernel.org/linux-mm/20240226094936.2677493-1-kernel@pankajraghav.com/\n[3] https://lore.kernel.org/linux-mm/9dd96da-efa2-5123-20d4-4992136ef3ad@google.com/\n[4] https://lore.kernel.org/linux-mm/cbb1d6a0-66dd-47d0-8733-f836fe050374@arm.com/\n[5] https://lore.kernel.org/linux-mm/20240213215520.1048625-1-zi.yan@sent.com/\n\n\nThis patch (of 8):\n\nAs multi-size THP support is added, not all THPs are PMD-mapped, thus\nduring a huge page split, there is no need to always split PMD mapping in\nunmap_folio().  Make it conditional.\n\nLink: https://lkml.kernel.org/r/20240226205534.1603748-1-zi.yan@sent.com\nLink: https://lkml.kernel.org/r/20240226205534.1603748-2-zi.yan@sent.com\nSigned-off-by: Zi Yan <ziy@nvidia.com>\nReviewed-by: David Hildenbrand <david@redhat.com>\nCc: Hugh Dickins <hughd@google.com>\nCc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>\nCc: Luis Chamberlain <mcgrof@kernel.org>\nCc: Matthew Wilcox <willy@infradead.org>\nCc: Michal Koutny <mkoutny@suse.com>\nCc: Roman Gushchin <roman.gushchin@linux.dev>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Yu Zhao <yuzhao@google.com>\nCc: Zach O'Keefe <zokeefe@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/page_owner: use order instead of nr in split_page_owner()", "commit_message": "We do not have non power of two pages, using nr is error prone if nr is\nnot power-of-two.  Use page order instead.\n\nLink: https://lkml.kernel.org/r/20240226205534.1603748-5-zi.yan@sent.com\nSigned-off-by: Zi Yan <ziy@nvidia.com>\nAcked-by: David Hildenbrand <david@redhat.com>\nCc: Hugh Dickins <hughd@google.com>\nCc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>\nCc: Luis Chamberlain <mcgrof@kernel.org>\nCc: \"Matthew Wilcox (Oracle)\" <willy@infradead.org>\nCc: Michal Koutny <mkoutny@suse.com>\nCc: Roman Gushchin <roman.gushchin@linux.dev>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Yu Zhao <yuzhao@google.com>\nCc: Zach O'Keefe <zokeefe@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: memcg: make memcg huge page split support any order split", "commit_message": "It sets memcg information for the pages after the split.  A new parameter\nnew_order is added to tell the order of subpages in the new page, always 0\nfor now.  It prepares for upcoming changes to support split huge page to\nany lower order.\n\nLink: https://lkml.kernel.org/r/20240226205534.1603748-6-zi.yan@sent.com\nSigned-off-by: Zi Yan <ziy@nvidia.com>\nAcked-by: David Hildenbrand <david@redhat.com>\nCc: Hugh Dickins <hughd@google.com>\nCc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>\nCc: Luis Chamberlain <mcgrof@kernel.org>\nCc: \"Matthew Wilcox (Oracle)\" <willy@infradead.org>\nCc: Michal Koutny <mkoutny@suse.com>\nCc: Roman Gushchin <roman.gushchin@linux.dev>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Yu Zhao <yuzhao@google.com>\nCc: Zach O'Keefe <zokeefe@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}]
    }
}

2024-12-03 22:25:56,963 - extractor - DEBUG - deepseek response:
ChatCompletion(id='7488cd72-2369-4dc4-b39a-936aa9b5ea03', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "proper_nouns": [\n        {\n            "folio": "In the Linux kernel, a \'folio\' is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages."\n        },\n        {\n            "THP (Transparent Huge Pages)": "THP, or Transparent Huge Pages, is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture. THP automatically manages these huge pages, making them transparent to applications."\n        },\n        {\n            "PMD (Page Middle Directory)": "The Page Middle Directory (PMD) is a part of the Linux kernel\'s memory management system, specifically within the page table hierarchy. It sits between the Page Global Directory (PGD) and the Page Table Entry (PTE) in the multi-level page table structure. PMD is responsible for mapping large memory regions, often using huge pages, which can significantly reduce the overhead of memory management."\n        },\n        {\n            "memcg (Memory Control Group)": "Memory Control Group (memcg) is a feature in the Linux kernel that allows for the hierarchical management of memory resources. It enables the kernel to allocate and limit memory usage for groups of processes, providing better control over memory consumption and improving overall system stability."\n        }\n    ],\n    "background_knowledge": [\n        "The ability to split a large folio into smaller folios of any order is a significant enhancement in memory management. This capability is particularly useful in scenarios where large memory pages (such as THPs) need to be broken down into smaller units for more granular memory allocation and management.",\n        "Historically, the Linux kernel\'s memory management system was limited to splitting large pages into smaller pages of a fixed order (typically order-0 pages, which are 4KB each). This limitation could lead to inefficiencies, especially when dealing with large files or complex memory operations that required more flexible memory management."\n    ],\n    "kernel_changes": [\n        "Before the implementation of this feature, the Linux kernel could only split large folios into smaller pages of a fixed order, which often resulted in suboptimal memory usage. This limitation was particularly evident in scenarios involving large files or complex memory operations that required more flexible memory management.",\n        "After the implementation, the kernel gained the ability to split a large folio into smaller folios of any order. This change allowed for more granular memory allocation and management, improving memory utilization and reducing fragmentation. Specifically, the kernel can now split a large folio into smaller folios of varying sizes, depending on the specific memory requirements of the operation at hand.",\n        "Additionally, the implementation addressed issues related to the handling of order-1 folios, ensuring that they are properly managed without causing list corruption or other memory management issues. This enhancement is particularly important for file-backed folios, which do not use the deferred list and thus require different handling compared to anonymous folios."\n    ],\n    "comprehensive_understanding": "The feature of splitting a folio to any lower order folios represents a significant advancement in the Linux kernel\'s memory management capabilities. By enabling the kernel to split large folios into smaller folios of any order, this feature provides greater flexibility in memory allocation and management, leading to improved memory utilization and reduced fragmentation. This enhancement is particularly beneficial in scenarios involving large files or complex memory operations that require more granular control over memory resources. Additionally, the implementation addresses specific issues related to the handling of order-1 folios, ensuring that they are managed correctly without causing memory management issues. Overall, this feature enhances the Linux kernel\'s ability to efficiently manage memory resources, contributing to better system performance and stability."\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733235910, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=861, prompt_tokens=4522, total_tokens=5383, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=1472, prompt_cache_miss_tokens=3050))
2024-12-03 22:25:56,964 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个命名实体抽取 (NER) 任务的专家，同时也是 linux 内核的专家。给你一个 linux 内核特性的描述 (feature_description)、对应的若干 commit 的标题和提交信息 (commits)、该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)、为了让开发者理解该特性所需的与这个特性直接相关的背景知识 (background_knowledge)、该特性被实现前后，linux 内核发生的变化 (kernel_changes)、对该特性的综合理解 (comprehensive_understanding)。你的任务是：根据这些信息提取出在 linux 内核中涉及到的和这个特性相关的概念实体 (concepts)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}],
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": [
                "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
            ]
        },
        "output": {
            "concepts": ["Copy-on-Write (CoW)", "mTHP", "folio", "memory reclamation", "memory availability", "anti-fragmentation"]
        }
    ],
    "input": {
        "feature_description": "Split a folio to any lower order folios",
        "commits": [{"commit_subject": "mm: thp: split huge page to any lower order pages", "commit_message": "To split a THP to any lower order pages, we need to reform THPs on\nsubpages at given order and add page refcount based on the new page order.\nAlso we need to reinitialize page_deferred_list after removing the page\nfrom the split_queue, otherwise a subsequent split will see list\ncorruption when checking the page_deferred_list again.\n\nNote: Anonymous order-1 folio is not supported because _deferred_list,\nwhich is used by partially mapped folios, is stored in subpage 2 and an\norder-1 folio only has subpage 0 and 1.  File-backed order-1 folios are\nfine, since they do not use _deferred_list.\n\n[ziy@nvidia.com: fixup per discussion with Ryan]\n  Link: https://lkml.kernel.org/r/494F48CD-1F0F-4CAD-884E-6D48F40AF990@nvidia.com\nLink: https://lkml.kernel.org/r/20240226205534.1603748-8-zi.yan@sent.com\nSigned-off-by: Zi Yan <ziy@nvidia.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Hugh Dickins <hughd@google.com>\nCc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>\nCc: Luis Chamberlain <mcgrof@kernel.org>\nCc: \"Matthew Wilcox (Oracle)\" <willy@infradead.org>\nCc: Michal Koutny <mkoutny@suse.com>\nCc: Roman Gushchin <roman.gushchin@linux.dev>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Yu Zhao <yuzhao@google.com>\nCc: Zach O'Keefe <zokeefe@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: support order-1 folios in the page cache", "commit_message": "Folios of order 1 have no space to store the deferred list.  This is not a\nproblem for the page cache as file-backed folios are never placed on the\ndeferred list.  All we need to do is prevent the core MM from touching the\ndeferred list for order 1 folios and remove the code which prevented us\nfrom allocating order 1 folios.\n\nLink: https://lore.kernel.org/linux-mm/90344ea7-4eec-47ee-5996-0c22f42d6a6a@google.com/\nLink: https://lkml.kernel.org/r/20240226205534.1603748-3-zi.yan@sent.com\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nSigned-off-by: Zi Yan <ziy@nvidia.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Hugh Dickins <hughd@google.com>\nCc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>\nCc: Luis Chamberlain <mcgrof@kernel.org>\nCc: Michal Koutny <mkoutny@suse.com>\nCc: Roman Gushchin <roman.gushchin@linux.dev>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Yu Zhao <yuzhao@google.com>\nCc: Zach O'Keefe <zokeefe@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/memcg: use order instead of nr in split_page_memcg()", "commit_message": "We do not have non power of two pages, using nr is error prone if nr is\nnot power-of-two.  Use page order instead.\n\nLink: https://lkml.kernel.org/r/20240226205534.1603748-4-zi.yan@sent.com\nSigned-off-by: Zi Yan <ziy@nvidia.com>\nAcked-by: David Hildenbrand <david@redhat.com>\nCc: Hugh Dickins <hughd@google.com>\nCc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>\nCc: Luis Chamberlain <mcgrof@kernel.org>\nCc: \"Matthew Wilcox (Oracle)\" <willy@infradead.org>\nCc: Michal Koutny <mkoutny@suse.com>\nCc: Roman Gushchin <roman.gushchin@linux.dev>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Yu Zhao <yuzhao@google.com>\nCc: Zach O'Keefe <zokeefe@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/huge_memory: only split PMD mapping when necessary in unmap_folio()", "commit_message": "Patch series \"Split a folio to any lower order folios\", v5.\n\nFile folio supports any order and multi-size THP is upstreamed[1], so both\nfile and anonymous folios can be >0 order.  Currently, split_huge_page()\nonly splits a huge page to order-0 pages, but splitting to orders higher\nthan 0 might better utilize large folios, if done properly.  In addition,\nLarge Block Sizes in XFS support would benefit from it during truncate[2].\nThis patchset adds support for splitting a large folio to any lower order\nfolios.\n\nIn addition to this implementation of split_huge_page_to_list_to_order(),\na possible optimization could be splitting a large folio to arbitrary\nsmaller folios instead of a single order.  As both Hugh and Ryan pointed\nout [3,5] that split to a single order might not be optimal, an order-9\nfolio might be better split into 1 order-8, 1 order-7, ..., 1 order-1, and\n2 order-0 folios, depending on subsequent folio operations.  Leave this as\nfuture work.\n\n[1] https://lore.kernel.org/all/20231207161211.2374093-1-ryan.roberts@arm.com/\n[2] https://lore.kernel.org/linux-mm/20240226094936.2677493-1-kernel@pankajraghav.com/\n[3] https://lore.kernel.org/linux-mm/9dd96da-efa2-5123-20d4-4992136ef3ad@google.com/\n[4] https://lore.kernel.org/linux-mm/cbb1d6a0-66dd-47d0-8733-f836fe050374@arm.com/\n[5] https://lore.kernel.org/linux-mm/20240213215520.1048625-1-zi.yan@sent.com/\n\n\nThis patch (of 8):\n\nAs multi-size THP support is added, not all THPs are PMD-mapped, thus\nduring a huge page split, there is no need to always split PMD mapping in\nunmap_folio().  Make it conditional.\n\nLink: https://lkml.kernel.org/r/20240226205534.1603748-1-zi.yan@sent.com\nLink: https://lkml.kernel.org/r/20240226205534.1603748-2-zi.yan@sent.com\nSigned-off-by: Zi Yan <ziy@nvidia.com>\nReviewed-by: David Hildenbrand <david@redhat.com>\nCc: Hugh Dickins <hughd@google.com>\nCc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>\nCc: Luis Chamberlain <mcgrof@kernel.org>\nCc: Matthew Wilcox <willy@infradead.org>\nCc: Michal Koutny <mkoutny@suse.com>\nCc: Roman Gushchin <roman.gushchin@linux.dev>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Yu Zhao <yuzhao@google.com>\nCc: Zach O'Keefe <zokeefe@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/page_owner: use order instead of nr in split_page_owner()", "commit_message": "We do not have non power of two pages, using nr is error prone if nr is\nnot power-of-two.  Use page order instead.\n\nLink: https://lkml.kernel.org/r/20240226205534.1603748-5-zi.yan@sent.com\nSigned-off-by: Zi Yan <ziy@nvidia.com>\nAcked-by: David Hildenbrand <david@redhat.com>\nCc: Hugh Dickins <hughd@google.com>\nCc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>\nCc: Luis Chamberlain <mcgrof@kernel.org>\nCc: \"Matthew Wilcox (Oracle)\" <willy@infradead.org>\nCc: Michal Koutny <mkoutny@suse.com>\nCc: Roman Gushchin <roman.gushchin@linux.dev>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Yu Zhao <yuzhao@google.com>\nCc: Zach O'Keefe <zokeefe@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: memcg: make memcg huge page split support any order split", "commit_message": "It sets memcg information for the pages after the split.  A new parameter\nnew_order is added to tell the order of subpages in the new page, always 0\nfor now.  It prepares for upcoming changes to support split huge page to\nany lower order.\n\nLink: https://lkml.kernel.org/r/20240226205534.1603748-6-zi.yan@sent.com\nSigned-off-by: Zi Yan <ziy@nvidia.com>\nAcked-by: David Hildenbrand <david@redhat.com>\nCc: Hugh Dickins <hughd@google.com>\nCc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>\nCc: Luis Chamberlain <mcgrof@kernel.org>\nCc: \"Matthew Wilcox (Oracle)\" <willy@infradead.org>\nCc: Michal Koutny <mkoutny@suse.com>\nCc: Roman Gushchin <roman.gushchin@linux.dev>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Yu Zhao <yuzhao@google.com>\nCc: Zach O'Keefe <zokeefe@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}],
        "proper_nouns": [{"folio": "In the Linux kernel, a 'folio' is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages."}, {"THP (Transparent Huge Pages)": "THP, or Transparent Huge Pages, is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture. THP automatically manages these huge pages, making them transparent to applications."}, {"PMD (Page Middle Directory)": "The Page Middle Directory (PMD) is a part of the Linux kernel's memory management system, specifically within the page table hierarchy. It sits between the Page Global Directory (PGD) and the Page Table Entry (PTE) in the multi-level page table structure. PMD is responsible for mapping large memory regions, often using huge pages, which can significantly reduce the overhead of memory management."}, {"memcg (Memory Control Group)": "Memory Control Group (memcg) is a feature in the Linux kernel that allows for the hierarchical management of memory resources. It enables the kernel to allocate and limit memory usage for groups of processes, providing better control over memory consumption and improving overall system stability."}],
        "background_knowledge": ["The ability to split a large folio into smaller folios of any order is a significant enhancement in memory management. This capability is particularly useful in scenarios where large memory pages (such as THPs) need to be broken down into smaller units for more granular memory allocation and management.", "Historically, the Linux kernel's memory management system was limited to splitting large pages into smaller pages of a fixed order (typically order-0 pages, which are 4KB each). This limitation could lead to inefficiencies, especially when dealing with large files or complex memory operations that required more flexible memory management."],
        "kernel_changes": ["Before the implementation of this feature, the Linux kernel could only split large folios into smaller pages of a fixed order, which often resulted in suboptimal memory usage. This limitation was particularly evident in scenarios involving large files or complex memory operations that required more flexible memory management.", "After the implementation, the kernel gained the ability to split a large folio into smaller folios of any order. This change allowed for more granular memory allocation and management, improving memory utilization and reducing fragmentation. Specifically, the kernel can now split a large folio into smaller folios of varying sizes, depending on the specific memory requirements of the operation at hand.", "Additionally, the implementation addressed issues related to the handling of order-1 folios, ensuring that they are properly managed without causing list corruption or other memory management issues. This enhancement is particularly important for file-backed folios, which do not use the deferred list and thus require different handling compared to anonymous folios."],
        "comprehensive_understanding": "The feature of splitting a folio to any lower order folios represents a significant advancement in the Linux kernel's memory management capabilities. By enabling the kernel to split large folios into smaller folios of any order, this feature provides greater flexibility in memory allocation and management, leading to improved memory utilization and reduced fragmentation. This enhancement is particularly beneficial in scenarios involving large files or complex memory operations that require more granular control over memory resources. Additionally, the implementation addresses specific issues related to the handling of order-1 folios, ensuring that they are managed correctly without causing memory management issues. Overall, this feature enhances the Linux kernel's ability to efficiently manage memory resources, contributing to better system performance and stability."
    }
}

2024-12-03 22:26:01,456 - extractor - DEBUG - deepseek response:
ChatCompletion(id='678c1fff-40dc-4679-9952-12dc3bd76ce8', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "concepts": ["folio", "THP (Transparent Huge Pages)", "PMD (Page Middle Directory)", "memcg (Memory Control Group)", "memory allocation", "memory management", "memory fragmentation", "file-backed folios", "anonymous folios", "order-1 folios"]\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733235956, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=73, prompt_tokens=5384, total_tokens=5457, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=1536, prompt_cache_miss_tokens=3848))
2024-12-03 22:26:01,456 - extractor - DEBUG - feature_extracted of feature_id=33233:
{
    "feature_id": 33233,
    "feature_description": "Split a folio to any lower order folios",
    "version": "6.9",
    "commits": [
        {
            "commit_subject": "mm: thp: split huge page to any lower order pages",
            "commit_message": "To split a THP to any lower order pages, we need to reform THPs on\nsubpages at given order and add page refcount based on the new page order.\nAlso we need to reinitialize page_deferred_list after removing the page\nfrom the split_queue, otherwise a subsequent split will see list\ncorruption when checking the page_deferred_list again.\n\nNote: Anonymous order-1 folio is not supported because _deferred_list,\nwhich is used by partially mapped folios, is stored in subpage 2 and an\norder-1 folio only has subpage 0 and 1.  File-backed order-1 folios are\nfine, since they do not use _deferred_list.\n\n[ziy@nvidia.com: fixup per discussion with Ryan]\n  Link: https://lkml.kernel.org/r/494F48CD-1F0F-4CAD-884E-6D48F40AF990@nvidia.com\nLink: https://lkml.kernel.org/r/20240226205534.1603748-8-zi.yan@sent.com\nSigned-off-by: Zi Yan <ziy@nvidia.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Hugh Dickins <hughd@google.com>\nCc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>\nCc: Luis Chamberlain <mcgrof@kernel.org>\nCc: \"Matthew Wilcox (Oracle)\" <willy@infradead.org>\nCc: Michal Koutny <mkoutny@suse.com>\nCc: Roman Gushchin <roman.gushchin@linux.dev>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Yu Zhao <yuzhao@google.com>\nCc: Zach O'Keefe <zokeefe@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm: support order-1 folios in the page cache",
            "commit_message": "Folios of order 1 have no space to store the deferred list.  This is not a\nproblem for the page cache as file-backed folios are never placed on the\ndeferred list.  All we need to do is prevent the core MM from touching the\ndeferred list for order 1 folios and remove the code which prevented us\nfrom allocating order 1 folios.\n\nLink: https://lore.kernel.org/linux-mm/90344ea7-4eec-47ee-5996-0c22f42d6a6a@google.com/\nLink: https://lkml.kernel.org/r/20240226205534.1603748-3-zi.yan@sent.com\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nSigned-off-by: Zi Yan <ziy@nvidia.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Hugh Dickins <hughd@google.com>\nCc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>\nCc: Luis Chamberlain <mcgrof@kernel.org>\nCc: Michal Koutny <mkoutny@suse.com>\nCc: Roman Gushchin <roman.gushchin@linux.dev>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Yu Zhao <yuzhao@google.com>\nCc: Zach O'Keefe <zokeefe@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm/memcg: use order instead of nr in split_page_memcg()",
            "commit_message": "We do not have non power of two pages, using nr is error prone if nr is\nnot power-of-two.  Use page order instead.\n\nLink: https://lkml.kernel.org/r/20240226205534.1603748-4-zi.yan@sent.com\nSigned-off-by: Zi Yan <ziy@nvidia.com>\nAcked-by: David Hildenbrand <david@redhat.com>\nCc: Hugh Dickins <hughd@google.com>\nCc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>\nCc: Luis Chamberlain <mcgrof@kernel.org>\nCc: \"Matthew Wilcox (Oracle)\" <willy@infradead.org>\nCc: Michal Koutny <mkoutny@suse.com>\nCc: Roman Gushchin <roman.gushchin@linux.dev>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Yu Zhao <yuzhao@google.com>\nCc: Zach O'Keefe <zokeefe@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm/huge_memory: only split PMD mapping when necessary in unmap_folio()",
            "commit_message": "Patch series \"Split a folio to any lower order folios\", v5.\n\nFile folio supports any order and multi-size THP is upstreamed[1], so both\nfile and anonymous folios can be >0 order.  Currently, split_huge_page()\nonly splits a huge page to order-0 pages, but splitting to orders higher\nthan 0 might better utilize large folios, if done properly.  In addition,\nLarge Block Sizes in XFS support would benefit from it during truncate[2].\nThis patchset adds support for splitting a large folio to any lower order\nfolios.\n\nIn addition to this implementation of split_huge_page_to_list_to_order(),\na possible optimization could be splitting a large folio to arbitrary\nsmaller folios instead of a single order.  As both Hugh and Ryan pointed\nout [3,5] that split to a single order might not be optimal, an order-9\nfolio might be better split into 1 order-8, 1 order-7, ..., 1 order-1, and\n2 order-0 folios, depending on subsequent folio operations.  Leave this as\nfuture work.\n\n[1] https://lore.kernel.org/all/20231207161211.2374093-1-ryan.roberts@arm.com/\n[2] https://lore.kernel.org/linux-mm/20240226094936.2677493-1-kernel@pankajraghav.com/\n[3] https://lore.kernel.org/linux-mm/9dd96da-efa2-5123-20d4-4992136ef3ad@google.com/\n[4] https://lore.kernel.org/linux-mm/cbb1d6a0-66dd-47d0-8733-f836fe050374@arm.com/\n[5] https://lore.kernel.org/linux-mm/20240213215520.1048625-1-zi.yan@sent.com/\n\n\nThis patch (of 8):\n\nAs multi-size THP support is added, not all THPs are PMD-mapped, thus\nduring a huge page split, there is no need to always split PMD mapping in\nunmap_folio().  Make it conditional.\n\nLink: https://lkml.kernel.org/r/20240226205534.1603748-1-zi.yan@sent.com\nLink: https://lkml.kernel.org/r/20240226205534.1603748-2-zi.yan@sent.com\nSigned-off-by: Zi Yan <ziy@nvidia.com>\nReviewed-by: David Hildenbrand <david@redhat.com>\nCc: Hugh Dickins <hughd@google.com>\nCc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>\nCc: Luis Chamberlain <mcgrof@kernel.org>\nCc: Matthew Wilcox <willy@infradead.org>\nCc: Michal Koutny <mkoutny@suse.com>\nCc: Roman Gushchin <roman.gushchin@linux.dev>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Yu Zhao <yuzhao@google.com>\nCc: Zach O'Keefe <zokeefe@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm/page_owner: use order instead of nr in split_page_owner()",
            "commit_message": "We do not have non power of two pages, using nr is error prone if nr is\nnot power-of-two.  Use page order instead.\n\nLink: https://lkml.kernel.org/r/20240226205534.1603748-5-zi.yan@sent.com\nSigned-off-by: Zi Yan <ziy@nvidia.com>\nAcked-by: David Hildenbrand <david@redhat.com>\nCc: Hugh Dickins <hughd@google.com>\nCc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>\nCc: Luis Chamberlain <mcgrof@kernel.org>\nCc: \"Matthew Wilcox (Oracle)\" <willy@infradead.org>\nCc: Michal Koutny <mkoutny@suse.com>\nCc: Roman Gushchin <roman.gushchin@linux.dev>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Yu Zhao <yuzhao@google.com>\nCc: Zach O'Keefe <zokeefe@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm: memcg: make memcg huge page split support any order split",
            "commit_message": "It sets memcg information for the pages after the split.  A new parameter\nnew_order is added to tell the order of subpages in the new page, always 0\nfor now.  It prepares for upcoming changes to support split huge page to\nany lower order.\n\nLink: https://lkml.kernel.org/r/20240226205534.1603748-6-zi.yan@sent.com\nSigned-off-by: Zi Yan <ziy@nvidia.com>\nAcked-by: David Hildenbrand <david@redhat.com>\nCc: Hugh Dickins <hughd@google.com>\nCc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>\nCc: Luis Chamberlain <mcgrof@kernel.org>\nCc: \"Matthew Wilcox (Oracle)\" <willy@infradead.org>\nCc: Michal Koutny <mkoutny@suse.com>\nCc: Roman Gushchin <roman.gushchin@linux.dev>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Yu Zhao <yuzhao@google.com>\nCc: Zach O'Keefe <zokeefe@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        }
    ],
    "proper_nouns": [
        {
            "folio": "In the Linux kernel, a 'folio' is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages."
        },
        {
            "THP (Transparent Huge Pages)": "THP, or Transparent Huge Pages, is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture. THP automatically manages these huge pages, making them transparent to applications."
        },
        {
            "PMD (Page Middle Directory)": "The Page Middle Directory (PMD) is a part of the Linux kernel's memory management system, specifically within the page table hierarchy. It sits between the Page Global Directory (PGD) and the Page Table Entry (PTE) in the multi-level page table structure. PMD is responsible for mapping large memory regions, often using huge pages, which can significantly reduce the overhead of memory management."
        },
        {
            "memcg (Memory Control Group)": "Memory Control Group (memcg) is a feature in the Linux kernel that allows for the hierarchical management of memory resources. It enables the kernel to allocate and limit memory usage for groups of processes, providing better control over memory consumption and improving overall system stability."
        }
    ],
    "background_knowledge": [
        "The ability to split a large folio into smaller folios of any order is a significant enhancement in memory management. This capability is particularly useful in scenarios where large memory pages (such as THPs) need to be broken down into smaller units for more granular memory allocation and management.",
        "Historically, the Linux kernel's memory management system was limited to splitting large pages into smaller pages of a fixed order (typically order-0 pages, which are 4KB each). This limitation could lead to inefficiencies, especially when dealing with large files or complex memory operations that required more flexible memory management."
    ],
    "kernel_changes": [
        "Before the implementation of this feature, the Linux kernel could only split large folios into smaller pages of a fixed order, which often resulted in suboptimal memory usage. This limitation was particularly evident in scenarios involving large files or complex memory operations that required more flexible memory management.",
        "After the implementation, the kernel gained the ability to split a large folio into smaller folios of any order. This change allowed for more granular memory allocation and management, improving memory utilization and reducing fragmentation. Specifically, the kernel can now split a large folio into smaller folios of varying sizes, depending on the specific memory requirements of the operation at hand.",
        "Additionally, the implementation addressed issues related to the handling of order-1 folios, ensuring that they are properly managed without causing list corruption or other memory management issues. This enhancement is particularly important for file-backed folios, which do not use the deferred list and thus require different handling compared to anonymous folios."
    ],
    "comprehensive_understanding": "The feature of splitting a folio to any lower order folios represents a significant advancement in the Linux kernel's memory management capabilities. By enabling the kernel to split large folios into smaller folios of any order, this feature provides greater flexibility in memory allocation and management, leading to improved memory utilization and reduced fragmentation. This enhancement is particularly beneficial in scenarios involving large files or complex memory operations that require more granular control over memory resources. Additionally, the implementation addresses specific issues related to the handling of order-1 folios, ensuring that they are managed correctly without causing memory management issues. Overall, this feature enhances the Linux kernel's ability to efficiently manage memory resources, contributing to better system performance and stability.",
    "concepts": [
        "folio",
        "THP (Transparent Huge Pages)",
        "PMD (Page Middle Directory)",
        "memcg (Memory Control Group)",
        "memory allocation",
        "memory management",
        "memory fragmentation",
        "file-backed folios",
        "anonymous folios",
        "order-1 folios"
    ]
}
2024-12-03 22:26:01,456 - extractor - INFO - ---------Processing feature 8/815---------
2024-12-03 22:26:07,586 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个 linux 内核的专家。给你一个 linux 内核特性的描述以及其对应的若干 commit 的标题和提交信息，你的任务是为这个特性写一个多方面的扩写版描述。你的扩写需包含以下几个方面：1. 该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)；2. 为了让开发者理解该特性，需要哪些具体的，与这个特性直接相关的背景知识 (background_knowledge)；3. 该特性被实现前后，linux 内核发生了怎样的变化 (kernel_changes)；4. 你对该特性的综合理解 (comprehensive_understanding)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}]
        },
        "output": {
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
        }
    ],
    "input": {
        "feature_description": "madvise: pageout: ignore references rather than clearing young",
        "commits": [{"commit_subject": "mm: madvise: pageout: ignore references rather than clearing young", "commit_message": "While doing MADV_PAGEOUT, the current code will clear PTE young so that\nvmscan won't read young flags to allow the reclamation of madvised folios\nto go ahead.  It seems we can do it by directly ignoring references, thus\nwe can remove tlb flush in madvise and rmap overhead in vmscan.\n\nRegarding the side effect, in the original code, if a parallel thread runs\nside by side to access the madvised memory with the thread doing madvise,\nfolios will get a chance to be re-activated by vmscan (though the time gap\nis actually quite small since checking PTEs is done immediately after\nclearing PTEs young).  But with this patch, they will still be reclaimed. \nBut this behaviour doing PAGEOUT and doing access at the same time is\nquite silly like DoS.  So probably, we don't need to care.  Or ignoring\nthe new access during the quite small time gap is even better.\n\nFor DAMON's DAMOS_PAGEOUT based on physical address region, we still keep\nits behaviour as is since a physical address might be mapped by multiple\nprocesses.  MADV_PAGEOUT based on virtual address is actually much more\naggressive on reclamation.  To untouch paddr's DAMOS_PAGEOUT, we simply\npass ignore_references as false in reclaim_pages().\n\nA microbench as below has shown 6% decrement on the latency of\nMADV_PAGEOUT,\n\n #define PGSIZE 4096\n main()\n {\n \tint i;\n #define SIZE 512*1024*1024\n \tvolatile long *p = mmap(NULL, SIZE, PROT_READ | PROT_WRITE,\n \t\t\tMAP_PRIVATE | MAP_ANONYMOUS, -1, 0);\n\n \tfor (i = 0; i < SIZE/sizeof(long); i += PGSIZE / sizeof(long))\n \t\tp[i] =  0x11;\n\n \tmadvise(p, SIZE, MADV_PAGEOUT);\n }\n\nw/o patch                    w/ patch\nroot@10:~# time ./a.out      root@10:~# time ./a.out\nreal\t0m49.634s            real   0m46.334s\nuser\t0m0.637s             user   0m0.648s\nsys\t0m47.434s            sys    0m44.265s\n\nLink: https://lkml.kernel.org/r/20240226005739.24350-1-21cnbao@gmail.com\nSigned-off-by: Barry Song <v-songbaohua@oppo.com>\nAcked-by: Minchan Kim <minchan@kernel.org>\nCc: SeongJae Park <sj@kernel.org>\nCc: Michal Hocko <mhocko@suse.com>\nCc: Johannes Weiner <hannes@cmpxchg.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}]
    }
}

2024-12-03 22:26:45,996 - extractor - DEBUG - deepseek response:
ChatCompletion(id='0aadcbac-0a95-4959-a00d-30438606da44', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "proper_nouns": [\n        {\n            "MADV_PAGEOUT": "MADV_PAGEOUT is a madvise() command in the Linux kernel that suggests the kernel to reclaim (page out) the specified memory region. This command is used to free up memory by moving the pages of the specified region to the swap space, thereby making the physical memory available for other processes."\n        },\n        {\n            "PTE (Page Table Entry)": "A Page Table Entry (PTE) is a data structure used in the virtual memory systems of operating systems like Linux. It contains information about the mapping between virtual and physical memory pages, including attributes like the \'young\' bit, which indicates whether the page has been recently accessed."\n        },\n        {\n            "vmscan": "vmscan is a component of the Linux kernel\'s memory management subsystem responsible for reclaiming pages of memory that are no longer in use. It scans through the list of inactive pages and decides which pages can be freed or moved to the swap space."\n        },\n        {\n            "DAMON (Data Access Monitor)": "DAMON (Data Access Monitor) is a kernel subsystem that provides a way to monitor and analyze data access patterns in the system. It can be used to optimize memory management strategies based on observed access patterns."\n        },\n        {\n            "DAMOS_PAGEOUT": "DAMOS_PAGEOUT is a feature within the DAMON subsystem that allows for the proactive reclamation of memory pages based on observed access patterns. It is used to optimize memory usage by identifying and reclaiming pages that are not frequently accessed."\n        }\n    ],\n    "background_knowledge": [\n        "The current implementation of MADV_PAGEOUT clears the \'young\' bit in the Page Table Entry (PTE) to prevent vmscan from re-activating the pages during the reclamation process. This approach involves additional overhead, including TLB flushes and rmap operations.",\n        "The proposed change involves ignoring references rather than clearing the \'young\' bit, which reduces the overhead associated with TLB flushes and rmap operations in vmscan."\n    ],\n    "kernel_changes": [\n        "Before this change, the Linux kernel would clear the \'young\' bit in the PTE during MADV_PAGEOUT to prevent vmscan from re-activating the pages. This approach incurred additional overhead, including TLB flushes and rmap operations.",\n        "After the implementation of this feature, the kernel now ignores references during MADV_PAGEOUT, thereby reducing the overhead associated with TLB flushes and rmap operations in vmscan. This change results in a 6% reduction in the latency of MADV_PAGEOUT operations, as demonstrated by a microbenchmark."\n    ],\n    "comprehensive_understanding": "This feature improves the efficiency of the MADV_PAGEOUT operation in the Linux kernel by ignoring references rather than clearing the \'young\' bit in the Page Table Entry (PTE). This change reduces the overhead associated with TLB flushes and rmap operations in vmscan, leading to a 6% reduction in the latency of MADV_PAGEOUT operations. While this approach may result in more aggressive reclamation of pages, it is considered acceptable given the context of the operation, which is typically used in scenarios where freeing up memory is a priority."\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733235967, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=723, prompt_tokens=2352, total_tokens=3075, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=1472, prompt_cache_miss_tokens=880))
2024-12-03 22:26:45,997 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个命名实体抽取 (NER) 任务的专家，同时也是 linux 内核的专家。给你一个 linux 内核特性的描述 (feature_description)、对应的若干 commit 的标题和提交信息 (commits)、该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)、为了让开发者理解该特性所需的与这个特性直接相关的背景知识 (background_knowledge)、该特性被实现前后，linux 内核发生的变化 (kernel_changes)、对该特性的综合理解 (comprehensive_understanding)。你的任务是：根据这些信息提取出在 linux 内核中涉及到的和这个特性相关的概念实体 (concepts)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}],
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": [
                "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
            ]
        },
        "output": {
            "concepts": ["Copy-on-Write (CoW)", "mTHP", "folio", "memory reclamation", "memory availability", "anti-fragmentation"]
        }
    ],
    "input": {
        "feature_description": "madvise: pageout: ignore references rather than clearing young",
        "commits": [{"commit_subject": "mm: madvise: pageout: ignore references rather than clearing young", "commit_message": "While doing MADV_PAGEOUT, the current code will clear PTE young so that\nvmscan won't read young flags to allow the reclamation of madvised folios\nto go ahead.  It seems we can do it by directly ignoring references, thus\nwe can remove tlb flush in madvise and rmap overhead in vmscan.\n\nRegarding the side effect, in the original code, if a parallel thread runs\nside by side to access the madvised memory with the thread doing madvise,\nfolios will get a chance to be re-activated by vmscan (though the time gap\nis actually quite small since checking PTEs is done immediately after\nclearing PTEs young).  But with this patch, they will still be reclaimed. \nBut this behaviour doing PAGEOUT and doing access at the same time is\nquite silly like DoS.  So probably, we don't need to care.  Or ignoring\nthe new access during the quite small time gap is even better.\n\nFor DAMON's DAMOS_PAGEOUT based on physical address region, we still keep\nits behaviour as is since a physical address might be mapped by multiple\nprocesses.  MADV_PAGEOUT based on virtual address is actually much more\naggressive on reclamation.  To untouch paddr's DAMOS_PAGEOUT, we simply\npass ignore_references as false in reclaim_pages().\n\nA microbench as below has shown 6% decrement on the latency of\nMADV_PAGEOUT,\n\n #define PGSIZE 4096\n main()\n {\n \tint i;\n #define SIZE 512*1024*1024\n \tvolatile long *p = mmap(NULL, SIZE, PROT_READ | PROT_WRITE,\n \t\t\tMAP_PRIVATE | MAP_ANONYMOUS, -1, 0);\n\n \tfor (i = 0; i < SIZE/sizeof(long); i += PGSIZE / sizeof(long))\n \t\tp[i] =  0x11;\n\n \tmadvise(p, SIZE, MADV_PAGEOUT);\n }\n\nw/o patch                    w/ patch\nroot@10:~# time ./a.out      root@10:~# time ./a.out\nreal\t0m49.634s            real   0m46.334s\nuser\t0m0.637s             user   0m0.648s\nsys\t0m47.434s            sys    0m44.265s\n\nLink: https://lkml.kernel.org/r/20240226005739.24350-1-21cnbao@gmail.com\nSigned-off-by: Barry Song <v-songbaohua@oppo.com>\nAcked-by: Minchan Kim <minchan@kernel.org>\nCc: SeongJae Park <sj@kernel.org>\nCc: Michal Hocko <mhocko@suse.com>\nCc: Johannes Weiner <hannes@cmpxchg.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}],
        "proper_nouns": [{"MADV_PAGEOUT": "MADV_PAGEOUT is a madvise() command in the Linux kernel that suggests the kernel to reclaim (page out) the specified memory region. This command is used to free up memory by moving the pages of the specified region to the swap space, thereby making the physical memory available for other processes."}, {"PTE (Page Table Entry)": "A Page Table Entry (PTE) is a data structure used in the virtual memory systems of operating systems like Linux. It contains information about the mapping between virtual and physical memory pages, including attributes like the 'young' bit, which indicates whether the page has been recently accessed."}, {"vmscan": "vmscan is a component of the Linux kernel's memory management subsystem responsible for reclaiming pages of memory that are no longer in use. It scans through the list of inactive pages and decides which pages can be freed or moved to the swap space."}, {"DAMON (Data Access Monitor)": "DAMON (Data Access Monitor) is a kernel subsystem that provides a way to monitor and analyze data access patterns in the system. It can be used to optimize memory management strategies based on observed access patterns."}, {"DAMOS_PAGEOUT": "DAMOS_PAGEOUT is a feature within the DAMON subsystem that allows for the proactive reclamation of memory pages based on observed access patterns. It is used to optimize memory usage by identifying and reclaiming pages that are not frequently accessed."}],
        "background_knowledge": ["The current implementation of MADV_PAGEOUT clears the 'young' bit in the Page Table Entry (PTE) to prevent vmscan from re-activating the pages during the reclamation process. This approach involves additional overhead, including TLB flushes and rmap operations.", "The proposed change involves ignoring references rather than clearing the 'young' bit, which reduces the overhead associated with TLB flushes and rmap operations in vmscan."],
        "kernel_changes": ["Before this change, the Linux kernel would clear the 'young' bit in the PTE during MADV_PAGEOUT to prevent vmscan from re-activating the pages. This approach incurred additional overhead, including TLB flushes and rmap operations.", "After the implementation of this feature, the kernel now ignores references during MADV_PAGEOUT, thereby reducing the overhead associated with TLB flushes and rmap operations in vmscan. This change results in a 6% reduction in the latency of MADV_PAGEOUT operations, as demonstrated by a microbenchmark."],
        "comprehensive_understanding": "This feature improves the efficiency of the MADV_PAGEOUT operation in the Linux kernel by ignoring references rather than clearing the 'young' bit in the Page Table Entry (PTE). This change reduces the overhead associated with TLB flushes and rmap operations in vmscan, leading to a 6% reduction in the latency of MADV_PAGEOUT operations. While this approach may result in more aggressive reclamation of pages, it is considered acceptable given the context of the operation, which is typically used in scenarios where freeing up memory is a priority."
    }
}

2024-12-03 22:26:50,392 - extractor - DEBUG - deepseek response:
ChatCompletion(id='ac4728ea-a2c0-4616-b186-1c50bd6a8788', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "concepts": ["MADV_PAGEOUT", "PTE (Page Table Entry)", "vmscan", "DAMON (Data Access Monitor)", "DAMOS_PAGEOUT", "TLB flushes", "rmap operations", "memory reclamation", "latency reduction"]\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733236005, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=70, prompt_tokens=3071, total_tokens=3141, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=1536, prompt_cache_miss_tokens=1535))
2024-12-03 22:26:50,392 - extractor - DEBUG - feature_extracted of feature_id=33232:
{
    "feature_id": 33232,
    "feature_description": "madvise: pageout: ignore references rather than clearing young",
    "version": "6.9",
    "commits": [
        {
            "commit_subject": "mm: madvise: pageout: ignore references rather than clearing young",
            "commit_message": "While doing MADV_PAGEOUT, the current code will clear PTE young so that\nvmscan won't read young flags to allow the reclamation of madvised folios\nto go ahead.  It seems we can do it by directly ignoring references, thus\nwe can remove tlb flush in madvise and rmap overhead in vmscan.\n\nRegarding the side effect, in the original code, if a parallel thread runs\nside by side to access the madvised memory with the thread doing madvise,\nfolios will get a chance to be re-activated by vmscan (though the time gap\nis actually quite small since checking PTEs is done immediately after\nclearing PTEs young).  But with this patch, they will still be reclaimed. \nBut this behaviour doing PAGEOUT and doing access at the same time is\nquite silly like DoS.  So probably, we don't need to care.  Or ignoring\nthe new access during the quite small time gap is even better.\n\nFor DAMON's DAMOS_PAGEOUT based on physical address region, we still keep\nits behaviour as is since a physical address might be mapped by multiple\nprocesses.  MADV_PAGEOUT based on virtual address is actually much more\naggressive on reclamation.  To untouch paddr's DAMOS_PAGEOUT, we simply\npass ignore_references as false in reclaim_pages().\n\nA microbench as below has shown 6% decrement on the latency of\nMADV_PAGEOUT,\n\n #define PGSIZE 4096\n main()\n {\n \tint i;\n #define SIZE 512*1024*1024\n \tvolatile long *p = mmap(NULL, SIZE, PROT_READ | PROT_WRITE,\n \t\t\tMAP_PRIVATE | MAP_ANONYMOUS, -1, 0);\n\n \tfor (i = 0; i < SIZE/sizeof(long); i += PGSIZE / sizeof(long))\n \t\tp[i] =  0x11;\n\n \tmadvise(p, SIZE, MADV_PAGEOUT);\n }\n\nw/o patch                    w/ patch\nroot@10:~# time ./a.out      root@10:~# time ./a.out\nreal\t0m49.634s            real   0m46.334s\nuser\t0m0.637s             user   0m0.648s\nsys\t0m47.434s            sys    0m44.265s\n\nLink: https://lkml.kernel.org/r/20240226005739.24350-1-21cnbao@gmail.com\nSigned-off-by: Barry Song <v-songbaohua@oppo.com>\nAcked-by: Minchan Kim <minchan@kernel.org>\nCc: SeongJae Park <sj@kernel.org>\nCc: Michal Hocko <mhocko@suse.com>\nCc: Johannes Weiner <hannes@cmpxchg.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        }
    ],
    "proper_nouns": [
        {
            "MADV_PAGEOUT": "MADV_PAGEOUT is a madvise() command in the Linux kernel that suggests the kernel to reclaim (page out) the specified memory region. This command is used to free up memory by moving the pages of the specified region to the swap space, thereby making the physical memory available for other processes."
        },
        {
            "PTE (Page Table Entry)": "A Page Table Entry (PTE) is a data structure used in the virtual memory systems of operating systems like Linux. It contains information about the mapping between virtual and physical memory pages, including attributes like the 'young' bit, which indicates whether the page has been recently accessed."
        },
        {
            "vmscan": "vmscan is a component of the Linux kernel's memory management subsystem responsible for reclaiming pages of memory that are no longer in use. It scans through the list of inactive pages and decides which pages can be freed or moved to the swap space."
        },
        {
            "DAMON (Data Access Monitor)": "DAMON (Data Access Monitor) is a kernel subsystem that provides a way to monitor and analyze data access patterns in the system. It can be used to optimize memory management strategies based on observed access patterns."
        },
        {
            "DAMOS_PAGEOUT": "DAMOS_PAGEOUT is a feature within the DAMON subsystem that allows for the proactive reclamation of memory pages based on observed access patterns. It is used to optimize memory usage by identifying and reclaiming pages that are not frequently accessed."
        }
    ],
    "background_knowledge": [
        "The current implementation of MADV_PAGEOUT clears the 'young' bit in the Page Table Entry (PTE) to prevent vmscan from re-activating the pages during the reclamation process. This approach involves additional overhead, including TLB flushes and rmap operations.",
        "The proposed change involves ignoring references rather than clearing the 'young' bit, which reduces the overhead associated with TLB flushes and rmap operations in vmscan."
    ],
    "kernel_changes": [
        "Before this change, the Linux kernel would clear the 'young' bit in the PTE during MADV_PAGEOUT to prevent vmscan from re-activating the pages. This approach incurred additional overhead, including TLB flushes and rmap operations.",
        "After the implementation of this feature, the kernel now ignores references during MADV_PAGEOUT, thereby reducing the overhead associated with TLB flushes and rmap operations in vmscan. This change results in a 6% reduction in the latency of MADV_PAGEOUT operations, as demonstrated by a microbenchmark."
    ],
    "comprehensive_understanding": "This feature improves the efficiency of the MADV_PAGEOUT operation in the Linux kernel by ignoring references rather than clearing the 'young' bit in the Page Table Entry (PTE). This change reduces the overhead associated with TLB flushes and rmap operations in vmscan, leading to a 6% reduction in the latency of MADV_PAGEOUT operations. While this approach may result in more aggressive reclamation of pages, it is considered acceptable given the context of the operation, which is typically used in scenarios where freeing up memory is a priority.",
    "concepts": [
        "MADV_PAGEOUT",
        "PTE (Page Table Entry)",
        "vmscan",
        "DAMON (Data Access Monitor)",
        "DAMOS_PAGEOUT",
        "TLB flushes",
        "rmap operations",
        "memory reclamation",
        "latency reduction"
    ]
}
2024-12-03 22:26:50,393 - extractor - INFO - ---------Processing feature 9/815---------
2024-12-03 22:27:06,570 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个 linux 内核的专家。给你一个 linux 内核特性的描述以及其对应的若干 commit 的标题和提交信息，你的任务是为这个特性写一个多方面的扩写版描述。你的扩写需包含以下几个方面：1. 该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)；2. 为了让开发者理解该特性，需要哪些具体的，与这个特性直接相关的背景知识 (background_knowledge)；3. 该特性被实现前后，linux 内核发生了怎样的变化 (kernel_changes)；4. 你对该特性的综合理解 (comprehensive_understanding)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}]
        },
        "output": {
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
        }
    ],
    "input": {
        "feature_description": "Handle hugetlb faults under the VMA lock",
        "commits": [{"commit_subject": "hugetlb: allow faults to be handled under the VMA lock", "commit_message": "Hugetlb can now safely handle faults under the VMA lock, so allow it to do\nso.\n\nThis patch may cause ltp hugemmap10 to \"fail\".  Hugemmap10 tests hugetlb\ncounters, and expects the counters to remain unchanged on failure to\nhandle a fault.\n\nIn hugetlb_no_page(), vmf_anon_prepare() may bailout with no anon_vma\nunder the VMA lock after allocating a folio for the hugepage.  In\nfree_huge_folio(), this folio is completely freed on bailout iff there is\na surplus of hugetlb pages.  This will remove a folio off the freelist and\ndecrement the number of hugepages while ltp expects these counters to\nremain unchanged on failure.\n\nOriginally this could only happen due to OOM failures, but now it may also\noccur after we allocate a hugetlb folio without a suitable anon_vma under\nthe VMA lock.  This should only happen for the first freshly allocated\nhugepage in this vma.\n\nLink: https://lkml.kernel.org/r/20240221234732.187629-6-vishal.moola@gmail.com\nSigned-off-by: Vishal Moola (Oracle) <vishal.moola@gmail.com>\nReviewed-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Muchun Song <muchun.song@linux.dev>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "hugetlb: use vmf_anon_prepare() instead of anon_vma_prepare()", "commit_message": "hugetlb_no_page() and hugetlb_wp() call anon_vma_prepare().  In\npreparation for hugetlb to safely handle faults under the VMA lock, use\nvmf_anon_prepare() here instead.\n\nAdditionally, passing hugetlb_wp() the vm_fault struct from\nhugetlb_fault() works toward cleaning up the hugetlb code and function\nstack.\n\nLink: https://lkml.kernel.org/r/20240221234732.187629-5-vishal.moola@gmail.com\nSigned-off-by: Vishal Moola (Oracle) <vishal.moola@gmail.com>\nReviewed-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Muchun Song <muchun.song@linux.dev>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "hugetlb: pass struct vm_fault through to hugetlb_handle_userfault()", "commit_message": "Now that hugetlb_fault() has a struct vm_fault, have\nhugetlb_handle_userfault() use it instead of creating one of its own.\n\nThis lets us reduce the number of arguments passed to\nhugetlb_handle_userfault() from 7 to 3, cleaning up the code and stack.\n\nLink: https://lkml.kernel.org/r/20240221234732.187629-4-vishal.moola@gmail.com\nSigned-off-by: Vishal Moola (Oracle) <vishal.moola@gmail.com>\nReviewed-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Muchun Song <muchun.song@linux.dev>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/memory: change vmf_anon_prepare() to be non-static", "commit_message": "Patch series \"Handle hugetlb faults under the VMA lock\", v2.\n\nIt is generally safe to handle hugetlb faults under the VMA lock.  The\nonly time this is unsafe is when no anon_vma has been allocated to this\nvma yet, so we can use vmf_anon_prepare() instead of anon_vma_prepare() to\nbailout if necessary.  This should only happen for the first hugetlb page\nin the vma.\n\nAdditionally, this patchset begins to use struct vm_fault within\nhugetlb_fault().  This works towards cleaning up hugetlb code, and should\nsignificantly reduce the number of arguments passed to functions.\n\nThe last patch in this series may cause ltp hugemmap10 to \"fail\".  This is\nbecause vmf_anon_prepare() may bailout with no anon_vma under the VMA lock\nafter allocating a folio for the hugepage.  In free_huge_folio(), this\nfolio is completely freed on bailout iff there is a surplus of hugetlb\npages.  This will remove a folio off the freelist and decrement the number\nof hugepages while ltp expects these counters to remain unchanged on\nfailure.  The rest of the ltp testcases pass.\n\n\nThis patch (of 2):\n\nIn order to handle hugetlb faults under the VMA lock, hugetlb can use\nvmf_anon_prepare() to ensure we can safely prepare an anon_vma.  Change it\nto be a non-static function so it can be used within hugetlb as well.\n\nLink: https://lkml.kernel.org/r/20240221234732.187629-6-vishal.moola@gmail.com\nLink: https://lkml.kernel.org/r/20240221234732.187629-2-vishal.moola@gmail.com\nSigned-off-by: Vishal Moola (Oracle) <vishal.moola@gmail.com>\nReviewed-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Muchun Song <muchun.song@linux.dev>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}]
    }
}

2024-12-03 22:27:47,414 - extractor - DEBUG - deepseek response:
ChatCompletion(id='42bfad4a-13d9-4d4e-b43b-5d0e407c0696', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "proper_nouns": [\n        {\n            "VMA (Virtual Memory Area)": "A VMA is a contiguous range of virtual memory addresses in a process\'s address space. Each VMA represents a segment of memory that has specific attributes, such as whether it is readable, writable, or executable, and whether it is backed by a file or anonymous memory."\n        },\n        {\n            "hugetlb": "Hugetlb is a feature in the Linux kernel that allows the use of large memory pages, known as huge pages. These pages are significantly larger than the standard 4KB pages, often being 2MB or 1GB in size. Using huge pages can reduce the overhead associated with managing a large number of small pages, thereby improving memory performance."\n        },\n        {\n            "vm_fault": "A vm_fault is a structure in the Linux kernel that represents a page fault event. It contains information about the fault, such as the virtual address that caused the fault, the type of fault (e.g., read, write), and the VMA associated with the fault."\n        },\n        {\n            "anon_vma": "An anon_vma is an anonymous virtual memory area in the Linux kernel. It is used to manage the reverse mapping of anonymous pages, which are pages not backed by a file. The anon_vma structure helps in tracking the relationships between pages and their users, which is crucial for tasks like page migration and reclamation."\n        },\n        {\n            "vmf_anon_prepare()": "vmf_anon_prepare() is a function in the Linux kernel that prepares an anonymous virtual memory area (anon_vma) for a given VMA. It is used to ensure that the necessary structures are in place before handling a page fault. This function is particularly useful in scenarios where the fault handling needs to be done under the VMA lock."\n        }\n    ],\n    "background_knowledge": [\n        "Handling hugetlb faults under the VMA lock requires understanding the interaction between huge pages and the VMA lock. The VMA lock is a mechanism used to protect the VMA from concurrent modifications, ensuring that page faults and other operations on the VMA are handled safely. Previously, hugetlb faults were not handled under the VMA lock due to potential issues with anon_vma allocation. This feature addresses that limitation by using vmf_anon_prepare() to safely handle anon_vma allocation under the VMA lock."\n    ],\n    "kernel_changes": [\n        "Before this feature, hugetlb faults were not handled under the VMA lock due to the risk of anon_vma allocation failing, which could lead to inconsistencies. The kernel has been modified to use vmf_anon_prepare() instead of anon_vma_prepare() to handle anon_vma allocation safely under the VMA lock. This change allows hugetlb faults to be handled more efficiently and safely, reducing the risk of inconsistencies and improving the overall performance of memory management in the kernel."\n    ],\n    "comprehensive_understanding": "This feature enhances the handling of hugetlb faults by allowing them to be processed under the VMA lock. This is achieved by replacing the use of anon_vma_prepare() with vmf_anon_prepare(), which ensures that anon_vma allocation can be safely performed under the VMA lock. This change not only improves the efficiency of fault handling but also reduces the risk of inconsistencies in memory management. By enabling hugetlb faults to be handled under the VMA lock, the kernel can manage memory more effectively, leading to better performance and stability."\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733236026, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=783, prompt_tokens=3072, total_tokens=3855, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=1472, prompt_cache_miss_tokens=1600))
2024-12-03 22:27:47,414 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个命名实体抽取 (NER) 任务的专家，同时也是 linux 内核的专家。给你一个 linux 内核特性的描述 (feature_description)、对应的若干 commit 的标题和提交信息 (commits)、该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)、为了让开发者理解该特性所需的与这个特性直接相关的背景知识 (background_knowledge)、该特性被实现前后，linux 内核发生的变化 (kernel_changes)、对该特性的综合理解 (comprehensive_understanding)。你的任务是：根据这些信息提取出在 linux 内核中涉及到的和这个特性相关的概念实体 (concepts)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}],
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": [
                "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
            ]
        },
        "output": {
            "concepts": ["Copy-on-Write (CoW)", "mTHP", "folio", "memory reclamation", "memory availability", "anti-fragmentation"]
        }
    ],
    "input": {
        "feature_description": "Handle hugetlb faults under the VMA lock",
        "commits": [{"commit_subject": "hugetlb: allow faults to be handled under the VMA lock", "commit_message": "Hugetlb can now safely handle faults under the VMA lock, so allow it to do\nso.\n\nThis patch may cause ltp hugemmap10 to \"fail\".  Hugemmap10 tests hugetlb\ncounters, and expects the counters to remain unchanged on failure to\nhandle a fault.\n\nIn hugetlb_no_page(), vmf_anon_prepare() may bailout with no anon_vma\nunder the VMA lock after allocating a folio for the hugepage.  In\nfree_huge_folio(), this folio is completely freed on bailout iff there is\na surplus of hugetlb pages.  This will remove a folio off the freelist and\ndecrement the number of hugepages while ltp expects these counters to\nremain unchanged on failure.\n\nOriginally this could only happen due to OOM failures, but now it may also\noccur after we allocate a hugetlb folio without a suitable anon_vma under\nthe VMA lock.  This should only happen for the first freshly allocated\nhugepage in this vma.\n\nLink: https://lkml.kernel.org/r/20240221234732.187629-6-vishal.moola@gmail.com\nSigned-off-by: Vishal Moola (Oracle) <vishal.moola@gmail.com>\nReviewed-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Muchun Song <muchun.song@linux.dev>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "hugetlb: use vmf_anon_prepare() instead of anon_vma_prepare()", "commit_message": "hugetlb_no_page() and hugetlb_wp() call anon_vma_prepare().  In\npreparation for hugetlb to safely handle faults under the VMA lock, use\nvmf_anon_prepare() here instead.\n\nAdditionally, passing hugetlb_wp() the vm_fault struct from\nhugetlb_fault() works toward cleaning up the hugetlb code and function\nstack.\n\nLink: https://lkml.kernel.org/r/20240221234732.187629-5-vishal.moola@gmail.com\nSigned-off-by: Vishal Moola (Oracle) <vishal.moola@gmail.com>\nReviewed-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Muchun Song <muchun.song@linux.dev>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "hugetlb: pass struct vm_fault through to hugetlb_handle_userfault()", "commit_message": "Now that hugetlb_fault() has a struct vm_fault, have\nhugetlb_handle_userfault() use it instead of creating one of its own.\n\nThis lets us reduce the number of arguments passed to\nhugetlb_handle_userfault() from 7 to 3, cleaning up the code and stack.\n\nLink: https://lkml.kernel.org/r/20240221234732.187629-4-vishal.moola@gmail.com\nSigned-off-by: Vishal Moola (Oracle) <vishal.moola@gmail.com>\nReviewed-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Muchun Song <muchun.song@linux.dev>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/memory: change vmf_anon_prepare() to be non-static", "commit_message": "Patch series \"Handle hugetlb faults under the VMA lock\", v2.\n\nIt is generally safe to handle hugetlb faults under the VMA lock.  The\nonly time this is unsafe is when no anon_vma has been allocated to this\nvma yet, so we can use vmf_anon_prepare() instead of anon_vma_prepare() to\nbailout if necessary.  This should only happen for the first hugetlb page\nin the vma.\n\nAdditionally, this patchset begins to use struct vm_fault within\nhugetlb_fault().  This works towards cleaning up hugetlb code, and should\nsignificantly reduce the number of arguments passed to functions.\n\nThe last patch in this series may cause ltp hugemmap10 to \"fail\".  This is\nbecause vmf_anon_prepare() may bailout with no anon_vma under the VMA lock\nafter allocating a folio for the hugepage.  In free_huge_folio(), this\nfolio is completely freed on bailout iff there is a surplus of hugetlb\npages.  This will remove a folio off the freelist and decrement the number\nof hugepages while ltp expects these counters to remain unchanged on\nfailure.  The rest of the ltp testcases pass.\n\n\nThis patch (of 2):\n\nIn order to handle hugetlb faults under the VMA lock, hugetlb can use\nvmf_anon_prepare() to ensure we can safely prepare an anon_vma.  Change it\nto be a non-static function so it can be used within hugetlb as well.\n\nLink: https://lkml.kernel.org/r/20240221234732.187629-6-vishal.moola@gmail.com\nLink: https://lkml.kernel.org/r/20240221234732.187629-2-vishal.moola@gmail.com\nSigned-off-by: Vishal Moola (Oracle) <vishal.moola@gmail.com>\nReviewed-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Muchun Song <muchun.song@linux.dev>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}],
        "proper_nouns": [{"VMA (Virtual Memory Area)": "A VMA is a contiguous range of virtual memory addresses in a process's address space. Each VMA represents a segment of memory that has specific attributes, such as whether it is readable, writable, or executable, and whether it is backed by a file or anonymous memory."}, {"hugetlb": "Hugetlb is a feature in the Linux kernel that allows the use of large memory pages, known as huge pages. These pages are significantly larger than the standard 4KB pages, often being 2MB or 1GB in size. Using huge pages can reduce the overhead associated with managing a large number of small pages, thereby improving memory performance."}, {"vm_fault": "A vm_fault is a structure in the Linux kernel that represents a page fault event. It contains information about the fault, such as the virtual address that caused the fault, the type of fault (e.g., read, write), and the VMA associated with the fault."}, {"anon_vma": "An anon_vma is an anonymous virtual memory area in the Linux kernel. It is used to manage the reverse mapping of anonymous pages, which are pages not backed by a file. The anon_vma structure helps in tracking the relationships between pages and their users, which is crucial for tasks like page migration and reclamation."}, {"vmf_anon_prepare()": "vmf_anon_prepare() is a function in the Linux kernel that prepares an anonymous virtual memory area (anon_vma) for a given VMA. It is used to ensure that the necessary structures are in place before handling a page fault. This function is particularly useful in scenarios where the fault handling needs to be done under the VMA lock."}],
        "background_knowledge": ["Handling hugetlb faults under the VMA lock requires understanding the interaction between huge pages and the VMA lock. The VMA lock is a mechanism used to protect the VMA from concurrent modifications, ensuring that page faults and other operations on the VMA are handled safely. Previously, hugetlb faults were not handled under the VMA lock due to potential issues with anon_vma allocation. This feature addresses that limitation by using vmf_anon_prepare() to safely handle anon_vma allocation under the VMA lock."],
        "kernel_changes": ["Before this feature, hugetlb faults were not handled under the VMA lock due to the risk of anon_vma allocation failing, which could lead to inconsistencies. The kernel has been modified to use vmf_anon_prepare() instead of anon_vma_prepare() to handle anon_vma allocation safely under the VMA lock. This change allows hugetlb faults to be handled more efficiently and safely, reducing the risk of inconsistencies and improving the overall performance of memory management in the kernel."],
        "comprehensive_understanding": "This feature enhances the handling of hugetlb faults by allowing them to be processed under the VMA lock. This is achieved by replacing the use of anon_vma_prepare() with vmf_anon_prepare(), which ensures that anon_vma allocation can be safely performed under the VMA lock. This change not only improves the efficiency of fault handling but also reduces the risk of inconsistencies in memory management. By enabling hugetlb faults to be handled under the VMA lock, the kernel can manage memory more effectively, leading to better performance and stability."
    }
}

2024-12-03 22:27:51,820 - extractor - DEBUG - deepseek response:
ChatCompletion(id='844a0488-624f-4cf3-806a-bc69e48fa12e', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "concepts": ["VMA (Virtual Memory Area)", "hugetlb", "vm_fault", "anon_vma", "vmf_anon_prepare()", "VMA lock", "huge pages", "memory management", "page fault handling", "anon_vma allocation"]\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733236067, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=72, prompt_tokens=3855, total_tokens=3927, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=1536, prompt_cache_miss_tokens=2319))
2024-12-03 22:27:51,820 - extractor - DEBUG - feature_extracted of feature_id=33231:
{
    "feature_id": 33231,
    "feature_description": "Handle hugetlb faults under the VMA lock",
    "version": "6.9",
    "commits": [
        {
            "commit_subject": "hugetlb: allow faults to be handled under the VMA lock",
            "commit_message": "Hugetlb can now safely handle faults under the VMA lock, so allow it to do\nso.\n\nThis patch may cause ltp hugemmap10 to \"fail\".  Hugemmap10 tests hugetlb\ncounters, and expects the counters to remain unchanged on failure to\nhandle a fault.\n\nIn hugetlb_no_page(), vmf_anon_prepare() may bailout with no anon_vma\nunder the VMA lock after allocating a folio for the hugepage.  In\nfree_huge_folio(), this folio is completely freed on bailout iff there is\na surplus of hugetlb pages.  This will remove a folio off the freelist and\ndecrement the number of hugepages while ltp expects these counters to\nremain unchanged on failure.\n\nOriginally this could only happen due to OOM failures, but now it may also\noccur after we allocate a hugetlb folio without a suitable anon_vma under\nthe VMA lock.  This should only happen for the first freshly allocated\nhugepage in this vma.\n\nLink: https://lkml.kernel.org/r/20240221234732.187629-6-vishal.moola@gmail.com\nSigned-off-by: Vishal Moola (Oracle) <vishal.moola@gmail.com>\nReviewed-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Muchun Song <muchun.song@linux.dev>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "hugetlb: use vmf_anon_prepare() instead of anon_vma_prepare()",
            "commit_message": "hugetlb_no_page() and hugetlb_wp() call anon_vma_prepare().  In\npreparation for hugetlb to safely handle faults under the VMA lock, use\nvmf_anon_prepare() here instead.\n\nAdditionally, passing hugetlb_wp() the vm_fault struct from\nhugetlb_fault() works toward cleaning up the hugetlb code and function\nstack.\n\nLink: https://lkml.kernel.org/r/20240221234732.187629-5-vishal.moola@gmail.com\nSigned-off-by: Vishal Moola (Oracle) <vishal.moola@gmail.com>\nReviewed-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Muchun Song <muchun.song@linux.dev>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "hugetlb: pass struct vm_fault through to hugetlb_handle_userfault()",
            "commit_message": "Now that hugetlb_fault() has a struct vm_fault, have\nhugetlb_handle_userfault() use it instead of creating one of its own.\n\nThis lets us reduce the number of arguments passed to\nhugetlb_handle_userfault() from 7 to 3, cleaning up the code and stack.\n\nLink: https://lkml.kernel.org/r/20240221234732.187629-4-vishal.moola@gmail.com\nSigned-off-by: Vishal Moola (Oracle) <vishal.moola@gmail.com>\nReviewed-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Muchun Song <muchun.song@linux.dev>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm/memory: change vmf_anon_prepare() to be non-static",
            "commit_message": "Patch series \"Handle hugetlb faults under the VMA lock\", v2.\n\nIt is generally safe to handle hugetlb faults under the VMA lock.  The\nonly time this is unsafe is when no anon_vma has been allocated to this\nvma yet, so we can use vmf_anon_prepare() instead of anon_vma_prepare() to\nbailout if necessary.  This should only happen for the first hugetlb page\nin the vma.\n\nAdditionally, this patchset begins to use struct vm_fault within\nhugetlb_fault().  This works towards cleaning up hugetlb code, and should\nsignificantly reduce the number of arguments passed to functions.\n\nThe last patch in this series may cause ltp hugemmap10 to \"fail\".  This is\nbecause vmf_anon_prepare() may bailout with no anon_vma under the VMA lock\nafter allocating a folio for the hugepage.  In free_huge_folio(), this\nfolio is completely freed on bailout iff there is a surplus of hugetlb\npages.  This will remove a folio off the freelist and decrement the number\nof hugepages while ltp expects these counters to remain unchanged on\nfailure.  The rest of the ltp testcases pass.\n\n\nThis patch (of 2):\n\nIn order to handle hugetlb faults under the VMA lock, hugetlb can use\nvmf_anon_prepare() to ensure we can safely prepare an anon_vma.  Change it\nto be a non-static function so it can be used within hugetlb as well.\n\nLink: https://lkml.kernel.org/r/20240221234732.187629-6-vishal.moola@gmail.com\nLink: https://lkml.kernel.org/r/20240221234732.187629-2-vishal.moola@gmail.com\nSigned-off-by: Vishal Moola (Oracle) <vishal.moola@gmail.com>\nReviewed-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Muchun Song <muchun.song@linux.dev>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        }
    ],
    "proper_nouns": [
        {
            "VMA (Virtual Memory Area)": "A VMA is a contiguous range of virtual memory addresses in a process's address space. Each VMA represents a segment of memory that has specific attributes, such as whether it is readable, writable, or executable, and whether it is backed by a file or anonymous memory."
        },
        {
            "hugetlb": "Hugetlb is a feature in the Linux kernel that allows the use of large memory pages, known as huge pages. These pages are significantly larger than the standard 4KB pages, often being 2MB or 1GB in size. Using huge pages can reduce the overhead associated with managing a large number of small pages, thereby improving memory performance."
        },
        {
            "vm_fault": "A vm_fault is a structure in the Linux kernel that represents a page fault event. It contains information about the fault, such as the virtual address that caused the fault, the type of fault (e.g., read, write), and the VMA associated with the fault."
        },
        {
            "anon_vma": "An anon_vma is an anonymous virtual memory area in the Linux kernel. It is used to manage the reverse mapping of anonymous pages, which are pages not backed by a file. The anon_vma structure helps in tracking the relationships between pages and their users, which is crucial for tasks like page migration and reclamation."
        },
        {
            "vmf_anon_prepare()": "vmf_anon_prepare() is a function in the Linux kernel that prepares an anonymous virtual memory area (anon_vma) for a given VMA. It is used to ensure that the necessary structures are in place before handling a page fault. This function is particularly useful in scenarios where the fault handling needs to be done under the VMA lock."
        }
    ],
    "background_knowledge": [
        "Handling hugetlb faults under the VMA lock requires understanding the interaction between huge pages and the VMA lock. The VMA lock is a mechanism used to protect the VMA from concurrent modifications, ensuring that page faults and other operations on the VMA are handled safely. Previously, hugetlb faults were not handled under the VMA lock due to potential issues with anon_vma allocation. This feature addresses that limitation by using vmf_anon_prepare() to safely handle anon_vma allocation under the VMA lock."
    ],
    "kernel_changes": [
        "Before this feature, hugetlb faults were not handled under the VMA lock due to the risk of anon_vma allocation failing, which could lead to inconsistencies. The kernel has been modified to use vmf_anon_prepare() instead of anon_vma_prepare() to handle anon_vma allocation safely under the VMA lock. This change allows hugetlb faults to be handled more efficiently and safely, reducing the risk of inconsistencies and improving the overall performance of memory management in the kernel."
    ],
    "comprehensive_understanding": "This feature enhances the handling of hugetlb faults by allowing them to be processed under the VMA lock. This is achieved by replacing the use of anon_vma_prepare() with vmf_anon_prepare(), which ensures that anon_vma allocation can be safely performed under the VMA lock. This change not only improves the efficiency of fault handling but also reduces the risk of inconsistencies in memory management. By enabling hugetlb faults to be handled under the VMA lock, the kernel can manage memory more effectively, leading to better performance and stability.",
    "concepts": [
        "VMA (Virtual Memory Area)",
        "hugetlb",
        "vm_fault",
        "anon_vma",
        "vmf_anon_prepare()",
        "VMA lock",
        "huge pages",
        "memory management",
        "page fault handling",
        "anon_vma allocation"
    ]
}
2024-12-03 22:27:51,821 - extractor - INFO - ---------Processing feature 10/815---------
2024-12-03 22:27:56,843 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个 linux 内核的专家。给你一个 linux 内核特性的描述以及其对应的若干 commit 的标题和提交信息，你的任务是为这个特性写一个多方面的扩写版描述。你的扩写需包含以下几个方面：1. 该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)；2. 为了让开发者理解该特性，需要哪些具体的，与这个特性直接相关的背景知识 (background_knowledge)；3. 该特性被实现前后，linux 内核发生了怎样的变化 (kernel_changes)；4. 你对该特性的综合理解 (comprehensive_understanding)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}]
        },
        "output": {
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
        }
    ],
    "input": {
        "feature_description": "zswap: optimize for dynamic zswap_pools",
        "commits": [{"commit_subject": "mm/zswap: change zswap_pool kref to percpu_ref", "commit_message": "All zswap entries will take a reference of zswap_pool when zswap_store(),\nand drop it when free.  Change it to use the percpu_ref is better for\nscalability performance.\n\nAlthough percpu_ref use a bit more memory which should be ok for our use\ncase, since we almost have only one zswap_pool to be using.  The\nperformance gain is for zswap_store/load hotpath.\n\nTesting kernel build (32 threads) in tmpfs with memory.max=2GB.  (zswap\nshrinker and writeback enabled with one 50GB swapfile, on a 128 CPUs\nx86-64 machine, below is the average of 5 runs)\n\n        mm-unstable  zswap-global-lru\nreal    63.20        63.12\nuser    1061.75      1062.95\nsys     268.74       264.44\n\n[chengming.zhou@linux.dev: fix zswap_pools_lock usages after changing to percpu_ref]\n  Link: https://lkml.kernel.org/r/20240228154954.3028626-1-chengming.zhou@linux.dev\nLink: https://lkml.kernel.org/r/20240210-zswap-global-lru-v3-2-200495333595@bytedance.com\nSigned-off-by: Chengming Zhou <zhouchengming@bytedance.com>\nReviewed-by: Nhat Pham <nphamcs@gmail.com>\nCc: Johannes Weiner <hannes@cmpxchg.org>\nCc: Yosry Ahmed <yosryahmed@google.com>\nCc: Chengming Zhou <chengming.zhou@linux.dev>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/zswap: global lru and shrinker shared by all zswap_pools", "commit_message": "Patch series \"mm/zswap: optimize for dynamic zswap_pools\", v3.\n\nDynamic pool creation has been supported for a long time, which maybe not\nused so much in practice.  But with the per-memcg lru merged, the current\nstructure of zswap_pool's lru and shrinker become less optimal.\n\nIn the current structure, each zswap_pool has its own lru, shrinker and\nshrink_work, but only the latest zswap_pool will be the current used.\n\n1. When memory has pressure, all shrinkers of zswap_pools will try to\n   shrink its lru list, there is no order between them.\n\n2. When zswap limit hit, only the last zswap_pool's shrink_work will\n   try to shrink its own lru, which is inefficient.\n\nA more natural way is to have a global zswap lru shared between all\nzswap_pools, and so is the shrinker. The code becomes much simpler too.\n\nAnother optimization is changing zswap_pool kref to percpu_ref, which will\nbe taken reference by every zswap entry.  So the scalability is better.\n\nTesting kernel build (32 threads) in tmpfs with memory.max=2GB.  (zswap\nshrinker and writeback enabled with one 50GB swapfile, on a 128 CPUs\nx86-64 machine, below is the average of 5 runs)\n\n        mm-unstable  zswap-global-lru\nreal    63.20        63.12\nuser    1061.75      1062.95\nsys     268.74       264.44\n\n\nThis patch (of 3):\n\nDynamic zswap_pool creation may create/reuse to have multiple zswap_pools\nin a list, only the first will be current used.\n\nEach zswap_pool has its own lru and shrinker, which is not necessary and\nhas its problem:\n\n1. When memory has pressure, all shrinker of zswap_pools will\n   try to shrink its own lru, there is no order between them.\n\n2. When zswap limit hit, only the last zswap_pool's shrink_work\n   will try to shrink its lru list. The rationale here was to\n   try and empty the old pool first so that we can completely\n   drop it. However, since we only support exclusive loads now,\n   the LRU ordering should be entirely decided by the order of\n   stores, so the oldest entries on the LRU will naturally be\n   from the oldest pool.\n\nAnyway, having a global lru and shrinker shared by all zswap_pools is\nbetter and efficient.\n\nLink: https://lkml.kernel.org/r/20240210-zswap-global-lru-v3-0-200495333595@bytedance.com\nLink: https://lkml.kernel.org/r/20240210-zswap-global-lru-v3-1-200495333595@bytedance.com\nSigned-off-by: Chengming Zhou <zhouchengming@bytedance.com>\nAcked-by: Yosry Ahmed <yosryahmed@google.com>\nCc: Johannes Weiner <hannes@cmpxchg.org>\nCc: Nhat Pham <nphamcs@gmail.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}]
    }
}

2024-12-03 22:28:41,100 - extractor - DEBUG - deepseek response:
ChatCompletion(id='0f8387b1-b2a7-4a99-a572-67eee11fda81', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "proper_nouns": [\n        {\n            "zswap": "zswap is a lightweight compressed cache in the Linux kernel that sits between anonymous memory and swap space. It is designed to reduce the number of disk I/O operations by compressing memory pages before they are swapped out to disk. This can significantly improve performance in systems with limited swap space or high I/O latency."\n        },\n        {\n            "zswap_pool": "A zswap_pool is a logical container within the zswap subsystem that holds compressed memory pages. Each zswap_pool is associated with a specific compression algorithm and can dynamically grow or shrink based on the system\'s memory usage and swap activity."\n        },\n        {\n            "percpu_ref": "percpu_ref is a reference counting mechanism in the Linux kernel that allows for per-CPU reference counting. This means that each CPU core maintains its own reference count, which can improve scalability and performance in multi-core systems by reducing contention on shared data structures."\n        },\n        {\n            "LRU (Least Recently Used)": "LRU is a cache eviction policy used in the Linux kernel to manage memory pages. It works by evicting the least recently used pages first, which helps to keep the most frequently accessed pages in memory, thereby improving cache efficiency."\n        },\n        {\n            "shrinker": "A shrinker is a callback mechanism in the Linux kernel that allows subsystems to reclaim memory when the system is under memory pressure. Shrinkers are used to free up memory by evicting cached data or releasing unused resources."\n        }\n    ],\n    "background_knowledge": [\n        "The zswap subsystem in the Linux kernel is designed to reduce the number of disk I/O operations by compressing memory pages before they are swapped out to disk. This can significantly improve performance in systems with limited swap space or high I/O latency.",\n        "Dynamic zswap_pool creation allows the system to create or reuse multiple zswap_pools, but only the first one is currently used. Each zswap_pool has its own LRU and shrinker, which can lead to inefficiencies when multiple pools are active.",\n        "The percpu_ref mechanism is used to improve scalability by allowing each CPU core to maintain its own reference count, reducing contention on shared data structures."\n    ],\n    "kernel_changes": [\n        "Before this optimization, each zswap_pool had its own LRU and shrinker, which could lead to inefficiencies when multiple pools were active. The system would attempt to shrink all pools simultaneously when under memory pressure, leading to redundant operations and potential contention.",\n        "After the optimization, a global LRU and shrinker are shared by all zswap_pools. This reduces redundancy and improves efficiency by ensuring that memory pressure is handled uniformly across all pools. Additionally, the use of percpu_ref for zswap_pool references improves scalability by reducing contention on shared data structures.",\n        "The performance improvements were measured using a kernel build test with 32 threads, where the system was constrained to use only 2GB of memory. The results showed a slight improvement in system performance, with reduced real and user time, and a decrease in system time."\n    ],\n    "comprehensive_understanding": "This feature optimizes the zswap subsystem in the Linux kernel by introducing a global LRU and shrinker shared by all zswap_pools. This change addresses inefficiencies in the previous implementation, where each pool had its own LRU and shrinker, leading to redundant operations and potential contention. By consolidating these mechanisms, the system can more efficiently manage memory pressure and improve cache efficiency. Additionally, the use of percpu_ref for zswap_pool references enhances scalability by reducing contention on shared data structures. Overall, this optimization improves the performance and resource utilization of the zswap subsystem, particularly in multi-core systems with dynamic zswap_pool usage."\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733236076, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=839, prompt_tokens=2964, total_tokens=3803, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=1472, prompt_cache_miss_tokens=1492))
2024-12-03 22:28:41,101 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个命名实体抽取 (NER) 任务的专家，同时也是 linux 内核的专家。给你一个 linux 内核特性的描述 (feature_description)、对应的若干 commit 的标题和提交信息 (commits)、该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)、为了让开发者理解该特性所需的与这个特性直接相关的背景知识 (background_knowledge)、该特性被实现前后，linux 内核发生的变化 (kernel_changes)、对该特性的综合理解 (comprehensive_understanding)。你的任务是：根据这些信息提取出在 linux 内核中涉及到的和这个特性相关的概念实体 (concepts)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}],
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": [
                "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
            ]
        },
        "output": {
            "concepts": ["Copy-on-Write (CoW)", "mTHP", "folio", "memory reclamation", "memory availability", "anti-fragmentation"]
        }
    ],
    "input": {
        "feature_description": "zswap: optimize for dynamic zswap_pools",
        "commits": [{"commit_subject": "mm/zswap: change zswap_pool kref to percpu_ref", "commit_message": "All zswap entries will take a reference of zswap_pool when zswap_store(),\nand drop it when free.  Change it to use the percpu_ref is better for\nscalability performance.\n\nAlthough percpu_ref use a bit more memory which should be ok for our use\ncase, since we almost have only one zswap_pool to be using.  The\nperformance gain is for zswap_store/load hotpath.\n\nTesting kernel build (32 threads) in tmpfs with memory.max=2GB.  (zswap\nshrinker and writeback enabled with one 50GB swapfile, on a 128 CPUs\nx86-64 machine, below is the average of 5 runs)\n\n        mm-unstable  zswap-global-lru\nreal    63.20        63.12\nuser    1061.75      1062.95\nsys     268.74       264.44\n\n[chengming.zhou@linux.dev: fix zswap_pools_lock usages after changing to percpu_ref]\n  Link: https://lkml.kernel.org/r/20240228154954.3028626-1-chengming.zhou@linux.dev\nLink: https://lkml.kernel.org/r/20240210-zswap-global-lru-v3-2-200495333595@bytedance.com\nSigned-off-by: Chengming Zhou <zhouchengming@bytedance.com>\nReviewed-by: Nhat Pham <nphamcs@gmail.com>\nCc: Johannes Weiner <hannes@cmpxchg.org>\nCc: Yosry Ahmed <yosryahmed@google.com>\nCc: Chengming Zhou <chengming.zhou@linux.dev>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/zswap: global lru and shrinker shared by all zswap_pools", "commit_message": "Patch series \"mm/zswap: optimize for dynamic zswap_pools\", v3.\n\nDynamic pool creation has been supported for a long time, which maybe not\nused so much in practice.  But with the per-memcg lru merged, the current\nstructure of zswap_pool's lru and shrinker become less optimal.\n\nIn the current structure, each zswap_pool has its own lru, shrinker and\nshrink_work, but only the latest zswap_pool will be the current used.\n\n1. When memory has pressure, all shrinkers of zswap_pools will try to\n   shrink its lru list, there is no order between them.\n\n2. When zswap limit hit, only the last zswap_pool's shrink_work will\n   try to shrink its own lru, which is inefficient.\n\nA more natural way is to have a global zswap lru shared between all\nzswap_pools, and so is the shrinker. The code becomes much simpler too.\n\nAnother optimization is changing zswap_pool kref to percpu_ref, which will\nbe taken reference by every zswap entry.  So the scalability is better.\n\nTesting kernel build (32 threads) in tmpfs with memory.max=2GB.  (zswap\nshrinker and writeback enabled with one 50GB swapfile, on a 128 CPUs\nx86-64 machine, below is the average of 5 runs)\n\n        mm-unstable  zswap-global-lru\nreal    63.20        63.12\nuser    1061.75      1062.95\nsys     268.74       264.44\n\n\nThis patch (of 3):\n\nDynamic zswap_pool creation may create/reuse to have multiple zswap_pools\nin a list, only the first will be current used.\n\nEach zswap_pool has its own lru and shrinker, which is not necessary and\nhas its problem:\n\n1. When memory has pressure, all shrinker of zswap_pools will\n   try to shrink its own lru, there is no order between them.\n\n2. When zswap limit hit, only the last zswap_pool's shrink_work\n   will try to shrink its lru list. The rationale here was to\n   try and empty the old pool first so that we can completely\n   drop it. However, since we only support exclusive loads now,\n   the LRU ordering should be entirely decided by the order of\n   stores, so the oldest entries on the LRU will naturally be\n   from the oldest pool.\n\nAnyway, having a global lru and shrinker shared by all zswap_pools is\nbetter and efficient.\n\nLink: https://lkml.kernel.org/r/20240210-zswap-global-lru-v3-0-200495333595@bytedance.com\nLink: https://lkml.kernel.org/r/20240210-zswap-global-lru-v3-1-200495333595@bytedance.com\nSigned-off-by: Chengming Zhou <zhouchengming@bytedance.com>\nAcked-by: Yosry Ahmed <yosryahmed@google.com>\nCc: Johannes Weiner <hannes@cmpxchg.org>\nCc: Nhat Pham <nphamcs@gmail.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}],
        "proper_nouns": [{"zswap": "zswap is a lightweight compressed cache in the Linux kernel that sits between anonymous memory and swap space. It is designed to reduce the number of disk I/O operations by compressing memory pages before they are swapped out to disk. This can significantly improve performance in systems with limited swap space or high I/O latency."}, {"zswap_pool": "A zswap_pool is a logical container within the zswap subsystem that holds compressed memory pages. Each zswap_pool is associated with a specific compression algorithm and can dynamically grow or shrink based on the system's memory usage and swap activity."}, {"percpu_ref": "percpu_ref is a reference counting mechanism in the Linux kernel that allows for per-CPU reference counting. This means that each CPU core maintains its own reference count, which can improve scalability and performance in multi-core systems by reducing contention on shared data structures."}, {"LRU (Least Recently Used)": "LRU is a cache eviction policy used in the Linux kernel to manage memory pages. It works by evicting the least recently used pages first, which helps to keep the most frequently accessed pages in memory, thereby improving cache efficiency."}, {"shrinker": "A shrinker is a callback mechanism in the Linux kernel that allows subsystems to reclaim memory when the system is under memory pressure. Shrinkers are used to free up memory by evicting cached data or releasing unused resources."}],
        "background_knowledge": ["The zswap subsystem in the Linux kernel is designed to reduce the number of disk I/O operations by compressing memory pages before they are swapped out to disk. This can significantly improve performance in systems with limited swap space or high I/O latency.", "Dynamic zswap_pool creation allows the system to create or reuse multiple zswap_pools, but only the first one is currently used. Each zswap_pool has its own LRU and shrinker, which can lead to inefficiencies when multiple pools are active.", "The percpu_ref mechanism is used to improve scalability by allowing each CPU core to maintain its own reference count, reducing contention on shared data structures."],
        "kernel_changes": ["Before this optimization, each zswap_pool had its own LRU and shrinker, which could lead to inefficiencies when multiple pools were active. The system would attempt to shrink all pools simultaneously when under memory pressure, leading to redundant operations and potential contention.", "After the optimization, a global LRU and shrinker are shared by all zswap_pools. This reduces redundancy and improves efficiency by ensuring that memory pressure is handled uniformly across all pools. Additionally, the use of percpu_ref for zswap_pool references improves scalability by reducing contention on shared data structures.", "The performance improvements were measured using a kernel build test with 32 threads, where the system was constrained to use only 2GB of memory. The results showed a slight improvement in system performance, with reduced real and user time, and a decrease in system time."],
        "comprehensive_understanding": "This feature optimizes the zswap subsystem in the Linux kernel by introducing a global LRU and shrinker shared by all zswap_pools. This change addresses inefficiencies in the previous implementation, where each pool had its own LRU and shrinker, leading to redundant operations and potential contention. By consolidating these mechanisms, the system can more efficiently manage memory pressure and improve cache efficiency. Additionally, the use of percpu_ref for zswap_pool references enhances scalability by reducing contention on shared data structures. Overall, this optimization improves the performance and resource utilization of the zswap subsystem, particularly in multi-core systems with dynamic zswap_pool usage."
    }
}

2024-12-03 22:28:45,877 - extractor - DEBUG - deepseek response:
ChatCompletion(id='ae197729-77a7-4779-8ce7-c8021daacfe1', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "concepts": ["zswap", "zswap_pool", "percpu_ref", "LRU (Least Recently Used)", "shrinker", "memory pressure", "cache efficiency", "scalability", "multi-core systems", "swap space", "disk I/O operations", "compression", "memory management"]\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733236121, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=78, prompt_tokens=3795, total_tokens=3873, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=1536, prompt_cache_miss_tokens=2259))
2024-12-03 22:28:45,877 - extractor - DEBUG - feature_extracted of feature_id=33230:
{
    "feature_id": 33230,
    "feature_description": "zswap: optimize for dynamic zswap_pools",
    "version": "6.9",
    "commits": [
        {
            "commit_subject": "mm/zswap: change zswap_pool kref to percpu_ref",
            "commit_message": "All zswap entries will take a reference of zswap_pool when zswap_store(),\nand drop it when free.  Change it to use the percpu_ref is better for\nscalability performance.\n\nAlthough percpu_ref use a bit more memory which should be ok for our use\ncase, since we almost have only one zswap_pool to be using.  The\nperformance gain is for zswap_store/load hotpath.\n\nTesting kernel build (32 threads) in tmpfs with memory.max=2GB.  (zswap\nshrinker and writeback enabled with one 50GB swapfile, on a 128 CPUs\nx86-64 machine, below is the average of 5 runs)\n\n        mm-unstable  zswap-global-lru\nreal    63.20        63.12\nuser    1061.75      1062.95\nsys     268.74       264.44\n\n[chengming.zhou@linux.dev: fix zswap_pools_lock usages after changing to percpu_ref]\n  Link: https://lkml.kernel.org/r/20240228154954.3028626-1-chengming.zhou@linux.dev\nLink: https://lkml.kernel.org/r/20240210-zswap-global-lru-v3-2-200495333595@bytedance.com\nSigned-off-by: Chengming Zhou <zhouchengming@bytedance.com>\nReviewed-by: Nhat Pham <nphamcs@gmail.com>\nCc: Johannes Weiner <hannes@cmpxchg.org>\nCc: Yosry Ahmed <yosryahmed@google.com>\nCc: Chengming Zhou <chengming.zhou@linux.dev>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm/zswap: global lru and shrinker shared by all zswap_pools",
            "commit_message": "Patch series \"mm/zswap: optimize for dynamic zswap_pools\", v3.\n\nDynamic pool creation has been supported for a long time, which maybe not\nused so much in practice.  But with the per-memcg lru merged, the current\nstructure of zswap_pool's lru and shrinker become less optimal.\n\nIn the current structure, each zswap_pool has its own lru, shrinker and\nshrink_work, but only the latest zswap_pool will be the current used.\n\n1. When memory has pressure, all shrinkers of zswap_pools will try to\n   shrink its lru list, there is no order between them.\n\n2. When zswap limit hit, only the last zswap_pool's shrink_work will\n   try to shrink its own lru, which is inefficient.\n\nA more natural way is to have a global zswap lru shared between all\nzswap_pools, and so is the shrinker. The code becomes much simpler too.\n\nAnother optimization is changing zswap_pool kref to percpu_ref, which will\nbe taken reference by every zswap entry.  So the scalability is better.\n\nTesting kernel build (32 threads) in tmpfs with memory.max=2GB.  (zswap\nshrinker and writeback enabled with one 50GB swapfile, on a 128 CPUs\nx86-64 machine, below is the average of 5 runs)\n\n        mm-unstable  zswap-global-lru\nreal    63.20        63.12\nuser    1061.75      1062.95\nsys     268.74       264.44\n\n\nThis patch (of 3):\n\nDynamic zswap_pool creation may create/reuse to have multiple zswap_pools\nin a list, only the first will be current used.\n\nEach zswap_pool has its own lru and shrinker, which is not necessary and\nhas its problem:\n\n1. When memory has pressure, all shrinker of zswap_pools will\n   try to shrink its own lru, there is no order between them.\n\n2. When zswap limit hit, only the last zswap_pool's shrink_work\n   will try to shrink its lru list. The rationale here was to\n   try and empty the old pool first so that we can completely\n   drop it. However, since we only support exclusive loads now,\n   the LRU ordering should be entirely decided by the order of\n   stores, so the oldest entries on the LRU will naturally be\n   from the oldest pool.\n\nAnyway, having a global lru and shrinker shared by all zswap_pools is\nbetter and efficient.\n\nLink: https://lkml.kernel.org/r/20240210-zswap-global-lru-v3-0-200495333595@bytedance.com\nLink: https://lkml.kernel.org/r/20240210-zswap-global-lru-v3-1-200495333595@bytedance.com\nSigned-off-by: Chengming Zhou <zhouchengming@bytedance.com>\nAcked-by: Yosry Ahmed <yosryahmed@google.com>\nCc: Johannes Weiner <hannes@cmpxchg.org>\nCc: Nhat Pham <nphamcs@gmail.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        }
    ],
    "proper_nouns": [
        {
            "zswap": "zswap is a lightweight compressed cache in the Linux kernel that sits between anonymous memory and swap space. It is designed to reduce the number of disk I/O operations by compressing memory pages before they are swapped out to disk. This can significantly improve performance in systems with limited swap space or high I/O latency."
        },
        {
            "zswap_pool": "A zswap_pool is a logical container within the zswap subsystem that holds compressed memory pages. Each zswap_pool is associated with a specific compression algorithm and can dynamically grow or shrink based on the system's memory usage and swap activity."
        },
        {
            "percpu_ref": "percpu_ref is a reference counting mechanism in the Linux kernel that allows for per-CPU reference counting. This means that each CPU core maintains its own reference count, which can improve scalability and performance in multi-core systems by reducing contention on shared data structures."
        },
        {
            "LRU (Least Recently Used)": "LRU is a cache eviction policy used in the Linux kernel to manage memory pages. It works by evicting the least recently used pages first, which helps to keep the most frequently accessed pages in memory, thereby improving cache efficiency."
        },
        {
            "shrinker": "A shrinker is a callback mechanism in the Linux kernel that allows subsystems to reclaim memory when the system is under memory pressure. Shrinkers are used to free up memory by evicting cached data or releasing unused resources."
        }
    ],
    "background_knowledge": [
        "The zswap subsystem in the Linux kernel is designed to reduce the number of disk I/O operations by compressing memory pages before they are swapped out to disk. This can significantly improve performance in systems with limited swap space or high I/O latency.",
        "Dynamic zswap_pool creation allows the system to create or reuse multiple zswap_pools, but only the first one is currently used. Each zswap_pool has its own LRU and shrinker, which can lead to inefficiencies when multiple pools are active.",
        "The percpu_ref mechanism is used to improve scalability by allowing each CPU core to maintain its own reference count, reducing contention on shared data structures."
    ],
    "kernel_changes": [
        "Before this optimization, each zswap_pool had its own LRU and shrinker, which could lead to inefficiencies when multiple pools were active. The system would attempt to shrink all pools simultaneously when under memory pressure, leading to redundant operations and potential contention.",
        "After the optimization, a global LRU and shrinker are shared by all zswap_pools. This reduces redundancy and improves efficiency by ensuring that memory pressure is handled uniformly across all pools. Additionally, the use of percpu_ref for zswap_pool references improves scalability by reducing contention on shared data structures.",
        "The performance improvements were measured using a kernel build test with 32 threads, where the system was constrained to use only 2GB of memory. The results showed a slight improvement in system performance, with reduced real and user time, and a decrease in system time."
    ],
    "comprehensive_understanding": "This feature optimizes the zswap subsystem in the Linux kernel by introducing a global LRU and shrinker shared by all zswap_pools. This change addresses inefficiencies in the previous implementation, where each pool had its own LRU and shrinker, leading to redundant operations and potential contention. By consolidating these mechanisms, the system can more efficiently manage memory pressure and improve cache efficiency. Additionally, the use of percpu_ref for zswap_pool references enhances scalability by reducing contention on shared data structures. Overall, this optimization improves the performance and resource utilization of the zswap subsystem, particularly in multi-core systems with dynamic zswap_pool usage.",
    "concepts": [
        "zswap",
        "zswap_pool",
        "percpu_ref",
        "LRU (Least Recently Used)",
        "shrinker",
        "memory pressure",
        "cache efficiency",
        "scalability",
        "multi-core systems",
        "swap space",
        "disk I/O operations",
        "compression",
        "memory management"
    ]
}
2024-12-03 22:28:45,878 - extractor - INFO - ---------Processing feature 11/815---------
2024-12-03 22:29:07,596 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个 linux 内核的专家。给你一个 linux 内核特性的描述以及其对应的若干 commit 的标题和提交信息，你的任务是为这个特性写一个多方面的扩写版描述。你的扩写需包含以下几个方面：1. 该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)；2. 为了让开发者理解该特性，需要哪些具体的，与这个特性直接相关的背景知识 (background_knowledge)；3. 该特性被实现前后，linux 内核发生了怎样的变化 (kernel_changes)；4. 你对该特性的综合理解 (comprehensive_understanding)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}]
        },
        "output": {
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
        }
    ],
    "input": {
        "feature_description": "Enable >0 order folio memory compaction",
        "commits": [{"commit_subject": "mm/compaction: optimize >0 order folio compaction with free page split.", "commit_message": "During migration in a memory compaction, free pages are placed in an array\nof page lists based on their order.  But the desired free page order\n(i.e., the order of a source page) might not be always present, thus\nleading to migration failures and premature compaction termination.  Split\na high order free pages when source migration page has a lower order to\nincrease migration successful rate.\n\nNote: merging free pages when a migration fails and a lower order free\npage is returned via compaction_free() is possible, but there is too much\nwork.  Since the free pages are not buddy pages, it is hard to identify\nthese free pages using existing PFN-based page merging algorithm.\n\nLink: https://lkml.kernel.org/r/20240220183220.1451315-5-zi.yan@sent.com\nSigned-off-by: Zi Yan <ziy@nvidia.com>\nReviewed-by: Baolin Wang <baolin.wang@linux.alibaba.com>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\nTested-by: Baolin Wang <baolin.wang@linux.alibaba.com>\nTested-by: Yu Zhao <yuzhao@google.com>\nCc: Adam Manzanares <a.manzanares@samsung.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Huang Ying <ying.huang@intel.com>\nCc: Johannes Weiner <hannes@cmpxchg.org>\nCc: Kemeng Shi <shikemeng@huaweicloud.com>\nCc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>\nCc: Luis Chamberlain <mcgrof@kernel.org>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Mel Gorman <mgorman@techsingularity.net>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Vishal Moola (Oracle) <vishal.moola@gmail.com>\nCc: Vlastimil Babka <vbabka@suse.cz>\nCc: Yin Fengwei <fengwei.yin@intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/compaction: add support for >0 order folio memory compaction.", "commit_message": "Before last commit, memory compaction only migrates order-0 folios and\nskips >0 order folios.  Last commit splits all >0 order folios during\ncompaction.  This commit migrates >0 order folios during compaction by\nkeeping isolated free pages at their original size without splitting them\ninto order-0 pages and using them directly during migration process.\n\nWhat is different from the prior implementation:\n1. All isolated free pages are kept in a NR_PAGE_ORDERS array of page\n   lists, where each page list stores free pages in the same order.\n2. All free pages are not post_alloc_hook() processed nor buddy pages,\n   although their orders are stored in first page's private like buddy\n   pages.\n3. During migration, in new page allocation time (i.e., in\n   compaction_alloc()), free pages are then processed by post_alloc_hook().\n   When migration fails and a new page is returned (i.e., in\n   compaction_free()), free pages are restored by reversing the\n   post_alloc_hook() operations using newly added\n   free_pages_prepare_fpi_none().\n\nStep 3 is done for a latter optimization that splitting and/or merging\nfree pages during compaction becomes easier.\n\nNote: without splitting free pages, compaction can end prematurely due to\nmigration will return -ENOMEM even if there is free pages.  This happens\nwhen no order-0 free page exist and compaction_alloc() return NULL.\n\nLink: https://lkml.kernel.org/r/20240220183220.1451315-4-zi.yan@sent.com\nSigned-off-by: Zi Yan <ziy@nvidia.com>\nReviewed-by: Baolin Wang <baolin.wang@linux.alibaba.com>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\nTested-by: Baolin Wang <baolin.wang@linux.alibaba.com>\nTested-by: Yu Zhao <yuzhao@google.com>\nCc: Adam Manzanares <a.manzanares@samsung.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Huang Ying <ying.huang@intel.com>\nCc: Johannes Weiner <hannes@cmpxchg.org>\nCc: Kemeng Shi <shikemeng@huaweicloud.com>\nCc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>\nCc: Luis Chamberlain <mcgrof@kernel.org>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Mel Gorman <mgorman@techsingularity.net>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Vishal Moola (Oracle) <vishal.moola@gmail.com>\nCc: Vlastimil Babka <vbabka@suse.cz>\nCc: Yin Fengwei <fengwei.yin@intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/compaction: enable compacting >0 order folios.", "commit_message": "migrate_pages() supports >0 order folio migration and during compaction,\neven if compaction_alloc() cannot provide >0 order free pages,\nmigrate_pages() can split the source page and try to migrate the base\npages from the split.  It can be a baseline and start point for adding\nsupport for compacting >0 order folios.\n\nLink: https://lkml.kernel.org/r/20240220183220.1451315-3-zi.yan@sent.com\nSigned-off-by: Zi Yan <ziy@nvidia.com>\nSuggested-by: Huang Ying <ying.huang@intel.com>\nReviewed-by: Baolin Wang <baolin.wang@linux.alibaba.com>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\nTested-by: Baolin Wang <baolin.wang@linux.alibaba.com>\nTested-by: Yu Zhao <yuzhao@google.com>\nCc: Adam Manzanares <a.manzanares@samsung.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Johannes Weiner <hannes@cmpxchg.org>\nCc: Kemeng Shi <shikemeng@huaweicloud.com>\nCc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>\nCc: Luis Chamberlain <mcgrof@kernel.org>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Mel Gorman <mgorman@techsingularity.net>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Vishal Moola (Oracle) <vishal.moola@gmail.com>\nCc: Vlastimil Babka <vbabka@suse.cz>\nCc: Yin Fengwei <fengwei.yin@intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/page_alloc: remove unused fpi_flags in free_pages_prepare()", "commit_message": "Patch series \"Enable >0 order folio memory compaction\", v7.\n\nThis patchset enables >0 order folio memory compaction, which is one of\nthe prerequisitions for large folio support[1].\n\nI am aware of that split free pages is necessary for folio migration in\ncompaction, since if >0 order free pages are never split and no order-0\nfree page is scanned, compaction will end prematurely due to migration\nreturns -ENOMEM.  Free page split becomes a must instead of an\noptimization.\n\nlkp ncompare results (on a 8-CPU (Intel Xeon E5-2650 v4 @2.20GHz) 16G VM)\nfor default LRU (-no-mglru) and CONFIG_LRU_GEN are shown at the bottom,\ncopied from V3[4].  In sum, most of vm-scalability applications do not see\nperformance change, and the others see ~4% to ~26% performance boost under\ndefault LRU and ~2% to ~6% performance boost under CONFIG_LRU_GEN.\n\nOverview\n===\n\nTo support >0 order folio compaction, the patchset changes how free pages\nused for migration are kept during compaction.  Free pages used to be\nsplit into order-0 pages that are post allocation processed (i.e.,\nPageBuddy flag cleared, page order stored in page->private is zeroed, and\npage reference is set to 1).  Now all free pages are kept in a\nNR_PAGE_ORDER array of page lists based on their order without post\nallocation process.  When migrate_pages() asks for a new page, one of the\nfree pages, based on the requested page order, is then processed and given\nout.  And THP <2MB would need this feature.\n\n\n[1] https://lore.kernel.org/linux-mm/f8d47176-03a8-99bf-a813-b5942830fd73@arm.com/\n[2] https://lore.kernel.org/linux-mm/20231113170157.280181-1-zi.yan@sent.com/\n[3] https://lore.kernel.org/linux-mm/20240123034636.1095672-1-zi.yan@sent.com/\n[4] https://lore.kernel.org/linux-mm/20240202161554.565023-1-zi.yan@sent.com/\n[5] https://lore.kernel.org/linux-mm/20240212163510.859822-1-zi.yan@sent.com/\n[6] https://lore.kernel.org/linux-mm/20240214220420.1229173-1-zi.yan@sent.com/\n[7] https://lore.kernel.org/linux-mm/20240216170432.1268753-1-zi.yan@sent.com/\n\n\nThis patch (of 4):\n\nCommit 0a54864f8dfb (\"kasan: remove PG_skip_kasan_poison flag\") removes\nthe use of fpi_flags in should_skip_kasan_poison() and fpi_flags is only\npassed to should_skip_kasan_poison() in free_pages_prepare().  Remove the\nunused parameter.\n\nLink: https://lkml.kernel.org/r/20240220183220.1451315-1-zi.yan@sent.com\nLink: https://lkml.kernel.org/r/20240220183220.1451315-2-zi.yan@sent.com\nSigned-off-by: Zi Yan <ziy@nvidia.com>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\nReviewed-by: David Hildenbrand <david@redhat.com>\nCc: Adam Manzanares <a.manzanares@samsung.com>\nCc: Baolin Wang <baolin.wang@linux.alibaba.com>\nCc: \"Huang, Ying\" <ying.huang@intel.com>\nCc: Johannes Weiner <hannes@cmpxchg.org>\nCc: Kemeng Shi <shikemeng@huaweicloud.com>\nCc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>\nCc: Luis Chamberlain <mcgrof@kernel.org>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Mel Gorman <mgorman@techsingularity.net>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Vishal Moola (Oracle) <vishal.moola@gmail.com>\nCc: Yin Fengwei <fengwei.yin@intel.com>\nCc: Yu Zhao <yuzhao@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}]
    }
}

2024-12-03 22:29:50,535 - extractor - DEBUG - deepseek response:
ChatCompletion(id='63ac00d6-b6fe-46f6-a9eb-3415cff78cb0', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "proper_nouns": [\n        {\n            "folio": "In the Linux kernel, a \'folio\' is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages."\n        },\n        {\n            "memory compaction": "Memory compaction is a process in the Linux kernel that aims to reduce memory fragmentation by relocating pages of memory. This process involves moving pages to create larger contiguous blocks of free memory, which can then be allocated more efficiently."\n        },\n        {\n            "migrate_pages()": "The \'migrate_pages()\' function is a key component in the Linux kernel\'s memory management system. It is responsible for moving pages of memory from one location to another, often as part of memory compaction or other memory management tasks."\n        },\n        {\n            "compaction_alloc()": "The \'compaction_alloc()\' function is used during memory compaction to allocate new pages for migration. It ensures that the necessary pages are available for the migration process, which is crucial for the success of memory compaction."\n        },\n        {\n            "compaction_free()": "The \'compaction_free()\' function is used during memory compaction to free up pages that are no longer needed after a migration. This function helps in managing the memory resources more efficiently by returning unused pages to the system."\n        }\n    ],\n    "background_knowledge": [\n        "Memory compaction is a critical process in the Linux kernel that aims to reduce memory fragmentation by relocating pages of memory. This process involves moving pages to create larger contiguous blocks of free memory, which can then be allocated more efficiently.",\n        "The \'migrate_pages()\' function is responsible for moving pages of memory from one location to another, often as part of memory compaction or other memory management tasks. It ensures that the necessary pages are available for the migration process, which is crucial for the success of memory compaction.",\n        "The \'compaction_alloc()\' and \'compaction_free()\' functions are used during memory compaction to allocate and free pages, respectively. These functions help in managing the memory resources more efficiently by ensuring that the necessary pages are available for migration and by returning unused pages to the system."\n    ],\n    "kernel_changes": [\n        "Before the implementation of this feature, memory compaction only supported the migration of order-0 folios, which limited its effectiveness in managing larger memory blocks. The new feature enables the compaction of folios with orders greater than 0, thereby improving the efficiency of memory management.",\n        "The kernel was modified to split high-order free pages when the source migration page has a lower order, increasing the success rate of migration. This change helps in reducing the premature termination of compaction due to the unavailability of the desired free page order.",\n        "Additionally, the kernel now keeps isolated free pages at their original size during the migration process, without splitting them into order-0 pages. This approach simplifies the management of free pages and makes it easier to identify and use them during the migration process."\n    ],\n    "comprehensive_understanding": "This feature enhances the Linux kernel\'s memory compaction process by enabling the compaction of folios with orders greater than 0. This improvement is achieved by optimizing the handling of free pages during migration, including the splitting of high-order free pages and the preservation of their original size. These changes significantly enhance the efficiency of memory management, reduce fragmentation, and improve the overall performance of the system."\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733236147, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=794, prompt_tokens=4739, total_tokens=5533, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=4736, prompt_cache_miss_tokens=3))
2024-12-03 22:29:50,536 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个命名实体抽取 (NER) 任务的专家，同时也是 linux 内核的专家。给你一个 linux 内核特性的描述 (feature_description)、对应的若干 commit 的标题和提交信息 (commits)、该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)、为了让开发者理解该特性所需的与这个特性直接相关的背景知识 (background_knowledge)、该特性被实现前后，linux 内核发生的变化 (kernel_changes)、对该特性的综合理解 (comprehensive_understanding)。你的任务是：根据这些信息提取出在 linux 内核中涉及到的和这个特性相关的概念实体 (concepts)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}],
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": [
                "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
            ]
        },
        "output": {
            "concepts": ["Copy-on-Write (CoW)", "mTHP", "folio", "memory reclamation", "memory availability", "anti-fragmentation"]
        }
    ],
    "input": {
        "feature_description": "Enable >0 order folio memory compaction",
        "commits": [{"commit_subject": "mm/compaction: optimize >0 order folio compaction with free page split.", "commit_message": "During migration in a memory compaction, free pages are placed in an array\nof page lists based on their order.  But the desired free page order\n(i.e., the order of a source page) might not be always present, thus\nleading to migration failures and premature compaction termination.  Split\na high order free pages when source migration page has a lower order to\nincrease migration successful rate.\n\nNote: merging free pages when a migration fails and a lower order free\npage is returned via compaction_free() is possible, but there is too much\nwork.  Since the free pages are not buddy pages, it is hard to identify\nthese free pages using existing PFN-based page merging algorithm.\n\nLink: https://lkml.kernel.org/r/20240220183220.1451315-5-zi.yan@sent.com\nSigned-off-by: Zi Yan <ziy@nvidia.com>\nReviewed-by: Baolin Wang <baolin.wang@linux.alibaba.com>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\nTested-by: Baolin Wang <baolin.wang@linux.alibaba.com>\nTested-by: Yu Zhao <yuzhao@google.com>\nCc: Adam Manzanares <a.manzanares@samsung.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Huang Ying <ying.huang@intel.com>\nCc: Johannes Weiner <hannes@cmpxchg.org>\nCc: Kemeng Shi <shikemeng@huaweicloud.com>\nCc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>\nCc: Luis Chamberlain <mcgrof@kernel.org>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Mel Gorman <mgorman@techsingularity.net>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Vishal Moola (Oracle) <vishal.moola@gmail.com>\nCc: Vlastimil Babka <vbabka@suse.cz>\nCc: Yin Fengwei <fengwei.yin@intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/compaction: add support for >0 order folio memory compaction.", "commit_message": "Before last commit, memory compaction only migrates order-0 folios and\nskips >0 order folios.  Last commit splits all >0 order folios during\ncompaction.  This commit migrates >0 order folios during compaction by\nkeeping isolated free pages at their original size without splitting them\ninto order-0 pages and using them directly during migration process.\n\nWhat is different from the prior implementation:\n1. All isolated free pages are kept in a NR_PAGE_ORDERS array of page\n   lists, where each page list stores free pages in the same order.\n2. All free pages are not post_alloc_hook() processed nor buddy pages,\n   although their orders are stored in first page's private like buddy\n   pages.\n3. During migration, in new page allocation time (i.e., in\n   compaction_alloc()), free pages are then processed by post_alloc_hook().\n   When migration fails and a new page is returned (i.e., in\n   compaction_free()), free pages are restored by reversing the\n   post_alloc_hook() operations using newly added\n   free_pages_prepare_fpi_none().\n\nStep 3 is done for a latter optimization that splitting and/or merging\nfree pages during compaction becomes easier.\n\nNote: without splitting free pages, compaction can end prematurely due to\nmigration will return -ENOMEM even if there is free pages.  This happens\nwhen no order-0 free page exist and compaction_alloc() return NULL.\n\nLink: https://lkml.kernel.org/r/20240220183220.1451315-4-zi.yan@sent.com\nSigned-off-by: Zi Yan <ziy@nvidia.com>\nReviewed-by: Baolin Wang <baolin.wang@linux.alibaba.com>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\nTested-by: Baolin Wang <baolin.wang@linux.alibaba.com>\nTested-by: Yu Zhao <yuzhao@google.com>\nCc: Adam Manzanares <a.manzanares@samsung.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Huang Ying <ying.huang@intel.com>\nCc: Johannes Weiner <hannes@cmpxchg.org>\nCc: Kemeng Shi <shikemeng@huaweicloud.com>\nCc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>\nCc: Luis Chamberlain <mcgrof@kernel.org>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Mel Gorman <mgorman@techsingularity.net>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Vishal Moola (Oracle) <vishal.moola@gmail.com>\nCc: Vlastimil Babka <vbabka@suse.cz>\nCc: Yin Fengwei <fengwei.yin@intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/compaction: enable compacting >0 order folios.", "commit_message": "migrate_pages() supports >0 order folio migration and during compaction,\neven if compaction_alloc() cannot provide >0 order free pages,\nmigrate_pages() can split the source page and try to migrate the base\npages from the split.  It can be a baseline and start point for adding\nsupport for compacting >0 order folios.\n\nLink: https://lkml.kernel.org/r/20240220183220.1451315-3-zi.yan@sent.com\nSigned-off-by: Zi Yan <ziy@nvidia.com>\nSuggested-by: Huang Ying <ying.huang@intel.com>\nReviewed-by: Baolin Wang <baolin.wang@linux.alibaba.com>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\nTested-by: Baolin Wang <baolin.wang@linux.alibaba.com>\nTested-by: Yu Zhao <yuzhao@google.com>\nCc: Adam Manzanares <a.manzanares@samsung.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Johannes Weiner <hannes@cmpxchg.org>\nCc: Kemeng Shi <shikemeng@huaweicloud.com>\nCc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>\nCc: Luis Chamberlain <mcgrof@kernel.org>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Mel Gorman <mgorman@techsingularity.net>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Vishal Moola (Oracle) <vishal.moola@gmail.com>\nCc: Vlastimil Babka <vbabka@suse.cz>\nCc: Yin Fengwei <fengwei.yin@intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/page_alloc: remove unused fpi_flags in free_pages_prepare()", "commit_message": "Patch series \"Enable >0 order folio memory compaction\", v7.\n\nThis patchset enables >0 order folio memory compaction, which is one of\nthe prerequisitions for large folio support[1].\n\nI am aware of that split free pages is necessary for folio migration in\ncompaction, since if >0 order free pages are never split and no order-0\nfree page is scanned, compaction will end prematurely due to migration\nreturns -ENOMEM.  Free page split becomes a must instead of an\noptimization.\n\nlkp ncompare results (on a 8-CPU (Intel Xeon E5-2650 v4 @2.20GHz) 16G VM)\nfor default LRU (-no-mglru) and CONFIG_LRU_GEN are shown at the bottom,\ncopied from V3[4].  In sum, most of vm-scalability applications do not see\nperformance change, and the others see ~4% to ~26% performance boost under\ndefault LRU and ~2% to ~6% performance boost under CONFIG_LRU_GEN.\n\nOverview\n===\n\nTo support >0 order folio compaction, the patchset changes how free pages\nused for migration are kept during compaction.  Free pages used to be\nsplit into order-0 pages that are post allocation processed (i.e.,\nPageBuddy flag cleared, page order stored in page->private is zeroed, and\npage reference is set to 1).  Now all free pages are kept in a\nNR_PAGE_ORDER array of page lists based on their order without post\nallocation process.  When migrate_pages() asks for a new page, one of the\nfree pages, based on the requested page order, is then processed and given\nout.  And THP <2MB would need this feature.\n\n\n[1] https://lore.kernel.org/linux-mm/f8d47176-03a8-99bf-a813-b5942830fd73@arm.com/\n[2] https://lore.kernel.org/linux-mm/20231113170157.280181-1-zi.yan@sent.com/\n[3] https://lore.kernel.org/linux-mm/20240123034636.1095672-1-zi.yan@sent.com/\n[4] https://lore.kernel.org/linux-mm/20240202161554.565023-1-zi.yan@sent.com/\n[5] https://lore.kernel.org/linux-mm/20240212163510.859822-1-zi.yan@sent.com/\n[6] https://lore.kernel.org/linux-mm/20240214220420.1229173-1-zi.yan@sent.com/\n[7] https://lore.kernel.org/linux-mm/20240216170432.1268753-1-zi.yan@sent.com/\n\n\nThis patch (of 4):\n\nCommit 0a54864f8dfb (\"kasan: remove PG_skip_kasan_poison flag\") removes\nthe use of fpi_flags in should_skip_kasan_poison() and fpi_flags is only\npassed to should_skip_kasan_poison() in free_pages_prepare().  Remove the\nunused parameter.\n\nLink: https://lkml.kernel.org/r/20240220183220.1451315-1-zi.yan@sent.com\nLink: https://lkml.kernel.org/r/20240220183220.1451315-2-zi.yan@sent.com\nSigned-off-by: Zi Yan <ziy@nvidia.com>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\nReviewed-by: David Hildenbrand <david@redhat.com>\nCc: Adam Manzanares <a.manzanares@samsung.com>\nCc: Baolin Wang <baolin.wang@linux.alibaba.com>\nCc: \"Huang, Ying\" <ying.huang@intel.com>\nCc: Johannes Weiner <hannes@cmpxchg.org>\nCc: Kemeng Shi <shikemeng@huaweicloud.com>\nCc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>\nCc: Luis Chamberlain <mcgrof@kernel.org>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Mel Gorman <mgorman@techsingularity.net>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Vishal Moola (Oracle) <vishal.moola@gmail.com>\nCc: Yin Fengwei <fengwei.yin@intel.com>\nCc: Yu Zhao <yuzhao@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}],
        "proper_nouns": [{"folio": "In the Linux kernel, a 'folio' is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages."}, {"memory compaction": "Memory compaction is a process in the Linux kernel that aims to reduce memory fragmentation by relocating pages of memory. This process involves moving pages to create larger contiguous blocks of free memory, which can then be allocated more efficiently."}, {"migrate_pages()": "The 'migrate_pages()' function is a key component in the Linux kernel's memory management system. It is responsible for moving pages of memory from one location to another, often as part of memory compaction or other memory management tasks."}, {"compaction_alloc()": "The 'compaction_alloc()' function is used during memory compaction to allocate new pages for migration. It ensures that the necessary pages are available for the migration process, which is crucial for the success of memory compaction."}, {"compaction_free()": "The 'compaction_free()' function is used during memory compaction to free up pages that are no longer needed after a migration. This function helps in managing the memory resources more efficiently by returning unused pages to the system."}],
        "background_knowledge": ["Memory compaction is a critical process in the Linux kernel that aims to reduce memory fragmentation by relocating pages of memory. This process involves moving pages to create larger contiguous blocks of free memory, which can then be allocated more efficiently.", "The 'migrate_pages()' function is responsible for moving pages of memory from one location to another, often as part of memory compaction or other memory management tasks. It ensures that the necessary pages are available for the migration process, which is crucial for the success of memory compaction.", "The 'compaction_alloc()' and 'compaction_free()' functions are used during memory compaction to allocate and free pages, respectively. These functions help in managing the memory resources more efficiently by ensuring that the necessary pages are available for migration and by returning unused pages to the system."],
        "kernel_changes": ["Before the implementation of this feature, memory compaction only supported the migration of order-0 folios, which limited its effectiveness in managing larger memory blocks. The new feature enables the compaction of folios with orders greater than 0, thereby improving the efficiency of memory management.", "The kernel was modified to split high-order free pages when the source migration page has a lower order, increasing the success rate of migration. This change helps in reducing the premature termination of compaction due to the unavailability of the desired free page order.", "Additionally, the kernel now keeps isolated free pages at their original size during the migration process, without splitting them into order-0 pages. This approach simplifies the management of free pages and makes it easier to identify and use them during the migration process."],
        "comprehensive_understanding": "This feature enhances the Linux kernel's memory compaction process by enabling the compaction of folios with orders greater than 0. This improvement is achieved by optimizing the handling of free pages during migration, including the splitting of high-order free pages and the preservation of their original size. These changes significantly enhance the efficiency of memory management, reduce fragmentation, and improve the overall performance of the system."
    }
}

2024-12-03 22:29:54,413 - extractor - DEBUG - deepseek response:
ChatCompletion(id='15541c83-9ce2-4112-bf80-b3cf4c108eb6', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "concepts": ["folio", "memory compaction", "migrate_pages()", "compaction_alloc()", "compaction_free()", "memory fragmentation", "high-order free pages", "order-0 folios"]\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733236190, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=60, prompt_tokens=5525, total_tokens=5585, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=4864, prompt_cache_miss_tokens=661))
2024-12-03 22:29:54,413 - extractor - DEBUG - feature_extracted of feature_id=33229:
{
    "feature_id": 33229,
    "feature_description": "Enable >0 order folio memory compaction",
    "version": "6.9",
    "commits": [
        {
            "commit_subject": "mm/compaction: optimize >0 order folio compaction with free page split.",
            "commit_message": "During migration in a memory compaction, free pages are placed in an array\nof page lists based on their order.  But the desired free page order\n(i.e., the order of a source page) might not be always present, thus\nleading to migration failures and premature compaction termination.  Split\na high order free pages when source migration page has a lower order to\nincrease migration successful rate.\n\nNote: merging free pages when a migration fails and a lower order free\npage is returned via compaction_free() is possible, but there is too much\nwork.  Since the free pages are not buddy pages, it is hard to identify\nthese free pages using existing PFN-based page merging algorithm.\n\nLink: https://lkml.kernel.org/r/20240220183220.1451315-5-zi.yan@sent.com\nSigned-off-by: Zi Yan <ziy@nvidia.com>\nReviewed-by: Baolin Wang <baolin.wang@linux.alibaba.com>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\nTested-by: Baolin Wang <baolin.wang@linux.alibaba.com>\nTested-by: Yu Zhao <yuzhao@google.com>\nCc: Adam Manzanares <a.manzanares@samsung.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Huang Ying <ying.huang@intel.com>\nCc: Johannes Weiner <hannes@cmpxchg.org>\nCc: Kemeng Shi <shikemeng@huaweicloud.com>\nCc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>\nCc: Luis Chamberlain <mcgrof@kernel.org>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Mel Gorman <mgorman@techsingularity.net>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Vishal Moola (Oracle) <vishal.moola@gmail.com>\nCc: Vlastimil Babka <vbabka@suse.cz>\nCc: Yin Fengwei <fengwei.yin@intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm/compaction: add support for >0 order folio memory compaction.",
            "commit_message": "Before last commit, memory compaction only migrates order-0 folios and\nskips >0 order folios.  Last commit splits all >0 order folios during\ncompaction.  This commit migrates >0 order folios during compaction by\nkeeping isolated free pages at their original size without splitting them\ninto order-0 pages and using them directly during migration process.\n\nWhat is different from the prior implementation:\n1. All isolated free pages are kept in a NR_PAGE_ORDERS array of page\n   lists, where each page list stores free pages in the same order.\n2. All free pages are not post_alloc_hook() processed nor buddy pages,\n   although their orders are stored in first page's private like buddy\n   pages.\n3. During migration, in new page allocation time (i.e., in\n   compaction_alloc()), free pages are then processed by post_alloc_hook().\n   When migration fails and a new page is returned (i.e., in\n   compaction_free()), free pages are restored by reversing the\n   post_alloc_hook() operations using newly added\n   free_pages_prepare_fpi_none().\n\nStep 3 is done for a latter optimization that splitting and/or merging\nfree pages during compaction becomes easier.\n\nNote: without splitting free pages, compaction can end prematurely due to\nmigration will return -ENOMEM even if there is free pages.  This happens\nwhen no order-0 free page exist and compaction_alloc() return NULL.\n\nLink: https://lkml.kernel.org/r/20240220183220.1451315-4-zi.yan@sent.com\nSigned-off-by: Zi Yan <ziy@nvidia.com>\nReviewed-by: Baolin Wang <baolin.wang@linux.alibaba.com>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\nTested-by: Baolin Wang <baolin.wang@linux.alibaba.com>\nTested-by: Yu Zhao <yuzhao@google.com>\nCc: Adam Manzanares <a.manzanares@samsung.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Huang Ying <ying.huang@intel.com>\nCc: Johannes Weiner <hannes@cmpxchg.org>\nCc: Kemeng Shi <shikemeng@huaweicloud.com>\nCc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>\nCc: Luis Chamberlain <mcgrof@kernel.org>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Mel Gorman <mgorman@techsingularity.net>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Vishal Moola (Oracle) <vishal.moola@gmail.com>\nCc: Vlastimil Babka <vbabka@suse.cz>\nCc: Yin Fengwei <fengwei.yin@intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm/compaction: enable compacting >0 order folios.",
            "commit_message": "migrate_pages() supports >0 order folio migration and during compaction,\neven if compaction_alloc() cannot provide >0 order free pages,\nmigrate_pages() can split the source page and try to migrate the base\npages from the split.  It can be a baseline and start point for adding\nsupport for compacting >0 order folios.\n\nLink: https://lkml.kernel.org/r/20240220183220.1451315-3-zi.yan@sent.com\nSigned-off-by: Zi Yan <ziy@nvidia.com>\nSuggested-by: Huang Ying <ying.huang@intel.com>\nReviewed-by: Baolin Wang <baolin.wang@linux.alibaba.com>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\nTested-by: Baolin Wang <baolin.wang@linux.alibaba.com>\nTested-by: Yu Zhao <yuzhao@google.com>\nCc: Adam Manzanares <a.manzanares@samsung.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Johannes Weiner <hannes@cmpxchg.org>\nCc: Kemeng Shi <shikemeng@huaweicloud.com>\nCc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>\nCc: Luis Chamberlain <mcgrof@kernel.org>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Mel Gorman <mgorman@techsingularity.net>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Vishal Moola (Oracle) <vishal.moola@gmail.com>\nCc: Vlastimil Babka <vbabka@suse.cz>\nCc: Yin Fengwei <fengwei.yin@intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm/page_alloc: remove unused fpi_flags in free_pages_prepare()",
            "commit_message": "Patch series \"Enable >0 order folio memory compaction\", v7.\n\nThis patchset enables >0 order folio memory compaction, which is one of\nthe prerequisitions for large folio support[1].\n\nI am aware of that split free pages is necessary for folio migration in\ncompaction, since if >0 order free pages are never split and no order-0\nfree page is scanned, compaction will end prematurely due to migration\nreturns -ENOMEM.  Free page split becomes a must instead of an\noptimization.\n\nlkp ncompare results (on a 8-CPU (Intel Xeon E5-2650 v4 @2.20GHz) 16G VM)\nfor default LRU (-no-mglru) and CONFIG_LRU_GEN are shown at the bottom,\ncopied from V3[4].  In sum, most of vm-scalability applications do not see\nperformance change, and the others see ~4% to ~26% performance boost under\ndefault LRU and ~2% to ~6% performance boost under CONFIG_LRU_GEN.\n\nOverview\n===\n\nTo support >0 order folio compaction, the patchset changes how free pages\nused for migration are kept during compaction.  Free pages used to be\nsplit into order-0 pages that are post allocation processed (i.e.,\nPageBuddy flag cleared, page order stored in page->private is zeroed, and\npage reference is set to 1).  Now all free pages are kept in a\nNR_PAGE_ORDER array of page lists based on their order without post\nallocation process.  When migrate_pages() asks for a new page, one of the\nfree pages, based on the requested page order, is then processed and given\nout.  And THP <2MB would need this feature.\n\n\n[1] https://lore.kernel.org/linux-mm/f8d47176-03a8-99bf-a813-b5942830fd73@arm.com/\n[2] https://lore.kernel.org/linux-mm/20231113170157.280181-1-zi.yan@sent.com/\n[3] https://lore.kernel.org/linux-mm/20240123034636.1095672-1-zi.yan@sent.com/\n[4] https://lore.kernel.org/linux-mm/20240202161554.565023-1-zi.yan@sent.com/\n[5] https://lore.kernel.org/linux-mm/20240212163510.859822-1-zi.yan@sent.com/\n[6] https://lore.kernel.org/linux-mm/20240214220420.1229173-1-zi.yan@sent.com/\n[7] https://lore.kernel.org/linux-mm/20240216170432.1268753-1-zi.yan@sent.com/\n\n\nThis patch (of 4):\n\nCommit 0a54864f8dfb (\"kasan: remove PG_skip_kasan_poison flag\") removes\nthe use of fpi_flags in should_skip_kasan_poison() and fpi_flags is only\npassed to should_skip_kasan_poison() in free_pages_prepare().  Remove the\nunused parameter.\n\nLink: https://lkml.kernel.org/r/20240220183220.1451315-1-zi.yan@sent.com\nLink: https://lkml.kernel.org/r/20240220183220.1451315-2-zi.yan@sent.com\nSigned-off-by: Zi Yan <ziy@nvidia.com>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\nReviewed-by: David Hildenbrand <david@redhat.com>\nCc: Adam Manzanares <a.manzanares@samsung.com>\nCc: Baolin Wang <baolin.wang@linux.alibaba.com>\nCc: \"Huang, Ying\" <ying.huang@intel.com>\nCc: Johannes Weiner <hannes@cmpxchg.org>\nCc: Kemeng Shi <shikemeng@huaweicloud.com>\nCc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>\nCc: Luis Chamberlain <mcgrof@kernel.org>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Mel Gorman <mgorman@techsingularity.net>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Vishal Moola (Oracle) <vishal.moola@gmail.com>\nCc: Yin Fengwei <fengwei.yin@intel.com>\nCc: Yu Zhao <yuzhao@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        }
    ],
    "proper_nouns": [
        {
            "folio": "In the Linux kernel, a 'folio' is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages."
        },
        {
            "memory compaction": "Memory compaction is a process in the Linux kernel that aims to reduce memory fragmentation by relocating pages of memory. This process involves moving pages to create larger contiguous blocks of free memory, which can then be allocated more efficiently."
        },
        {
            "migrate_pages()": "The 'migrate_pages()' function is a key component in the Linux kernel's memory management system. It is responsible for moving pages of memory from one location to another, often as part of memory compaction or other memory management tasks."
        },
        {
            "compaction_alloc()": "The 'compaction_alloc()' function is used during memory compaction to allocate new pages for migration. It ensures that the necessary pages are available for the migration process, which is crucial for the success of memory compaction."
        },
        {
            "compaction_free()": "The 'compaction_free()' function is used during memory compaction to free up pages that are no longer needed after a migration. This function helps in managing the memory resources more efficiently by returning unused pages to the system."
        }
    ],
    "background_knowledge": [
        "Memory compaction is a critical process in the Linux kernel that aims to reduce memory fragmentation by relocating pages of memory. This process involves moving pages to create larger contiguous blocks of free memory, which can then be allocated more efficiently.",
        "The 'migrate_pages()' function is responsible for moving pages of memory from one location to another, often as part of memory compaction or other memory management tasks. It ensures that the necessary pages are available for the migration process, which is crucial for the success of memory compaction.",
        "The 'compaction_alloc()' and 'compaction_free()' functions are used during memory compaction to allocate and free pages, respectively. These functions help in managing the memory resources more efficiently by ensuring that the necessary pages are available for migration and by returning unused pages to the system."
    ],
    "kernel_changes": [
        "Before the implementation of this feature, memory compaction only supported the migration of order-0 folios, which limited its effectiveness in managing larger memory blocks. The new feature enables the compaction of folios with orders greater than 0, thereby improving the efficiency of memory management.",
        "The kernel was modified to split high-order free pages when the source migration page has a lower order, increasing the success rate of migration. This change helps in reducing the premature termination of compaction due to the unavailability of the desired free page order.",
        "Additionally, the kernel now keeps isolated free pages at their original size during the migration process, without splitting them into order-0 pages. This approach simplifies the management of free pages and makes it easier to identify and use them during the migration process."
    ],
    "comprehensive_understanding": "This feature enhances the Linux kernel's memory compaction process by enabling the compaction of folios with orders greater than 0. This improvement is achieved by optimizing the handling of free pages during migration, including the splitting of high-order free pages and the preservation of their original size. These changes significantly enhance the efficiency of memory management, reduce fragmentation, and improve the overall performance of the system.",
    "concepts": [
        "folio",
        "memory compaction",
        "migrate_pages()",
        "compaction_alloc()",
        "compaction_free()",
        "memory fragmentation",
        "high-order free pages",
        "order-0 folios"
    ]
}
2024-12-03 22:29:54,414 - extractor - INFO - ---------Processing feature 12/815---------
2024-12-03 22:29:58,184 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个 linux 内核的专家。给你一个 linux 内核特性的描述以及其对应的若干 commit 的标题和提交信息，你的任务是为这个特性写一个多方面的扩写版描述。你的扩写需包含以下几个方面：1. 该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)；2. 为了让开发者理解该特性，需要哪些具体的，与这个特性直接相关的背景知识 (background_knowledge)；3. 该特性被实现前后，linux 内核发生了怎样的变化 (kernel_changes)；4. 你对该特性的综合理解 (comprehensive_understanding)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}]
        },
        "output": {
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
        }
    ],
    "input": {
        "feature_description": "shmem: properly report quota mount options",
        "commits": [{"commit_subject": "shmem: properly report quota mount options", "commit_message": "Report quota options among the set of mount options. This allows proper\nuser visibility into whether quotas are enabled or not.\n\nLink: https://lkml.kernel.org/r/20240129120131.21145-1-jack@suse.cz\nFixes: e09764cff44b (\"shmem: quota support\")\nSigned-off-by: Jan Kara <jack@suse.cz>\nReviewed-by: Carlos Maiolino <cmaiolino@redhat.com>\nAcked-by: Hugh Dickins <hughd@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}]
    }
}

2024-12-03 22:30:26,255 - extractor - DEBUG - deepseek response:
ChatCompletion(id='85fa4537-4e0b-4776-adaa-08126cc30c45', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "proper_nouns": [\n        {\n            "shmem": "shmem, short for \'shared memory,\' is a feature in the Linux kernel that allows processes to share memory. It is often used for inter-process communication (IPC) and can be accessed via the /dev/shm directory. shmem is particularly useful for applications that require high-speed data exchange between processes."\n        },\n        {\n            "quota": "Quota in the context of Linux file systems refers to a mechanism that limits the amount of disk space or the number of files a user or group can use. This is crucial for managing resources on multi-user systems and ensuring fair usage of storage."\n        },\n        {\n            "mount options": "Mount options are parameters specified when a file system is mounted in Linux. These options can affect how the file system behaves, such as enabling or disabling features like quotas, read-only access, or data journaling."\n        }\n    ],\n    "background_knowledge": [\n        "In the context of shmem, quotas are used to limit the amount of shared memory a user or group can allocate. This is important for preventing any single user or group from consuming all available shared memory, which could lead to system instability or denial of service.",\n        "Previously, the mount options for shmem did not properly report whether quotas were enabled. This lack of visibility could lead to confusion or mismanagement of shared memory resources."\n    ],\n    "kernel_changes": [\n        "Before this feature was implemented, the mount options for shmem did not correctly report whether quota options were enabled. This meant that users and administrators were not fully informed about the quota settings for shared memory, potentially leading to overuse or mismanagement of resources.",\n        "After the implementation of this feature, the mount options for shmem now properly report whether quotas are enabled. This provides clear visibility into the quota settings, allowing users and administrators to better manage shared memory resources and ensure that quotas are enforced as intended."\n    ],\n    "comprehensive_understanding": "This feature enhances the management of shared memory (shmem) in the Linux kernel by ensuring that quota mount options are properly reported. Previously, the lack of visibility into quota settings could lead to overuse of shared memory resources, potentially causing system instability. By correctly reporting quota options, this feature improves resource management, ensures that quotas are enforced, and provides better transparency for users and administrators."\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733236198, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=525, prompt_tokens=1747, total_tokens=2272, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=1472, prompt_cache_miss_tokens=275))
2024-12-03 22:30:26,255 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个命名实体抽取 (NER) 任务的专家，同时也是 linux 内核的专家。给你一个 linux 内核特性的描述 (feature_description)、对应的若干 commit 的标题和提交信息 (commits)、该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)、为了让开发者理解该特性所需的与这个特性直接相关的背景知识 (background_knowledge)、该特性被实现前后，linux 内核发生的变化 (kernel_changes)、对该特性的综合理解 (comprehensive_understanding)。你的任务是：根据这些信息提取出在 linux 内核中涉及到的和这个特性相关的概念实体 (concepts)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}],
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": [
                "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
            ]
        },
        "output": {
            "concepts": ["Copy-on-Write (CoW)", "mTHP", "folio", "memory reclamation", "memory availability", "anti-fragmentation"]
        }
    ],
    "input": {
        "feature_description": "shmem: properly report quota mount options",
        "commits": [{"commit_subject": "shmem: properly report quota mount options", "commit_message": "Report quota options among the set of mount options. This allows proper\nuser visibility into whether quotas are enabled or not.\n\nLink: https://lkml.kernel.org/r/20240129120131.21145-1-jack@suse.cz\nFixes: e09764cff44b (\"shmem: quota support\")\nSigned-off-by: Jan Kara <jack@suse.cz>\nReviewed-by: Carlos Maiolino <cmaiolino@redhat.com>\nAcked-by: Hugh Dickins <hughd@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}],
        "proper_nouns": [{"shmem": "shmem, short for 'shared memory,' is a feature in the Linux kernel that allows processes to share memory. It is often used for inter-process communication (IPC) and can be accessed via the /dev/shm directory. shmem is particularly useful for applications that require high-speed data exchange between processes."}, {"quota": "Quota in the context of Linux file systems refers to a mechanism that limits the amount of disk space or the number of files a user or group can use. This is crucial for managing resources on multi-user systems and ensuring fair usage of storage."}, {"mount options": "Mount options are parameters specified when a file system is mounted in Linux. These options can affect how the file system behaves, such as enabling or disabling features like quotas, read-only access, or data journaling."}],
        "background_knowledge": ["In the context of shmem, quotas are used to limit the amount of shared memory a user or group can allocate. This is important for preventing any single user or group from consuming all available shared memory, which could lead to system instability or denial of service.", "Previously, the mount options for shmem did not properly report whether quotas were enabled. This lack of visibility could lead to confusion or mismanagement of shared memory resources."],
        "kernel_changes": ["Before this feature was implemented, the mount options for shmem did not correctly report whether quota options were enabled. This meant that users and administrators were not fully informed about the quota settings for shared memory, potentially leading to overuse or mismanagement of resources.", "After the implementation of this feature, the mount options for shmem now properly report whether quotas are enabled. This provides clear visibility into the quota settings, allowing users and administrators to better manage shared memory resources and ensure that quotas are enforced as intended."],
        "comprehensive_understanding": "This feature enhances the management of shared memory (shmem) in the Linux kernel by ensuring that quota mount options are properly reported. Previously, the lack of visibility into quota settings could lead to overuse of shared memory resources, potentially causing system instability. By correctly reporting quota options, this feature improves resource management, ensures that quotas are enforced, and provides better transparency for users and administrators."
    }
}

2024-12-03 22:30:29,088 - extractor - DEBUG - deepseek response:
ChatCompletion(id='e4bc25c7-2bc6-45cc-bd1a-4de1340691d5', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "concepts": ["shmem", "quota", "mount options", "shared memory", "resource management", "quota enforcement", "system stability"]\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733236226, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=41, prompt_tokens=2282, total_tokens=2323, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=1536, prompt_cache_miss_tokens=746))
2024-12-03 22:30:29,088 - extractor - DEBUG - feature_extracted of feature_id=33228:
{
    "feature_id": 33228,
    "feature_description": "shmem: properly report quota mount options",
    "version": "6.9",
    "commits": [
        {
            "commit_subject": "shmem: properly report quota mount options",
            "commit_message": "Report quota options among the set of mount options. This allows proper\nuser visibility into whether quotas are enabled or not.\n\nLink: https://lkml.kernel.org/r/20240129120131.21145-1-jack@suse.cz\nFixes: e09764cff44b (\"shmem: quota support\")\nSigned-off-by: Jan Kara <jack@suse.cz>\nReviewed-by: Carlos Maiolino <cmaiolino@redhat.com>\nAcked-by: Hugh Dickins <hughd@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        }
    ],
    "proper_nouns": [
        {
            "shmem": "shmem, short for 'shared memory,' is a feature in the Linux kernel that allows processes to share memory. It is often used for inter-process communication (IPC) and can be accessed via the /dev/shm directory. shmem is particularly useful for applications that require high-speed data exchange between processes."
        },
        {
            "quota": "Quota in the context of Linux file systems refers to a mechanism that limits the amount of disk space or the number of files a user or group can use. This is crucial for managing resources on multi-user systems and ensuring fair usage of storage."
        },
        {
            "mount options": "Mount options are parameters specified when a file system is mounted in Linux. These options can affect how the file system behaves, such as enabling or disabling features like quotas, read-only access, or data journaling."
        }
    ],
    "background_knowledge": [
        "In the context of shmem, quotas are used to limit the amount of shared memory a user or group can allocate. This is important for preventing any single user or group from consuming all available shared memory, which could lead to system instability or denial of service.",
        "Previously, the mount options for shmem did not properly report whether quotas were enabled. This lack of visibility could lead to confusion or mismanagement of shared memory resources."
    ],
    "kernel_changes": [
        "Before this feature was implemented, the mount options for shmem did not correctly report whether quota options were enabled. This meant that users and administrators were not fully informed about the quota settings for shared memory, potentially leading to overuse or mismanagement of resources.",
        "After the implementation of this feature, the mount options for shmem now properly report whether quotas are enabled. This provides clear visibility into the quota settings, allowing users and administrators to better manage shared memory resources and ensure that quotas are enforced as intended."
    ],
    "comprehensive_understanding": "This feature enhances the management of shared memory (shmem) in the Linux kernel by ensuring that quota mount options are properly reported. Previously, the lack of visibility into quota settings could lead to overuse of shared memory resources, potentially causing system instability. By correctly reporting quota options, this feature improves resource management, ensures that quotas are enforced, and provides better transparency for users and administrators.",
    "concepts": [
        "shmem",
        "quota",
        "mount options",
        "shared memory",
        "resource management",
        "quota enforcement",
        "system stability"
    ]
}
2024-12-03 22:30:29,089 - extractor - INFO - ---------Processing feature 13/815---------
2024-12-03 22:30:34,609 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个 linux 内核的专家。给你一个 linux 内核特性的描述以及其对应的若干 commit 的标题和提交信息，你的任务是为这个特性写一个多方面的扩写版描述。你的扩写需包含以下几个方面：1. 该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)；2. 为了让开发者理解该特性，需要哪些具体的，与这个特性直接相关的背景知识 (background_knowledge)；3. 该特性被实现前后，linux 内核发生了怎样的变化 (kernel_changes)；4. 你对该特性的综合理解 (comprehensive_understanding)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}]
        },
        "output": {
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
        }
    ],
    "input": {
        "feature_description": "khugepaged: bypassing unnecessary scans with  check",
        "commits": [{"commit_subject": "mm/khugepaged: bypassing unnecessary scans with MMF_DISABLE_THP check", "commit_message": "khugepaged scans the entire address space in the background for each\ngiven mm, looking for opportunities to merge sequences of basic pages\ninto huge pages.  However, when an mm is inserted to the mm_slots list,\nand the MMF_DISABLE_THP flag is set later, this scanning process\nbecomes unnecessary for that mm and can be skipped to avoid redundant\noperations, especially in scenarios with a large address space.\n\nOn an Intel Core i5 CPU, the time taken by khugepaged to scan the\naddress space of the process, which has been set with the\nMMF_DISABLE_THP flag after being added to the mm_slots list, is as\nfollows (shorter is better):\n\nVMA Count |   Old   |   New   |  Change\n---------------------------------------\n    50    |   23us  |    9us  |  -60.9%\n   100    |   32us  |    9us  |  -71.9%\n   200    |   44us  |    9us  |  -79.5%\n   400    |   75us  |    9us  |  -88.0%\n   800    |   98us  |    9us  |  -90.8%\n\nOnce the count of VMAs for the process exceeds page_to_scan, khugepaged\nneeds to wait for scan_sleep_millisecs ms before scanning the next\nprocess.  IMO, unnecessary scans could actually be skipped with a very\ninexpensive mm->flags check in this case.\n\nThis commit introduces a check before each scanning process to test the\nMMF_DISABLE_THP flag for the given mm; if the flag is set, the scanning\nprocess is bypassed, thereby improving the efficiency of khugepaged.\n\nThis optimization is not a correctness issue but rather an enhancement\nto save expensive checks on each VMA when userspace cannot prctl itself\nbefore spawning into the new process.\n\nOn some servers within our company, we deploy a daemon responsible for\nmonitoring and updating local applications.  Some applications prefer\nnot to use THP, so the daemon calls prctl to disable THP before\nfork/exec.  Conversely, for other applications, the daemon calls prctl\nto enable THP before fork/exec.\n\nIdeally, the daemon should invoke prctl after the fork, but its current\nimplementation follows the described approach.  In the Go standard\nlibrary, there is no direct encapsulation of the fork system call;\ninstead, fork and execve are combined into one through\nsyscall.ForkExec.\n\nLink: https://lkml.kernel.org/r/20240129054551.57728-1-ioworker0@gmail.com\nSigned-off-by: Lance Yang <ioworker0@gmail.com>\nAcked-by: David Hildenbrand <david@redhat.com>\nCc: Michal Hocko <mhocko@suse.com>\nCc: Minchan Kim <minchan@kernel.org>\nCc: Muchun Song <songmuchun@bytedance.com>\nCc: Peter Xu <peterx@redhat.com>\nCc: Zach O'Keefe <zokeefe@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}]
    }
}

2024-12-03 22:31:11,787 - extractor - DEBUG - deepseek response:
ChatCompletion(id='3aae5abb-a881-41b8-8ec2-e25eda806043', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "proper_nouns": [\n        {\n            "khugepaged": "khugepaged is a background daemon in the Linux kernel responsible for scanning the address space of processes to identify opportunities to merge sequences of basic pages into huge pages, thereby optimizing memory usage and performance."\n        },\n        {\n            "MMF_DISABLE_THP": "MMF_DISABLE_THP is a flag in the Linux kernel that, when set, indicates that Transparent Huge Pages (THP) should be disabled for a specific memory management context (mm). This flag is used to control whether huge pages are used in memory management operations."\n        },\n        {\n            "Transparent Huge Pages (THP)": "Transparent Huge Pages (THP) is a feature in the Linux kernel that automatically manages the use of large memory pages (huge pages) to improve memory performance. THP aims to reduce the overhead associated with memory management by using larger memory pages, which can lead to fewer page table entries and reduced CPU cache misses."\n        }\n    ],\n    "background_knowledge": [\n        "khugepaged scans the entire address space in the background for each given mm, looking for opportunities to merge sequences of basic pages into huge pages. However, when an mm is inserted to the mm_slots list and the MMF_DISABLE_THP flag is set later, this scanning process becomes unnecessary for that mm and can be skipped to avoid redundant operations, especially in scenarios with a large address space.",\n        "The MMF_DISABLE_THP flag is used to indicate that THP should be disabled for a specific memory management context (mm). When this flag is set, the khugepaged daemon should bypass unnecessary scans for that mm to improve efficiency."\n    ],\n    "kernel_changes": [\n        "Before this feature was implemented, khugepaged would scan the entire address space of each mm, even if the MMF_DISABLE_THP flag was set later. This led to unnecessary and redundant scanning operations, especially in scenarios with a large address space. After the implementation of this feature, khugepaged now checks the MMF_DISABLE_THP flag before each scanning process. If the flag is set, the scanning process is bypassed, thereby improving the efficiency of khugepaged and reducing the time taken for unnecessary scans.",\n        "This change results in significant performance improvements, as demonstrated by the reduction in scan times for processes with a large number of VMAs (Virtual Memory Areas). The optimization is particularly beneficial in scenarios where processes are dynamically configured to disable THP, such as in daemons that manage application configurations."\n    ],\n    "comprehensive_understanding": "This feature enhances the efficiency of the khugepaged daemon by introducing a check for the MMF_DISABLE_THP flag before initiating a scan of the address space for each mm. If the flag is set, indicating that THP is disabled for that mm, the scan is bypassed, thereby avoiding unnecessary and redundant operations. This optimization significantly reduces the time taken for scans, especially in scenarios with a large address space, leading to improved overall system performance and resource utilization. The feature is particularly useful in environments where processes are dynamically configured to enable or disable THP, such as in daemons that manage application configurations."\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733236234, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=703, prompt_tokens=2392, total_tokens=3095, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=1472, prompt_cache_miss_tokens=920))
2024-12-03 22:31:11,788 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个命名实体抽取 (NER) 任务的专家，同时也是 linux 内核的专家。给你一个 linux 内核特性的描述 (feature_description)、对应的若干 commit 的标题和提交信息 (commits)、该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)、为了让开发者理解该特性所需的与这个特性直接相关的背景知识 (background_knowledge)、该特性被实现前后，linux 内核发生的变化 (kernel_changes)、对该特性的综合理解 (comprehensive_understanding)。你的任务是：根据这些信息提取出在 linux 内核中涉及到的和这个特性相关的概念实体 (concepts)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}],
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": [
                "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
            ]
        },
        "output": {
            "concepts": ["Copy-on-Write (CoW)", "mTHP", "folio", "memory reclamation", "memory availability", "anti-fragmentation"]
        }
    ],
    "input": {
        "feature_description": "khugepaged: bypassing unnecessary scans with  check",
        "commits": [{"commit_subject": "mm/khugepaged: bypassing unnecessary scans with MMF_DISABLE_THP check", "commit_message": "khugepaged scans the entire address space in the background for each\ngiven mm, looking for opportunities to merge sequences of basic pages\ninto huge pages.  However, when an mm is inserted to the mm_slots list,\nand the MMF_DISABLE_THP flag is set later, this scanning process\nbecomes unnecessary for that mm and can be skipped to avoid redundant\noperations, especially in scenarios with a large address space.\n\nOn an Intel Core i5 CPU, the time taken by khugepaged to scan the\naddress space of the process, which has been set with the\nMMF_DISABLE_THP flag after being added to the mm_slots list, is as\nfollows (shorter is better):\n\nVMA Count |   Old   |   New   |  Change\n---------------------------------------\n    50    |   23us  |    9us  |  -60.9%\n   100    |   32us  |    9us  |  -71.9%\n   200    |   44us  |    9us  |  -79.5%\n   400    |   75us  |    9us  |  -88.0%\n   800    |   98us  |    9us  |  -90.8%\n\nOnce the count of VMAs for the process exceeds page_to_scan, khugepaged\nneeds to wait for scan_sleep_millisecs ms before scanning the next\nprocess.  IMO, unnecessary scans could actually be skipped with a very\ninexpensive mm->flags check in this case.\n\nThis commit introduces a check before each scanning process to test the\nMMF_DISABLE_THP flag for the given mm; if the flag is set, the scanning\nprocess is bypassed, thereby improving the efficiency of khugepaged.\n\nThis optimization is not a correctness issue but rather an enhancement\nto save expensive checks on each VMA when userspace cannot prctl itself\nbefore spawning into the new process.\n\nOn some servers within our company, we deploy a daemon responsible for\nmonitoring and updating local applications.  Some applications prefer\nnot to use THP, so the daemon calls prctl to disable THP before\nfork/exec.  Conversely, for other applications, the daemon calls prctl\nto enable THP before fork/exec.\n\nIdeally, the daemon should invoke prctl after the fork, but its current\nimplementation follows the described approach.  In the Go standard\nlibrary, there is no direct encapsulation of the fork system call;\ninstead, fork and execve are combined into one through\nsyscall.ForkExec.\n\nLink: https://lkml.kernel.org/r/20240129054551.57728-1-ioworker0@gmail.com\nSigned-off-by: Lance Yang <ioworker0@gmail.com>\nAcked-by: David Hildenbrand <david@redhat.com>\nCc: Michal Hocko <mhocko@suse.com>\nCc: Minchan Kim <minchan@kernel.org>\nCc: Muchun Song <songmuchun@bytedance.com>\nCc: Peter Xu <peterx@redhat.com>\nCc: Zach O'Keefe <zokeefe@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}],
        "proper_nouns": [{"khugepaged": "khugepaged is a background daemon in the Linux kernel responsible for scanning the address space of processes to identify opportunities to merge sequences of basic pages into huge pages, thereby optimizing memory usage and performance."}, {"MMF_DISABLE_THP": "MMF_DISABLE_THP is a flag in the Linux kernel that, when set, indicates that Transparent Huge Pages (THP) should be disabled for a specific memory management context (mm). This flag is used to control whether huge pages are used in memory management operations."}, {"Transparent Huge Pages (THP)": "Transparent Huge Pages (THP) is a feature in the Linux kernel that automatically manages the use of large memory pages (huge pages) to improve memory performance. THP aims to reduce the overhead associated with memory management by using larger memory pages, which can lead to fewer page table entries and reduced CPU cache misses."}],
        "background_knowledge": ["khugepaged scans the entire address space in the background for each given mm, looking for opportunities to merge sequences of basic pages into huge pages. However, when an mm is inserted to the mm_slots list and the MMF_DISABLE_THP flag is set later, this scanning process becomes unnecessary for that mm and can be skipped to avoid redundant operations, especially in scenarios with a large address space.", "The MMF_DISABLE_THP flag is used to indicate that THP should be disabled for a specific memory management context (mm). When this flag is set, the khugepaged daemon should bypass unnecessary scans for that mm to improve efficiency."],
        "kernel_changes": ["Before this feature was implemented, khugepaged would scan the entire address space of each mm, even if the MMF_DISABLE_THP flag was set later. This led to unnecessary and redundant scanning operations, especially in scenarios with a large address space. After the implementation of this feature, khugepaged now checks the MMF_DISABLE_THP flag before each scanning process. If the flag is set, the scanning process is bypassed, thereby improving the efficiency of khugepaged and reducing the time taken for unnecessary scans.", "This change results in significant performance improvements, as demonstrated by the reduction in scan times for processes with a large number of VMAs (Virtual Memory Areas). The optimization is particularly beneficial in scenarios where processes are dynamically configured to disable THP, such as in daemons that manage application configurations."],
        "comprehensive_understanding": "This feature enhances the efficiency of the khugepaged daemon by introducing a check for the MMF_DISABLE_THP flag before initiating a scan of the address space for each mm. If the flag is set, indicating that THP is disabled for that mm, the scan is bypassed, thereby avoiding unnecessary and redundant operations. This optimization significantly reduces the time taken for scans, especially in scenarios with a large address space, leading to improved overall system performance and resource utilization. The feature is particularly useful in environments where processes are dynamically configured to enable or disable THP, such as in daemons that manage application configurations."
    }
}

2024-12-03 22:31:16,035 - extractor - DEBUG - deepseek response:
ChatCompletion(id='d8827a14-9167-43d8-a240-44c91d0a4c82', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "concepts": ["khugepaged", "MMF_DISABLE_THP", "Transparent Huge Pages (THP)", "address space", "memory management", "huge pages", "scanning process", "efficiency", "performance"]\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733236271, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=61, prompt_tokens=3105, total_tokens=3166, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=1536, prompt_cache_miss_tokens=1569))
2024-12-03 22:31:16,035 - extractor - DEBUG - feature_extracted of feature_id=33227:
{
    "feature_id": 33227,
    "feature_description": "khugepaged: bypassing unnecessary scans with  check",
    "version": "6.9",
    "commits": [
        {
            "commit_subject": "mm/khugepaged: bypassing unnecessary scans with MMF_DISABLE_THP check",
            "commit_message": "khugepaged scans the entire address space in the background for each\ngiven mm, looking for opportunities to merge sequences of basic pages\ninto huge pages.  However, when an mm is inserted to the mm_slots list,\nand the MMF_DISABLE_THP flag is set later, this scanning process\nbecomes unnecessary for that mm and can be skipped to avoid redundant\noperations, especially in scenarios with a large address space.\n\nOn an Intel Core i5 CPU, the time taken by khugepaged to scan the\naddress space of the process, which has been set with the\nMMF_DISABLE_THP flag after being added to the mm_slots list, is as\nfollows (shorter is better):\n\nVMA Count |   Old   |   New   |  Change\n---------------------------------------\n    50    |   23us  |    9us  |  -60.9%\n   100    |   32us  |    9us  |  -71.9%\n   200    |   44us  |    9us  |  -79.5%\n   400    |   75us  |    9us  |  -88.0%\n   800    |   98us  |    9us  |  -90.8%\n\nOnce the count of VMAs for the process exceeds page_to_scan, khugepaged\nneeds to wait for scan_sleep_millisecs ms before scanning the next\nprocess.  IMO, unnecessary scans could actually be skipped with a very\ninexpensive mm->flags check in this case.\n\nThis commit introduces a check before each scanning process to test the\nMMF_DISABLE_THP flag for the given mm; if the flag is set, the scanning\nprocess is bypassed, thereby improving the efficiency of khugepaged.\n\nThis optimization is not a correctness issue but rather an enhancement\nto save expensive checks on each VMA when userspace cannot prctl itself\nbefore spawning into the new process.\n\nOn some servers within our company, we deploy a daemon responsible for\nmonitoring and updating local applications.  Some applications prefer\nnot to use THP, so the daemon calls prctl to disable THP before\nfork/exec.  Conversely, for other applications, the daemon calls prctl\nto enable THP before fork/exec.\n\nIdeally, the daemon should invoke prctl after the fork, but its current\nimplementation follows the described approach.  In the Go standard\nlibrary, there is no direct encapsulation of the fork system call;\ninstead, fork and execve are combined into one through\nsyscall.ForkExec.\n\nLink: https://lkml.kernel.org/r/20240129054551.57728-1-ioworker0@gmail.com\nSigned-off-by: Lance Yang <ioworker0@gmail.com>\nAcked-by: David Hildenbrand <david@redhat.com>\nCc: Michal Hocko <mhocko@suse.com>\nCc: Minchan Kim <minchan@kernel.org>\nCc: Muchun Song <songmuchun@bytedance.com>\nCc: Peter Xu <peterx@redhat.com>\nCc: Zach O'Keefe <zokeefe@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        }
    ],
    "proper_nouns": [
        {
            "khugepaged": "khugepaged is a background daemon in the Linux kernel responsible for scanning the address space of processes to identify opportunities to merge sequences of basic pages into huge pages, thereby optimizing memory usage and performance."
        },
        {
            "MMF_DISABLE_THP": "MMF_DISABLE_THP is a flag in the Linux kernel that, when set, indicates that Transparent Huge Pages (THP) should be disabled for a specific memory management context (mm). This flag is used to control whether huge pages are used in memory management operations."
        },
        {
            "Transparent Huge Pages (THP)": "Transparent Huge Pages (THP) is a feature in the Linux kernel that automatically manages the use of large memory pages (huge pages) to improve memory performance. THP aims to reduce the overhead associated with memory management by using larger memory pages, which can lead to fewer page table entries and reduced CPU cache misses."
        }
    ],
    "background_knowledge": [
        "khugepaged scans the entire address space in the background for each given mm, looking for opportunities to merge sequences of basic pages into huge pages. However, when an mm is inserted to the mm_slots list and the MMF_DISABLE_THP flag is set later, this scanning process becomes unnecessary for that mm and can be skipped to avoid redundant operations, especially in scenarios with a large address space.",
        "The MMF_DISABLE_THP flag is used to indicate that THP should be disabled for a specific memory management context (mm). When this flag is set, the khugepaged daemon should bypass unnecessary scans for that mm to improve efficiency."
    ],
    "kernel_changes": [
        "Before this feature was implemented, khugepaged would scan the entire address space of each mm, even if the MMF_DISABLE_THP flag was set later. This led to unnecessary and redundant scanning operations, especially in scenarios with a large address space. After the implementation of this feature, khugepaged now checks the MMF_DISABLE_THP flag before each scanning process. If the flag is set, the scanning process is bypassed, thereby improving the efficiency of khugepaged and reducing the time taken for unnecessary scans.",
        "This change results in significant performance improvements, as demonstrated by the reduction in scan times for processes with a large number of VMAs (Virtual Memory Areas). The optimization is particularly beneficial in scenarios where processes are dynamically configured to disable THP, such as in daemons that manage application configurations."
    ],
    "comprehensive_understanding": "This feature enhances the efficiency of the khugepaged daemon by introducing a check for the MMF_DISABLE_THP flag before initiating a scan of the address space for each mm. If the flag is set, indicating that THP is disabled for that mm, the scan is bypassed, thereby avoiding unnecessary and redundant operations. This optimization significantly reduces the time taken for scans, especially in scenarios with a large address space, leading to improved overall system performance and resource utilization. The feature is particularly useful in environments where processes are dynamically configured to enable or disable THP, such as in daemons that manage application configurations.",
    "concepts": [
        "khugepaged",
        "MMF_DISABLE_THP",
        "Transparent Huge Pages (THP)",
        "address space",
        "memory management",
        "huge pages",
        "scanning process",
        "efficiency",
        "performance"
    ]
}
2024-12-03 22:31:16,035 - extractor - INFO - ---------Processing feature 14/815---------
2024-12-03 22:32:15,223 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个 linux 内核的专家。给你一个 linux 内核特性的描述以及其对应的若干 commit 的标题和提交信息，你的任务是为这个特性写一个多方面的扩写版描述。你的扩写需包含以下几个方面：1. 该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)；2. 为了让开发者理解该特性，需要哪些具体的，与这个特性直接相关的背景知识 (background_knowledge)；3. 该特性被实现前后，linux 内核发生了怎样的变化 (kernel_changes)；4. 你对该特性的综合理解 (comprehensive_understanding)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}]
        },
        "output": {
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
        }
    ],
    "input": {
        "feature_description": "Mitigate a vmap lock contention",
        "commits": [{"commit_subject": "mm: vmalloc: remove global purge_vmap_area_root rb-tree", "commit_message": "Similar to busy VA, lazily-freed area is stored to a node it belongs to. \nSuch approach does not require any global locking primitive, instead an\naccess becomes scalable what mitigates a contention.\n\nThis patch removes a global purge-lock, global purge-tree and global purge\nlist.\n\nLink: https://lkml.kernel.org/r/20240102184633.748113-7-urezki@gmail.com\nSigned-off-by: Uladzislau Rezki (Sony) <urezki@gmail.com>\nReviewed-by: Baoquan He <bhe@redhat.com>\nCc: Christoph Hellwig <hch@lst.de>\nCc: Dave Chinner <david@fromorbit.com>\nCc: Joel Fernandes (Google) <joel@joelfernandes.org>\nCc: Kazuhito Hagio <k-hagio-ab@nec.com>\nCc: Liam R. Howlett <Liam.Howlett@oracle.com>\nCc: Lorenzo Stoakes <lstoakes@gmail.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Oleksiy Avramchenko <oleksiy.avramchenko@sony.com>\nCc: Paul E. McKenney <paulmck@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: vmalloc: offload free_vmap_area_lock lock", "commit_message": "Concurrent access to a global vmap space is a bottle-neck.  We can\nsimulate a high contention by running a vmalloc test suite.\n\nTo address it, introduce an effective vmap node logic.  Each node behaves\nas independent entity.  When a node is accessed it serves a request\ndirectly(if possible) from its pool.\n\nThis model has a size based pool for requests, i.e.  pools are serialized\nand populated based on object size and real demand.  A maximum object size\nthat pool can handle is set to 256 pages.\n\nThis technique reduces a pressure on the global vmap lock.\n\nLink: https://lkml.kernel.org/r/20240102184633.748113-8-urezki@gmail.com\nSigned-off-by: Uladzislau Rezki (Sony) <urezki@gmail.com>\nCc: Baoquan He <bhe@redhat.com>\nCc: Christoph Hellwig <hch@lst.de>\nCc: Dave Chinner <david@fromorbit.com>\nCc: Joel Fernandes (Google) <joel@joelfernandes.org>\nCc: Kazuhito Hagio <k-hagio-ab@nec.com>\nCc: Liam R. Howlett <Liam.Howlett@oracle.com>\nCc: Lorenzo Stoakes <lstoakes@gmail.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Oleksiy Avramchenko <oleksiy.avramchenko@sony.com>\nCc: Paul E. McKenney <paulmck@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: vmalloc: rename adjust_va_to_fit_type() function", "commit_message": "This patch renames the adjust_va_to_fit_type() function to va_clip() which\nis shorter and more expressive.\n\nThere is no a functional change as a result of this patch.\n\nLink: https://lkml.kernel.org/r/20240102184633.748113-3-urezki@gmail.com\nSigned-off-by: Uladzislau Rezki (Sony) <urezki@gmail.com>\nReviewed-by: Baoquan He <bhe@redhat.com>\nReviewed-by: Christoph Hellwig <hch@lst.de>\nReviewed-by: Lorenzo Stoakes <lstoakes@gmail.com>\nCc: Dave Chinner <david@fromorbit.com>\nCc: Joel Fernandes (Google) <joel@joelfernandes.org>\nCc: Kazuhito Hagio <k-hagio-ab@nec.com>\nCc: Liam R. Howlett <Liam.Howlett@oracle.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Oleksiy Avramchenko <oleksiy.avramchenko@sony.com>\nCc: Paul E. McKenney <paulmck@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: vmalloc: add a shrinker to drain vmap pools", "commit_message": "The added shrinker is used to return back current cached VAs into a global\nvmap space, when a system enters into a low memory mode.\n\nLink: https://lkml.kernel.org/r/20240102184633.748113-12-urezki@gmail.com\nSigned-off-by: Uladzislau Rezki (Sony) <urezki@gmail.com>\nCc: Kazuhito Hagio <k-hagio-ab@nec.com>\nCc: Baoquan He <bhe@redhat.com>\nCc: Christoph Hellwig <hch@lst.de>\nCc: Dave Chinner <david@fromorbit.com>\nCc: Joel Fernandes (Google) <joel@joelfernandes.org>\nCc: Liam R. Howlett <Liam.Howlett@oracle.com>\nCc: Lorenzo Stoakes <lstoakes@gmail.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Oleksiy Avramchenko <oleksiy.avramchenko@sony.com>\nCc: Paul E. McKenney <paulmck@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: vmalloc: move vmap_init_free_space() down in vmalloc.c", "commit_message": "A vmap_init_free_space() is a function that setups a vmap space and is\nconsidered as part of initialization phase.  Since a main entry which is\nvmalloc_init(), has been moved down in vmalloc.c it makes sense to follow\nthe pattern.\n\nThere is no a functional change as a result of this patch.\n\nLink: https://lkml.kernel.org/r/20240102184633.748113-4-urezki@gmail.com\nSigned-off-by: Uladzislau Rezki (Sony) <urezki@gmail.com>\nReviewed-by: Baoquan He <bhe@redhat.com>\nReviewed-by: Christoph Hellwig <hch@lst.de>\nReviewed-by: Lorenzo Stoakes <lstoakes@gmail.com>\nCc: Dave Chinner <david@fromorbit.com>\nCc: Joel Fernandes (Google) <joel@joelfernandes.org>\nCc: Kazuhito Hagio <k-hagio-ab@nec.com>\nCc: Liam R. Howlett <Liam.Howlett@oracle.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Oleksiy Avramchenko <oleksiy.avramchenko@sony.com>\nCc: Paul E. McKenney <paulmck@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: vmalloc: set nr_nodes based on CPUs in a system", "commit_message": "A number of nodes which are used in the alloc/free paths is set based on\nnum_possible_cpus() in a system.  Please note a high limit threshold\nthough is fixed and corresponds to 128 nodes.\n\nFor 32-bit or single core systems an access to a global vmap heap is not\nbalanced.  Such small systems do not suffer from lock contentions due to\nlow number of CPUs.  In such case the nr_nodes is equal to 1.\n\nTest on AMD Ryzen Threadripper 3970X 32-Core Processor: sudo\n./test_vmalloc.sh run_test_mask=7 nr_threads=64\n\n<default perf>\n 94.41%     0.89%  [kernel]        [k] _raw_spin_lock\n 93.35%    93.07%  [kernel]        [k] native_queued_spin_lock_slowpath\n 76.13%     0.28%  [kernel]        [k] __vmalloc_node_range\n 72.96%     0.81%  [kernel]        [k] alloc_vmap_area\n 56.94%     0.00%  [kernel]        [k] __get_vm_area_node\n 41.95%     0.00%  [kernel]        [k] vmalloc\n 37.15%     0.01%  [test_vmalloc]  [k] full_fit_alloc_test\n 35.17%     0.00%  [kernel]        [k] ret_from_fork_asm\n 35.17%     0.00%  [kernel]        [k] ret_from_fork\n 35.17%     0.00%  [kernel]        [k] kthread\n 35.08%     0.00%  [test_vmalloc]  [k] test_func\n 34.45%     0.00%  [test_vmalloc]  [k] fix_size_alloc_test\n 28.09%     0.01%  [test_vmalloc]  [k] long_busy_list_alloc_test\n 23.53%     0.25%  [kernel]        [k] vfree.part.0\n 21.72%     0.00%  [kernel]        [k] remove_vm_area\n 20.08%     0.21%  [kernel]        [k] find_unlink_vmap_area\n  2.34%     0.61%  [kernel]        [k] free_vmap_area_noflush\n<default perf>\n   vs\n<patch-series perf>\n 82.32%     0.22%  [test_vmalloc]  [k] long_busy_list_alloc_test\n 63.36%     0.02%  [kernel]        [k] vmalloc\n 63.34%     2.64%  [kernel]        [k] __vmalloc_node_range\n 30.42%     4.46%  [kernel]        [k] vfree.part.0\n 28.98%     2.51%  [kernel]        [k] __alloc_pages_bulk\n 27.28%     0.19%  [kernel]        [k] __get_vm_area_node\n 26.13%     1.50%  [kernel]        [k] alloc_vmap_area\n 21.72%    21.67%  [kernel]        [k] clear_page_rep\n 19.51%     2.43%  [kernel]        [k] _raw_spin_lock\n 16.61%    16.51%  [kernel]        [k] native_queued_spin_lock_slowpath\n 13.40%     2.07%  [kernel]        [k] free_unref_page\n 10.62%     0.01%  [kernel]        [k] remove_vm_area\n  9.02%     8.73%  [kernel]        [k] insert_vmap_area\n  8.94%     0.00%  [kernel]        [k] ret_from_fork_asm\n  8.94%     0.00%  [kernel]        [k] ret_from_fork\n  8.94%     0.00%  [kernel]        [k] kthread\n  8.29%     0.00%  [test_vmalloc]  [k] test_func\n  7.81%     0.05%  [test_vmalloc]  [k] full_fit_alloc_test\n  5.30%     4.73%  [kernel]        [k] purge_vmap_node\n  4.47%     2.65%  [kernel]        [k] free_vmap_area_noflush\n<patch-series perf>\n\nconfirms that a native_queued_spin_lock_slowpath goes down to\n16.51% percent from 93.07%.\n\nThe throughput is ~12x higher:\n\nurezki@pc638:~$ time sudo ./test_vmalloc.sh run_test_mask=7 nr_threads=64\nRun the test with following parameters: run_test_mask=7 nr_threads=64\nDone.\nCheck the kernel ring buffer to see the summary.\n\nreal    10m51.271s\nuser    0m0.013s\nsys     0m0.187s\nurezki@pc638:~$\n\nurezki@pc638:~$ time sudo ./test_vmalloc.sh run_test_mask=7 nr_threads=64\nRun the test with following parameters: run_test_mask=7 nr_threads=64\nDone.\nCheck the kernel ring buffer to see the summary.\n\nreal    0m51.301s\nuser    0m0.015s\nsys     0m0.040s\nurezki@pc638:~$\n\nLink: https://lkml.kernel.org/r/20240102184633.748113-11-urezki@gmail.com\nSigned-off-by: Uladzislau Rezki (Sony) <urezki@gmail.com>\nCc: Baoquan He <bhe@redhat.com>\nCc: Christoph Hellwig <hch@lst.de>\nCc: Dave Chinner <david@fromorbit.com>\nCc: Joel Fernandes (Google) <joel@joelfernandes.org>\nCc: Kazuhito Hagio <k-hagio-ab@nec.com>\nCc: Liam R. Howlett <Liam.Howlett@oracle.com>\nCc: Lorenzo Stoakes <lstoakes@gmail.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Oleksiy Avramchenko <oleksiy.avramchenko@sony.com>\nCc: Paul E. McKenney <paulmck@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: vmalloc: add va_alloc() helper", "commit_message": "Patch series \"Mitigate a vmap lock contention\", v3.\n\n1. Motivation\n\n- Offload global vmap locks making it scaled to number of CPUS;\n\n- If possible and there is an agreement, we can remove the \"Per cpu kva\n  allocator\" to make the vmap code to be more simple;\n\n- There were complaints from XFS folk that a vmalloc might be contented\n  on their workloads.\n\n2. Design(high level overview)\n\nWe introduce an effective vmap node logic.  A node behaves as independent\nentity to serve an allocation request directly(if possible) from its pool.\nThat way it bypasses a global vmap space that is protected by its own\nlock.\n\nAn access to pools are serialized by CPUs.  Number of nodes are equal to\nnumber of CPUs in a system.  Please note the high threshold is bound to\n128 nodes.\n\nPools are size segregated and populated based on system demand.  The\nmaximum alloc request that can be stored into a segregated storage is 256\npages.  The lazily drain path decays a pool by 25% as a first step and as\nsecond populates it by fresh freed VAs for reuse instead of returning them\ninto a global space.\n\nWhen a VA is obtained(alloc path), it is stored in separate nodes.  A\nva->va_start address is converted into a correct node where it should be\nplaced and resided.  Doing so we balance VAs across the nodes as a result\nan access becomes scalable.  The addr_to_node() function does a proper\naddress conversion to a correct node.\n\nA vmap space is divided on segments with fixed size, it is 16 pages.  That\nway any address can be associated with a segment number.  Number of\nsegments are equal to num_possible_cpus() but not grater then 128.  The\nnumeration starts from 0.  See below how it is converted:\n\nstatic inline unsigned int\naddr_to_node_id(unsigned long addr)\n{\n\treturn (addr / zone_size) % nr_nodes;\n}\n\nOn a free path, a VA can be easily found by converting its \"va_start\"\naddress to a certain node it resides.  It is moved from \"busy\" data to\n\"lazy\" data structure.  Later on, as noted earlier, the lazy kworker\ndecays each node pool and populates it by fresh incoming VAs.  Please\nnote, a VA is returned to a node that did an alloc request.\n\n3. Test on AMD Ryzen Threadripper 3970X 32-Core Processor\n\nsudo ./test_vmalloc.sh run_test_mask=7 nr_threads=64\n\n<default perf>\n 94.41%     0.89%  [kernel]        [k] _raw_spin_lock\n 93.35%    93.07%  [kernel]        [k] native_queued_spin_lock_slowpath\n 76.13%     0.28%  [kernel]        [k] __vmalloc_node_range\n 72.96%     0.81%  [kernel]        [k] alloc_vmap_area\n 56.94%     0.00%  [kernel]        [k] __get_vm_area_node\n 41.95%     0.00%  [kernel]        [k] vmalloc\n 37.15%     0.01%  [test_vmalloc]  [k] full_fit_alloc_test\n 35.17%     0.00%  [kernel]        [k] ret_from_fork_asm\n 35.17%     0.00%  [kernel]        [k] ret_from_fork\n 35.17%     0.00%  [kernel]        [k] kthread\n 35.08%     0.00%  [test_vmalloc]  [k] test_func\n 34.45%     0.00%  [test_vmalloc]  [k] fix_size_alloc_test\n 28.09%     0.01%  [test_vmalloc]  [k] long_busy_list_alloc_test\n 23.53%     0.25%  [kernel]        [k] vfree.part.0\n 21.72%     0.00%  [kernel]        [k] remove_vm_area\n 20.08%     0.21%  [kernel]        [k] find_unlink_vmap_area\n  2.34%     0.61%  [kernel]        [k] free_vmap_area_noflush\n<default perf>\n   vs\n<patch-series perf>\n 82.32%     0.22%  [test_vmalloc]  [k] long_busy_list_alloc_test\n 63.36%     0.02%  [kernel]        [k] vmalloc\n 63.34%     2.64%  [kernel]        [k] __vmalloc_node_range\n 30.42%     4.46%  [kernel]        [k] vfree.part.0\n 28.98%     2.51%  [kernel]        [k] __alloc_pages_bulk\n 27.28%     0.19%  [kernel]        [k] __get_vm_area_node\n 26.13%     1.50%  [kernel]        [k] alloc_vmap_area\n 21.72%    21.67%  [kernel]        [k] clear_page_rep\n 19.51%     2.43%  [kernel]        [k] _raw_spin_lock\n 16.61%    16.51%  [kernel]        [k] native_queued_spin_lock_slowpath\n 13.40%     2.07%  [kernel]        [k] free_unref_page\n 10.62%     0.01%  [kernel]        [k] remove_vm_area\n  9.02%     8.73%  [kernel]        [k] insert_vmap_area\n  8.94%     0.00%  [kernel]        [k] ret_from_fork_asm\n  8.94%     0.00%  [kernel]        [k] ret_from_fork\n  8.94%     0.00%  [kernel]        [k] kthread\n  8.29%     0.00%  [test_vmalloc]  [k] test_func\n  7.81%     0.05%  [test_vmalloc]  [k] full_fit_alloc_test\n  5.30%     4.73%  [kernel]        [k] purge_vmap_node\n  4.47%     2.65%  [kernel]        [k] free_vmap_area_noflush\n<patch-series perf>\n\nconfirms that a native_queued_spin_lock_slowpath goes down to\n16.51% percent from 93.07%.\n\nThe throughput is ~12x higher:\n\nurezki@pc638:~$ time sudo ./test_vmalloc.sh run_test_mask=7 nr_threads=64\nRun the test with following parameters: run_test_mask=7 nr_threads=64\nDone.\nCheck the kernel ring buffer to see the summary.\n\nreal    10m51.271s\nuser    0m0.013s\nsys     0m0.187s\nurezki@pc638:~$\n\nurezki@pc638:~$ time sudo ./test_vmalloc.sh run_test_mask=7 nr_threads=64\nRun the test with following parameters: run_test_mask=7 nr_threads=64\nDone.\nCheck the kernel ring buffer to see the summary.\n\nreal    0m51.301s\nuser    0m0.015s\nsys     0m0.040s\nurezki@pc638:~$\n\n\nThis patch (of 11):\n\nCurrently __alloc_vmap_area() function contains an open codded logic that\nfinds and adjusts a VA based on allocation request.\n\nIntroduce a va_alloc() helper that adjusts found VA only.  There is no a\nfunctional change as a result of this patch.\n\nLink: https://lkml.kernel.org/r/20240102184633.748113-1-urezki@gmail.com\nLink: https://lkml.kernel.org/r/20240102184633.748113-2-urezki@gmail.com\nSigned-off-by: Uladzislau Rezki (Sony) <urezki@gmail.com>\nReviewed-by: Baoquan He <bhe@redhat.com>\nReviewed-by: Christoph Hellwig <hch@lst.de>\nReviewed-by: Lorenzo Stoakes <lstoakes@gmail.com>\nCc: Dave Chinner <david@fromorbit.com>\nCc: Joel Fernandes (Google) <joel@joelfernandes.org>\nCc: Liam R. Howlett <Liam.Howlett@oracle.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Oleksiy Avramchenko <oleksiy.avramchenko@sony.com>\nCc: Paul E. McKenney <paulmck@kernel.org>\nCc: Kazuhito Hagio <k-hagio-ab@nec.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: vmalloc: support multiple nodes in vread_iter", "commit_message": "Extend the vread_iter() to be able to perform a sequential reading of VAs\nwhich are spread among multiple nodes.  So a data read over the /dev/kmem\ncorrectly reflects a vmalloc memory layout.\n\nLink: https://lkml.kernel.org/r/20240102184633.748113-9-urezki@gmail.com\nSigned-off-by: Uladzislau Rezki (Sony) <urezki@gmail.com>\nReviewed-by: Baoquan He <bhe@redhat.com>\nCc: Christoph Hellwig <hch@lst.de>\nCc: Dave Chinner <david@fromorbit.com>\nCc: Joel Fernandes (Google) <joel@joelfernandes.org>\nCc: Kazuhito Hagio <k-hagio-ab@nec.com>\nCc: Liam R. Howlett <Liam.Howlett@oracle.com>\nCc: Lorenzo Stoakes <lstoakes@gmail.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Oleksiy Avramchenko <oleksiy.avramchenko@sony.com>\nCc: Paul E. McKenney <paulmck@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/vmalloc: remove vmap_area_list", "commit_message": "Earlier, vmap_area_list is exported to vmcoreinfo so that makedumpfile get\nthe base address of vmalloc area.  Now, vmap_area_list is empty, so export\nVMALLOC_START to vmcoreinfo instead, and remove vmap_area_list.\n\n[urezki@gmail.com: fix a warning in the crash_save_vmcoreinfo_init()]\n  Link: https://lkml.kernel.org/r/20240111192329.449189-1-urezki@gmail.com\nLink: https://lkml.kernel.org/r/20240102184633.748113-6-urezki@gmail.com\nSigned-off-by: Baoquan He <bhe@redhat.com>\nSigned-off-by: Uladzislau Rezki (Sony) <urezki@gmail.com>\nAcked-by: Lorenzo Stoakes <lstoakes@gmail.com>\nCc: Christoph Hellwig <hch@lst.de>\nCc: Dave Chinner <david@fromorbit.com>\nCc: Joel Fernandes (Google) <joel@joelfernandes.org>\nCc: Kazuhito Hagio <k-hagio-ab@nec.com>\nCc: Liam R. Howlett <Liam.Howlett@oracle.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Oleksiy Avramchenko <oleksiy.avramchenko@sony.com>\nCc: Paul E. McKenney <paulmck@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: vmalloc: remove global vmap_area_root rb-tree", "commit_message": "Store allocated objects in a separate nodes.  A va->va_start address is\nconverted into a correct node where it should be placed and resided.  An\naddr_to_node() function is used to do a proper address conversion to\ndetermine a node that contains a VA.\n\nSuch approach balances VAs across nodes as a result an access becomes\nscalable.  Number of nodes in a system depends on number of CPUs.\n\nPlease note:\n\n1. As of now allocated VAs are bound to a node-0. It means the\n   patch does not give any difference comparing with a current\n   behavior;\n\n2. The global vmap_area_lock, vmap_area_root are removed as there\n   is no need in it anymore. The vmap_area_list is still kept and\n   is _empty_. It is exported for a kexec only;\n\n3. The vmallocinfo and vread() have to be reworked to be able to\n   handle multiple nodes.\n\n[urezki@gmail.com: mark vmap_init_free_space() with __init tag]\n  Link: https://lkml.kernel.org/r/20240111132628.299644-1-urezki@gmail.com\n[urezki@gmail.com: fix a wrong value passed to __find_vmap_area()]\n  Link: https://lkml.kernel.org/r/20240111121104.180993-1-urezki@gmail.com\nLink: https://lkml.kernel.org/r/20240102184633.748113-5-urezki@gmail.com\nSigned-off-by: Uladzislau Rezki (Sony) <urezki@gmail.com>\nReviewed-by: Baoquan He <bhe@redhat.com>\nReviewed-by: Christoph Hellwig <hch@lst.de>\nReviewed-by: Lorenzo Stoakes <lstoakes@gmail.com>\nReviewed-by: Anshuman Khandual <anshuman.khandual@arm.com>\nCc: Dave Chinner <david@fromorbit.com>\nCc: Joel Fernandes (Google) <joel@joelfernandes.org>\nCc: Kazuhito Hagio <k-hagio-ab@nec.com>\nCc: Liam R. Howlett <Liam.Howlett@oracle.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Oleksiy Avramchenko <oleksiy.avramchenko@sony.com>\nCc: Paul E. McKenney <paulmck@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: vmalloc: support multiple nodes in vmallocinfo", "commit_message": "Allocated areas are spread among nodes, it implies that the scanning has\nto be performed individually of each node in order to dump all existing\nVAs.\n\nLink: https://lkml.kernel.org/r/20240102184633.748113-10-urezki@gmail.com\nSigned-off-by: Uladzislau Rezki (Sony) <urezki@gmail.com>\nCc: Baoquan He <bhe@redhat.com>\nCc: Christoph Hellwig <hch@lst.de>\nCc: Dave Chinner <david@fromorbit.com>\nCc: Joel Fernandes (Google) <joel@joelfernandes.org>\nCc: Kazuhito Hagio <k-hagio-ab@nec.com>\nCc: Liam R. Howlett <Liam.Howlett@oracle.com>\nCc: Lorenzo Stoakes <lstoakes@gmail.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Oleksiy Avramchenko <oleksiy.avramchenko@sony.com>\nCc: Paul E. McKenney <paulmck@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}]
    }
}

2024-12-03 22:32:58,399 - extractor - DEBUG - deepseek response:
ChatCompletion(id='0ae958e1-b138-41bf-9096-ec912b526992', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "proper_nouns": [\n        {\n            "vmap_area": "A \'vmap_area\' is a data structure in the Linux kernel used to manage virtual memory mappings. It represents a region of virtual memory that has been allocated and mapped, typically for use by the vmalloc subsystem. Each vmap_area contains metadata about the allocated region, such as its start and end addresses, and is stored in a red-black tree for efficient lookup and management."\n        },\n        {\n            "red-black tree": "A \'red-black tree\' is a type of self-balancing binary search tree used in the Linux kernel for efficient data storage and retrieval. In the context of vmalloc, the red-black tree is used to store vmap_area structures, allowing for quick lookups and insertions of virtual memory regions."\n        },\n        {\n            "vmalloc": "The \'vmalloc\' subsystem in the Linux kernel is responsible for allocating large contiguous blocks of virtual memory, which may be mapped to non-contiguous physical pages. This is useful for allocating large buffers or data structures that require a contiguous address space but do not need to be physically contiguous."\n        },\n        {\n            "spinlock": "A \'spinlock\' is a synchronization primitive used in the Linux kernel to protect shared resources from concurrent access. In the context of vmalloc, spinlocks are used to protect the global data structures that manage virtual memory mappings, ensuring that only one process can modify these structures at a time."\n        }\n    ],\n    "background_knowledge": [\n        "The vmalloc subsystem in the Linux kernel is designed to allocate large blocks of virtual memory, which can be mapped to non-contiguous physical pages. This is particularly useful for allocating large buffers or data structures that require a contiguous address space but do not need to be physically contiguous.",\n        "The vmap_area structure is used to manage these allocated regions of virtual memory. Each vmap_area is stored in a red-black tree for efficient lookup and management.",\n        "The global purge-lock and purge-tree were previously used to manage lazily-freed areas in the vmalloc space. These structures required a global locking primitive, which could lead to contention and reduced scalability, especially on systems with many CPUs."\n    ],\n    "kernel_changes": [\n        "Before the implementation of this feature, the vmalloc subsystem used a global purge-lock and purge-tree to manage lazily-freed areas. This approach required a global locking primitive, which could lead to contention and reduced scalability on multi-CPU systems.",\n        "After the implementation, the kernel was modified to store lazily-freed areas in a node-based structure, where each node corresponds to a CPU. This approach eliminates the need for a global locking primitive, making the access to these areas more scalable and reducing contention.",\n        "Additionally, the global purge-lock, purge-tree, and purge list were removed, further reducing the overhead associated with managing lazily-freed areas in the vmalloc space.",\n        "The introduction of a shrinker mechanism was also added to return cached VAs back to the global vmalloc space when the system enters a low memory mode, helping to manage memory more efficiently."\n    ],\n    "comprehensive_understanding": "This feature aims to mitigate vmap lock contention by improving the scalability of the vmalloc subsystem. By removing the global purge-lock and purge-tree, and instead storing lazily-freed areas in a node-based structure, the kernel reduces contention and improves scalability, especially on systems with many CPUs. Additionally, the introduction of a shrinker mechanism helps to manage memory more efficiently by returning cached VAs to the global vmalloc space when necessary. Overall, this feature enhances the performance and scalability of the vmalloc subsystem, making it more suitable for modern, multi-CPU systems."\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733236335, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=818, prompt_tokens=9714, total_tokens=10532, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=1472, prompt_cache_miss_tokens=8242))
2024-12-03 22:32:58,399 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个命名实体抽取 (NER) 任务的专家，同时也是 linux 内核的专家。给你一个 linux 内核特性的描述 (feature_description)、对应的若干 commit 的标题和提交信息 (commits)、该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)、为了让开发者理解该特性所需的与这个特性直接相关的背景知识 (background_knowledge)、该特性被实现前后，linux 内核发生的变化 (kernel_changes)、对该特性的综合理解 (comprehensive_understanding)。你的任务是：根据这些信息提取出在 linux 内核中涉及到的和这个特性相关的概念实体 (concepts)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}],
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": [
                "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
            ]
        },
        "output": {
            "concepts": ["Copy-on-Write (CoW)", "mTHP", "folio", "memory reclamation", "memory availability", "anti-fragmentation"]
        }
    ],
    "input": {
        "feature_description": "Mitigate a vmap lock contention",
        "commits": [{"commit_subject": "mm: vmalloc: remove global purge_vmap_area_root rb-tree", "commit_message": "Similar to busy VA, lazily-freed area is stored to a node it belongs to. \nSuch approach does not require any global locking primitive, instead an\naccess becomes scalable what mitigates a contention.\n\nThis patch removes a global purge-lock, global purge-tree and global purge\nlist.\n\nLink: https://lkml.kernel.org/r/20240102184633.748113-7-urezki@gmail.com\nSigned-off-by: Uladzislau Rezki (Sony) <urezki@gmail.com>\nReviewed-by: Baoquan He <bhe@redhat.com>\nCc: Christoph Hellwig <hch@lst.de>\nCc: Dave Chinner <david@fromorbit.com>\nCc: Joel Fernandes (Google) <joel@joelfernandes.org>\nCc: Kazuhito Hagio <k-hagio-ab@nec.com>\nCc: Liam R. Howlett <Liam.Howlett@oracle.com>\nCc: Lorenzo Stoakes <lstoakes@gmail.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Oleksiy Avramchenko <oleksiy.avramchenko@sony.com>\nCc: Paul E. McKenney <paulmck@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: vmalloc: offload free_vmap_area_lock lock", "commit_message": "Concurrent access to a global vmap space is a bottle-neck.  We can\nsimulate a high contention by running a vmalloc test suite.\n\nTo address it, introduce an effective vmap node logic.  Each node behaves\nas independent entity.  When a node is accessed it serves a request\ndirectly(if possible) from its pool.\n\nThis model has a size based pool for requests, i.e.  pools are serialized\nand populated based on object size and real demand.  A maximum object size\nthat pool can handle is set to 256 pages.\n\nThis technique reduces a pressure on the global vmap lock.\n\nLink: https://lkml.kernel.org/r/20240102184633.748113-8-urezki@gmail.com\nSigned-off-by: Uladzislau Rezki (Sony) <urezki@gmail.com>\nCc: Baoquan He <bhe@redhat.com>\nCc: Christoph Hellwig <hch@lst.de>\nCc: Dave Chinner <david@fromorbit.com>\nCc: Joel Fernandes (Google) <joel@joelfernandes.org>\nCc: Kazuhito Hagio <k-hagio-ab@nec.com>\nCc: Liam R. Howlett <Liam.Howlett@oracle.com>\nCc: Lorenzo Stoakes <lstoakes@gmail.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Oleksiy Avramchenko <oleksiy.avramchenko@sony.com>\nCc: Paul E. McKenney <paulmck@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: vmalloc: rename adjust_va_to_fit_type() function", "commit_message": "This patch renames the adjust_va_to_fit_type() function to va_clip() which\nis shorter and more expressive.\n\nThere is no a functional change as a result of this patch.\n\nLink: https://lkml.kernel.org/r/20240102184633.748113-3-urezki@gmail.com\nSigned-off-by: Uladzislau Rezki (Sony) <urezki@gmail.com>\nReviewed-by: Baoquan He <bhe@redhat.com>\nReviewed-by: Christoph Hellwig <hch@lst.de>\nReviewed-by: Lorenzo Stoakes <lstoakes@gmail.com>\nCc: Dave Chinner <david@fromorbit.com>\nCc: Joel Fernandes (Google) <joel@joelfernandes.org>\nCc: Kazuhito Hagio <k-hagio-ab@nec.com>\nCc: Liam R. Howlett <Liam.Howlett@oracle.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Oleksiy Avramchenko <oleksiy.avramchenko@sony.com>\nCc: Paul E. McKenney <paulmck@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: vmalloc: add a shrinker to drain vmap pools", "commit_message": "The added shrinker is used to return back current cached VAs into a global\nvmap space, when a system enters into a low memory mode.\n\nLink: https://lkml.kernel.org/r/20240102184633.748113-12-urezki@gmail.com\nSigned-off-by: Uladzislau Rezki (Sony) <urezki@gmail.com>\nCc: Kazuhito Hagio <k-hagio-ab@nec.com>\nCc: Baoquan He <bhe@redhat.com>\nCc: Christoph Hellwig <hch@lst.de>\nCc: Dave Chinner <david@fromorbit.com>\nCc: Joel Fernandes (Google) <joel@joelfernandes.org>\nCc: Liam R. Howlett <Liam.Howlett@oracle.com>\nCc: Lorenzo Stoakes <lstoakes@gmail.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Oleksiy Avramchenko <oleksiy.avramchenko@sony.com>\nCc: Paul E. McKenney <paulmck@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: vmalloc: move vmap_init_free_space() down in vmalloc.c", "commit_message": "A vmap_init_free_space() is a function that setups a vmap space and is\nconsidered as part of initialization phase.  Since a main entry which is\nvmalloc_init(), has been moved down in vmalloc.c it makes sense to follow\nthe pattern.\n\nThere is no a functional change as a result of this patch.\n\nLink: https://lkml.kernel.org/r/20240102184633.748113-4-urezki@gmail.com\nSigned-off-by: Uladzislau Rezki (Sony) <urezki@gmail.com>\nReviewed-by: Baoquan He <bhe@redhat.com>\nReviewed-by: Christoph Hellwig <hch@lst.de>\nReviewed-by: Lorenzo Stoakes <lstoakes@gmail.com>\nCc: Dave Chinner <david@fromorbit.com>\nCc: Joel Fernandes (Google) <joel@joelfernandes.org>\nCc: Kazuhito Hagio <k-hagio-ab@nec.com>\nCc: Liam R. Howlett <Liam.Howlett@oracle.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Oleksiy Avramchenko <oleksiy.avramchenko@sony.com>\nCc: Paul E. McKenney <paulmck@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: vmalloc: set nr_nodes based on CPUs in a system", "commit_message": "A number of nodes which are used in the alloc/free paths is set based on\nnum_possible_cpus() in a system.  Please note a high limit threshold\nthough is fixed and corresponds to 128 nodes.\n\nFor 32-bit or single core systems an access to a global vmap heap is not\nbalanced.  Such small systems do not suffer from lock contentions due to\nlow number of CPUs.  In such case the nr_nodes is equal to 1.\n\nTest on AMD Ryzen Threadripper 3970X 32-Core Processor: sudo\n./test_vmalloc.sh run_test_mask=7 nr_threads=64\n\n<default perf>\n 94.41%     0.89%  [kernel]        [k] _raw_spin_lock\n 93.35%    93.07%  [kernel]        [k] native_queued_spin_lock_slowpath\n 76.13%     0.28%  [kernel]        [k] __vmalloc_node_range\n 72.96%     0.81%  [kernel]        [k] alloc_vmap_area\n 56.94%     0.00%  [kernel]        [k] __get_vm_area_node\n 41.95%     0.00%  [kernel]        [k] vmalloc\n 37.15%     0.01%  [test_vmalloc]  [k] full_fit_alloc_test\n 35.17%     0.00%  [kernel]        [k] ret_from_fork_asm\n 35.17%     0.00%  [kernel]        [k] ret_from_fork\n 35.17%     0.00%  [kernel]        [k] kthread\n 35.08%     0.00%  [test_vmalloc]  [k] test_func\n 34.45%     0.00%  [test_vmalloc]  [k] fix_size_alloc_test\n 28.09%     0.01%  [test_vmalloc]  [k] long_busy_list_alloc_test\n 23.53%     0.25%  [kernel]        [k] vfree.part.0\n 21.72%     0.00%  [kernel]        [k] remove_vm_area\n 20.08%     0.21%  [kernel]        [k] find_unlink_vmap_area\n  2.34%     0.61%  [kernel]        [k] free_vmap_area_noflush\n<default perf>\n   vs\n<patch-series perf>\n 82.32%     0.22%  [test_vmalloc]  [k] long_busy_list_alloc_test\n 63.36%     0.02%  [kernel]        [k] vmalloc\n 63.34%     2.64%  [kernel]        [k] __vmalloc_node_range\n 30.42%     4.46%  [kernel]        [k] vfree.part.0\n 28.98%     2.51%  [kernel]        [k] __alloc_pages_bulk\n 27.28%     0.19%  [kernel]        [k] __get_vm_area_node\n 26.13%     1.50%  [kernel]        [k] alloc_vmap_area\n 21.72%    21.67%  [kernel]        [k] clear_page_rep\n 19.51%     2.43%  [kernel]        [k] _raw_spin_lock\n 16.61%    16.51%  [kernel]        [k] native_queued_spin_lock_slowpath\n 13.40%     2.07%  [kernel]        [k] free_unref_page\n 10.62%     0.01%  [kernel]        [k] remove_vm_area\n  9.02%     8.73%  [kernel]        [k] insert_vmap_area\n  8.94%     0.00%  [kernel]        [k] ret_from_fork_asm\n  8.94%     0.00%  [kernel]        [k] ret_from_fork\n  8.94%     0.00%  [kernel]        [k] kthread\n  8.29%     0.00%  [test_vmalloc]  [k] test_func\n  7.81%     0.05%  [test_vmalloc]  [k] full_fit_alloc_test\n  5.30%     4.73%  [kernel]        [k] purge_vmap_node\n  4.47%     2.65%  [kernel]        [k] free_vmap_area_noflush\n<patch-series perf>\n\nconfirms that a native_queued_spin_lock_slowpath goes down to\n16.51% percent from 93.07%.\n\nThe throughput is ~12x higher:\n\nurezki@pc638:~$ time sudo ./test_vmalloc.sh run_test_mask=7 nr_threads=64\nRun the test with following parameters: run_test_mask=7 nr_threads=64\nDone.\nCheck the kernel ring buffer to see the summary.\n\nreal    10m51.271s\nuser    0m0.013s\nsys     0m0.187s\nurezki@pc638:~$\n\nurezki@pc638:~$ time sudo ./test_vmalloc.sh run_test_mask=7 nr_threads=64\nRun the test with following parameters: run_test_mask=7 nr_threads=64\nDone.\nCheck the kernel ring buffer to see the summary.\n\nreal    0m51.301s\nuser    0m0.015s\nsys     0m0.040s\nurezki@pc638:~$\n\nLink: https://lkml.kernel.org/r/20240102184633.748113-11-urezki@gmail.com\nSigned-off-by: Uladzislau Rezki (Sony) <urezki@gmail.com>\nCc: Baoquan He <bhe@redhat.com>\nCc: Christoph Hellwig <hch@lst.de>\nCc: Dave Chinner <david@fromorbit.com>\nCc: Joel Fernandes (Google) <joel@joelfernandes.org>\nCc: Kazuhito Hagio <k-hagio-ab@nec.com>\nCc: Liam R. Howlett <Liam.Howlett@oracle.com>\nCc: Lorenzo Stoakes <lstoakes@gmail.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Oleksiy Avramchenko <oleksiy.avramchenko@sony.com>\nCc: Paul E. McKenney <paulmck@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: vmalloc: add va_alloc() helper", "commit_message": "Patch series \"Mitigate a vmap lock contention\", v3.\n\n1. Motivation\n\n- Offload global vmap locks making it scaled to number of CPUS;\n\n- If possible and there is an agreement, we can remove the \"Per cpu kva\n  allocator\" to make the vmap code to be more simple;\n\n- There were complaints from XFS folk that a vmalloc might be contented\n  on their workloads.\n\n2. Design(high level overview)\n\nWe introduce an effective vmap node logic.  A node behaves as independent\nentity to serve an allocation request directly(if possible) from its pool.\nThat way it bypasses a global vmap space that is protected by its own\nlock.\n\nAn access to pools are serialized by CPUs.  Number of nodes are equal to\nnumber of CPUs in a system.  Please note the high threshold is bound to\n128 nodes.\n\nPools are size segregated and populated based on system demand.  The\nmaximum alloc request that can be stored into a segregated storage is 256\npages.  The lazily drain path decays a pool by 25% as a first step and as\nsecond populates it by fresh freed VAs for reuse instead of returning them\ninto a global space.\n\nWhen a VA is obtained(alloc path), it is stored in separate nodes.  A\nva->va_start address is converted into a correct node where it should be\nplaced and resided.  Doing so we balance VAs across the nodes as a result\nan access becomes scalable.  The addr_to_node() function does a proper\naddress conversion to a correct node.\n\nA vmap space is divided on segments with fixed size, it is 16 pages.  That\nway any address can be associated with a segment number.  Number of\nsegments are equal to num_possible_cpus() but not grater then 128.  The\nnumeration starts from 0.  See below how it is converted:\n\nstatic inline unsigned int\naddr_to_node_id(unsigned long addr)\n{\n\treturn (addr / zone_size) % nr_nodes;\n}\n\nOn a free path, a VA can be easily found by converting its \"va_start\"\naddress to a certain node it resides.  It is moved from \"busy\" data to\n\"lazy\" data structure.  Later on, as noted earlier, the lazy kworker\ndecays each node pool and populates it by fresh incoming VAs.  Please\nnote, a VA is returned to a node that did an alloc request.\n\n3. Test on AMD Ryzen Threadripper 3970X 32-Core Processor\n\nsudo ./test_vmalloc.sh run_test_mask=7 nr_threads=64\n\n<default perf>\n 94.41%     0.89%  [kernel]        [k] _raw_spin_lock\n 93.35%    93.07%  [kernel]        [k] native_queued_spin_lock_slowpath\n 76.13%     0.28%  [kernel]        [k] __vmalloc_node_range\n 72.96%     0.81%  [kernel]        [k] alloc_vmap_area\n 56.94%     0.00%  [kernel]        [k] __get_vm_area_node\n 41.95%     0.00%  [kernel]        [k] vmalloc\n 37.15%     0.01%  [test_vmalloc]  [k] full_fit_alloc_test\n 35.17%     0.00%  [kernel]        [k] ret_from_fork_asm\n 35.17%     0.00%  [kernel]        [k] ret_from_fork\n 35.17%     0.00%  [kernel]        [k] kthread\n 35.08%     0.00%  [test_vmalloc]  [k] test_func\n 34.45%     0.00%  [test_vmalloc]  [k] fix_size_alloc_test\n 28.09%     0.01%  [test_vmalloc]  [k] long_busy_list_alloc_test\n 23.53%     0.25%  [kernel]        [k] vfree.part.0\n 21.72%     0.00%  [kernel]        [k] remove_vm_area\n 20.08%     0.21%  [kernel]        [k] find_unlink_vmap_area\n  2.34%     0.61%  [kernel]        [k] free_vmap_area_noflush\n<default perf>\n   vs\n<patch-series perf>\n 82.32%     0.22%  [test_vmalloc]  [k] long_busy_list_alloc_test\n 63.36%     0.02%  [kernel]        [k] vmalloc\n 63.34%     2.64%  [kernel]        [k] __vmalloc_node_range\n 30.42%     4.46%  [kernel]        [k] vfree.part.0\n 28.98%     2.51%  [kernel]        [k] __alloc_pages_bulk\n 27.28%     0.19%  [kernel]        [k] __get_vm_area_node\n 26.13%     1.50%  [kernel]        [k] alloc_vmap_area\n 21.72%    21.67%  [kernel]        [k] clear_page_rep\n 19.51%     2.43%  [kernel]        [k] _raw_spin_lock\n 16.61%    16.51%  [kernel]        [k] native_queued_spin_lock_slowpath\n 13.40%     2.07%  [kernel]        [k] free_unref_page\n 10.62%     0.01%  [kernel]        [k] remove_vm_area\n  9.02%     8.73%  [kernel]        [k] insert_vmap_area\n  8.94%     0.00%  [kernel]        [k] ret_from_fork_asm\n  8.94%     0.00%  [kernel]        [k] ret_from_fork\n  8.94%     0.00%  [kernel]        [k] kthread\n  8.29%     0.00%  [test_vmalloc]  [k] test_func\n  7.81%     0.05%  [test_vmalloc]  [k] full_fit_alloc_test\n  5.30%     4.73%  [kernel]        [k] purge_vmap_node\n  4.47%     2.65%  [kernel]        [k] free_vmap_area_noflush\n<patch-series perf>\n\nconfirms that a native_queued_spin_lock_slowpath goes down to\n16.51% percent from 93.07%.\n\nThe throughput is ~12x higher:\n\nurezki@pc638:~$ time sudo ./test_vmalloc.sh run_test_mask=7 nr_threads=64\nRun the test with following parameters: run_test_mask=7 nr_threads=64\nDone.\nCheck the kernel ring buffer to see the summary.\n\nreal    10m51.271s\nuser    0m0.013s\nsys     0m0.187s\nurezki@pc638:~$\n\nurezki@pc638:~$ time sudo ./test_vmalloc.sh run_test_mask=7 nr_threads=64\nRun the test with following parameters: run_test_mask=7 nr_threads=64\nDone.\nCheck the kernel ring buffer to see the summary.\n\nreal    0m51.301s\nuser    0m0.015s\nsys     0m0.040s\nurezki@pc638:~$\n\n\nThis patch (of 11):\n\nCurrently __alloc_vmap_area() function contains an open codded logic that\nfinds and adjusts a VA based on allocation request.\n\nIntroduce a va_alloc() helper that adjusts found VA only.  There is no a\nfunctional change as a result of this patch.\n\nLink: https://lkml.kernel.org/r/20240102184633.748113-1-urezki@gmail.com\nLink: https://lkml.kernel.org/r/20240102184633.748113-2-urezki@gmail.com\nSigned-off-by: Uladzislau Rezki (Sony) <urezki@gmail.com>\nReviewed-by: Baoquan He <bhe@redhat.com>\nReviewed-by: Christoph Hellwig <hch@lst.de>\nReviewed-by: Lorenzo Stoakes <lstoakes@gmail.com>\nCc: Dave Chinner <david@fromorbit.com>\nCc: Joel Fernandes (Google) <joel@joelfernandes.org>\nCc: Liam R. Howlett <Liam.Howlett@oracle.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Oleksiy Avramchenko <oleksiy.avramchenko@sony.com>\nCc: Paul E. McKenney <paulmck@kernel.org>\nCc: Kazuhito Hagio <k-hagio-ab@nec.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: vmalloc: support multiple nodes in vread_iter", "commit_message": "Extend the vread_iter() to be able to perform a sequential reading of VAs\nwhich are spread among multiple nodes.  So a data read over the /dev/kmem\ncorrectly reflects a vmalloc memory layout.\n\nLink: https://lkml.kernel.org/r/20240102184633.748113-9-urezki@gmail.com\nSigned-off-by: Uladzislau Rezki (Sony) <urezki@gmail.com>\nReviewed-by: Baoquan He <bhe@redhat.com>\nCc: Christoph Hellwig <hch@lst.de>\nCc: Dave Chinner <david@fromorbit.com>\nCc: Joel Fernandes (Google) <joel@joelfernandes.org>\nCc: Kazuhito Hagio <k-hagio-ab@nec.com>\nCc: Liam R. Howlett <Liam.Howlett@oracle.com>\nCc: Lorenzo Stoakes <lstoakes@gmail.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Oleksiy Avramchenko <oleksiy.avramchenko@sony.com>\nCc: Paul E. McKenney <paulmck@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/vmalloc: remove vmap_area_list", "commit_message": "Earlier, vmap_area_list is exported to vmcoreinfo so that makedumpfile get\nthe base address of vmalloc area.  Now, vmap_area_list is empty, so export\nVMALLOC_START to vmcoreinfo instead, and remove vmap_area_list.\n\n[urezki@gmail.com: fix a warning in the crash_save_vmcoreinfo_init()]\n  Link: https://lkml.kernel.org/r/20240111192329.449189-1-urezki@gmail.com\nLink: https://lkml.kernel.org/r/20240102184633.748113-6-urezki@gmail.com\nSigned-off-by: Baoquan He <bhe@redhat.com>\nSigned-off-by: Uladzislau Rezki (Sony) <urezki@gmail.com>\nAcked-by: Lorenzo Stoakes <lstoakes@gmail.com>\nCc: Christoph Hellwig <hch@lst.de>\nCc: Dave Chinner <david@fromorbit.com>\nCc: Joel Fernandes (Google) <joel@joelfernandes.org>\nCc: Kazuhito Hagio <k-hagio-ab@nec.com>\nCc: Liam R. Howlett <Liam.Howlett@oracle.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Oleksiy Avramchenko <oleksiy.avramchenko@sony.com>\nCc: Paul E. McKenney <paulmck@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: vmalloc: remove global vmap_area_root rb-tree", "commit_message": "Store allocated objects in a separate nodes.  A va->va_start address is\nconverted into a correct node where it should be placed and resided.  An\naddr_to_node() function is used to do a proper address conversion to\ndetermine a node that contains a VA.\n\nSuch approach balances VAs across nodes as a result an access becomes\nscalable.  Number of nodes in a system depends on number of CPUs.\n\nPlease note:\n\n1. As of now allocated VAs are bound to a node-0. It means the\n   patch does not give any difference comparing with a current\n   behavior;\n\n2. The global vmap_area_lock, vmap_area_root are removed as there\n   is no need in it anymore. The vmap_area_list is still kept and\n   is _empty_. It is exported for a kexec only;\n\n3. The vmallocinfo and vread() have to be reworked to be able to\n   handle multiple nodes.\n\n[urezki@gmail.com: mark vmap_init_free_space() with __init tag]\n  Link: https://lkml.kernel.org/r/20240111132628.299644-1-urezki@gmail.com\n[urezki@gmail.com: fix a wrong value passed to __find_vmap_area()]\n  Link: https://lkml.kernel.org/r/20240111121104.180993-1-urezki@gmail.com\nLink: https://lkml.kernel.org/r/20240102184633.748113-5-urezki@gmail.com\nSigned-off-by: Uladzislau Rezki (Sony) <urezki@gmail.com>\nReviewed-by: Baoquan He <bhe@redhat.com>\nReviewed-by: Christoph Hellwig <hch@lst.de>\nReviewed-by: Lorenzo Stoakes <lstoakes@gmail.com>\nReviewed-by: Anshuman Khandual <anshuman.khandual@arm.com>\nCc: Dave Chinner <david@fromorbit.com>\nCc: Joel Fernandes (Google) <joel@joelfernandes.org>\nCc: Kazuhito Hagio <k-hagio-ab@nec.com>\nCc: Liam R. Howlett <Liam.Howlett@oracle.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Oleksiy Avramchenko <oleksiy.avramchenko@sony.com>\nCc: Paul E. McKenney <paulmck@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: vmalloc: support multiple nodes in vmallocinfo", "commit_message": "Allocated areas are spread among nodes, it implies that the scanning has\nto be performed individually of each node in order to dump all existing\nVAs.\n\nLink: https://lkml.kernel.org/r/20240102184633.748113-10-urezki@gmail.com\nSigned-off-by: Uladzislau Rezki (Sony) <urezki@gmail.com>\nCc: Baoquan He <bhe@redhat.com>\nCc: Christoph Hellwig <hch@lst.de>\nCc: Dave Chinner <david@fromorbit.com>\nCc: Joel Fernandes (Google) <joel@joelfernandes.org>\nCc: Kazuhito Hagio <k-hagio-ab@nec.com>\nCc: Liam R. Howlett <Liam.Howlett@oracle.com>\nCc: Lorenzo Stoakes <lstoakes@gmail.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Oleksiy Avramchenko <oleksiy.avramchenko@sony.com>\nCc: Paul E. McKenney <paulmck@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}],
        "proper_nouns": [{"vmap_area": "A 'vmap_area' is a data structure in the Linux kernel used to manage virtual memory mappings. It represents a region of virtual memory that has been allocated and mapped, typically for use by the vmalloc subsystem. Each vmap_area contains metadata about the allocated region, such as its start and end addresses, and is stored in a red-black tree for efficient lookup and management."}, {"red-black tree": "A 'red-black tree' is a type of self-balancing binary search tree used in the Linux kernel for efficient data storage and retrieval. In the context of vmalloc, the red-black tree is used to store vmap_area structures, allowing for quick lookups and insertions of virtual memory regions."}, {"vmalloc": "The 'vmalloc' subsystem in the Linux kernel is responsible for allocating large contiguous blocks of virtual memory, which may be mapped to non-contiguous physical pages. This is useful for allocating large buffers or data structures that require a contiguous address space but do not need to be physically contiguous."}, {"spinlock": "A 'spinlock' is a synchronization primitive used in the Linux kernel to protect shared resources from concurrent access. In the context of vmalloc, spinlocks are used to protect the global data structures that manage virtual memory mappings, ensuring that only one process can modify these structures at a time."}],
        "background_knowledge": ["The vmalloc subsystem in the Linux kernel is designed to allocate large blocks of virtual memory, which can be mapped to non-contiguous physical pages. This is particularly useful for allocating large buffers or data structures that require a contiguous address space but do not need to be physically contiguous.", "The vmap_area structure is used to manage these allocated regions of virtual memory. Each vmap_area is stored in a red-black tree for efficient lookup and management.", "The global purge-lock and purge-tree were previously used to manage lazily-freed areas in the vmalloc space. These structures required a global locking primitive, which could lead to contention and reduced scalability, especially on systems with many CPUs."],
        "kernel_changes": ["Before the implementation of this feature, the vmalloc subsystem used a global purge-lock and purge-tree to manage lazily-freed areas. This approach required a global locking primitive, which could lead to contention and reduced scalability on multi-CPU systems.", "After the implementation, the kernel was modified to store lazily-freed areas in a node-based structure, where each node corresponds to a CPU. This approach eliminates the need for a global locking primitive, making the access to these areas more scalable and reducing contention.", "Additionally, the global purge-lock, purge-tree, and purge list were removed, further reducing the overhead associated with managing lazily-freed areas in the vmalloc space.", "The introduction of a shrinker mechanism was also added to return cached VAs back to the global vmalloc space when the system enters a low memory mode, helping to manage memory more efficiently."],
        "comprehensive_understanding": "This feature aims to mitigate vmap lock contention by improving the scalability of the vmalloc subsystem. By removing the global purge-lock and purge-tree, and instead storing lazily-freed areas in a node-based structure, the kernel reduces contention and improves scalability, especially on systems with many CPUs. Additionally, the introduction of a shrinker mechanism helps to manage memory more efficiently by returning cached VAs to the global vmalloc space when necessary. Overall, this feature enhances the performance and scalability of the vmalloc subsystem, making it more suitable for modern, multi-CPU systems."
    }
}

2024-12-03 22:33:02,345 - extractor - DEBUG - deepseek response:
ChatCompletion(id='7e6b33db-206f-48c9-9c79-5db5ebd98c6b', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "concepts": ["vmap_area", "red-black tree", "vmalloc", "spinlock", "scalability", "contention", "memory management", "node-based structure", "shrinker mechanism"]\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733236378, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=57, prompt_tokens=10529, total_tokens=10586, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=1536, prompt_cache_miss_tokens=8993))
2024-12-03 22:33:02,345 - extractor - DEBUG - feature_extracted of feature_id=33226:
{
    "feature_id": 33226,
    "feature_description": "Mitigate a vmap lock contention",
    "version": "6.9",
    "commits": [
        {
            "commit_subject": "mm: vmalloc: remove global purge_vmap_area_root rb-tree",
            "commit_message": "Similar to busy VA, lazily-freed area is stored to a node it belongs to. \nSuch approach does not require any global locking primitive, instead an\naccess becomes scalable what mitigates a contention.\n\nThis patch removes a global purge-lock, global purge-tree and global purge\nlist.\n\nLink: https://lkml.kernel.org/r/20240102184633.748113-7-urezki@gmail.com\nSigned-off-by: Uladzislau Rezki (Sony) <urezki@gmail.com>\nReviewed-by: Baoquan He <bhe@redhat.com>\nCc: Christoph Hellwig <hch@lst.de>\nCc: Dave Chinner <david@fromorbit.com>\nCc: Joel Fernandes (Google) <joel@joelfernandes.org>\nCc: Kazuhito Hagio <k-hagio-ab@nec.com>\nCc: Liam R. Howlett <Liam.Howlett@oracle.com>\nCc: Lorenzo Stoakes <lstoakes@gmail.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Oleksiy Avramchenko <oleksiy.avramchenko@sony.com>\nCc: Paul E. McKenney <paulmck@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm: vmalloc: offload free_vmap_area_lock lock",
            "commit_message": "Concurrent access to a global vmap space is a bottle-neck.  We can\nsimulate a high contention by running a vmalloc test suite.\n\nTo address it, introduce an effective vmap node logic.  Each node behaves\nas independent entity.  When a node is accessed it serves a request\ndirectly(if possible) from its pool.\n\nThis model has a size based pool for requests, i.e.  pools are serialized\nand populated based on object size and real demand.  A maximum object size\nthat pool can handle is set to 256 pages.\n\nThis technique reduces a pressure on the global vmap lock.\n\nLink: https://lkml.kernel.org/r/20240102184633.748113-8-urezki@gmail.com\nSigned-off-by: Uladzislau Rezki (Sony) <urezki@gmail.com>\nCc: Baoquan He <bhe@redhat.com>\nCc: Christoph Hellwig <hch@lst.de>\nCc: Dave Chinner <david@fromorbit.com>\nCc: Joel Fernandes (Google) <joel@joelfernandes.org>\nCc: Kazuhito Hagio <k-hagio-ab@nec.com>\nCc: Liam R. Howlett <Liam.Howlett@oracle.com>\nCc: Lorenzo Stoakes <lstoakes@gmail.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Oleksiy Avramchenko <oleksiy.avramchenko@sony.com>\nCc: Paul E. McKenney <paulmck@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm: vmalloc: rename adjust_va_to_fit_type() function",
            "commit_message": "This patch renames the adjust_va_to_fit_type() function to va_clip() which\nis shorter and more expressive.\n\nThere is no a functional change as a result of this patch.\n\nLink: https://lkml.kernel.org/r/20240102184633.748113-3-urezki@gmail.com\nSigned-off-by: Uladzislau Rezki (Sony) <urezki@gmail.com>\nReviewed-by: Baoquan He <bhe@redhat.com>\nReviewed-by: Christoph Hellwig <hch@lst.de>\nReviewed-by: Lorenzo Stoakes <lstoakes@gmail.com>\nCc: Dave Chinner <david@fromorbit.com>\nCc: Joel Fernandes (Google) <joel@joelfernandes.org>\nCc: Kazuhito Hagio <k-hagio-ab@nec.com>\nCc: Liam R. Howlett <Liam.Howlett@oracle.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Oleksiy Avramchenko <oleksiy.avramchenko@sony.com>\nCc: Paul E. McKenney <paulmck@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm: vmalloc: add a shrinker to drain vmap pools",
            "commit_message": "The added shrinker is used to return back current cached VAs into a global\nvmap space, when a system enters into a low memory mode.\n\nLink: https://lkml.kernel.org/r/20240102184633.748113-12-urezki@gmail.com\nSigned-off-by: Uladzislau Rezki (Sony) <urezki@gmail.com>\nCc: Kazuhito Hagio <k-hagio-ab@nec.com>\nCc: Baoquan He <bhe@redhat.com>\nCc: Christoph Hellwig <hch@lst.de>\nCc: Dave Chinner <david@fromorbit.com>\nCc: Joel Fernandes (Google) <joel@joelfernandes.org>\nCc: Liam R. Howlett <Liam.Howlett@oracle.com>\nCc: Lorenzo Stoakes <lstoakes@gmail.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Oleksiy Avramchenko <oleksiy.avramchenko@sony.com>\nCc: Paul E. McKenney <paulmck@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm: vmalloc: move vmap_init_free_space() down in vmalloc.c",
            "commit_message": "A vmap_init_free_space() is a function that setups a vmap space and is\nconsidered as part of initialization phase.  Since a main entry which is\nvmalloc_init(), has been moved down in vmalloc.c it makes sense to follow\nthe pattern.\n\nThere is no a functional change as a result of this patch.\n\nLink: https://lkml.kernel.org/r/20240102184633.748113-4-urezki@gmail.com\nSigned-off-by: Uladzislau Rezki (Sony) <urezki@gmail.com>\nReviewed-by: Baoquan He <bhe@redhat.com>\nReviewed-by: Christoph Hellwig <hch@lst.de>\nReviewed-by: Lorenzo Stoakes <lstoakes@gmail.com>\nCc: Dave Chinner <david@fromorbit.com>\nCc: Joel Fernandes (Google) <joel@joelfernandes.org>\nCc: Kazuhito Hagio <k-hagio-ab@nec.com>\nCc: Liam R. Howlett <Liam.Howlett@oracle.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Oleksiy Avramchenko <oleksiy.avramchenko@sony.com>\nCc: Paul E. McKenney <paulmck@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm: vmalloc: set nr_nodes based on CPUs in a system",
            "commit_message": "A number of nodes which are used in the alloc/free paths is set based on\nnum_possible_cpus() in a system.  Please note a high limit threshold\nthough is fixed and corresponds to 128 nodes.\n\nFor 32-bit or single core systems an access to a global vmap heap is not\nbalanced.  Such small systems do not suffer from lock contentions due to\nlow number of CPUs.  In such case the nr_nodes is equal to 1.\n\nTest on AMD Ryzen Threadripper 3970X 32-Core Processor: sudo\n./test_vmalloc.sh run_test_mask=7 nr_threads=64\n\n<default perf>\n 94.41%     0.89%  [kernel]        [k] _raw_spin_lock\n 93.35%    93.07%  [kernel]        [k] native_queued_spin_lock_slowpath\n 76.13%     0.28%  [kernel]        [k] __vmalloc_node_range\n 72.96%     0.81%  [kernel]        [k] alloc_vmap_area\n 56.94%     0.00%  [kernel]        [k] __get_vm_area_node\n 41.95%     0.00%  [kernel]        [k] vmalloc\n 37.15%     0.01%  [test_vmalloc]  [k] full_fit_alloc_test\n 35.17%     0.00%  [kernel]        [k] ret_from_fork_asm\n 35.17%     0.00%  [kernel]        [k] ret_from_fork\n 35.17%     0.00%  [kernel]        [k] kthread\n 35.08%     0.00%  [test_vmalloc]  [k] test_func\n 34.45%     0.00%  [test_vmalloc]  [k] fix_size_alloc_test\n 28.09%     0.01%  [test_vmalloc]  [k] long_busy_list_alloc_test\n 23.53%     0.25%  [kernel]        [k] vfree.part.0\n 21.72%     0.00%  [kernel]        [k] remove_vm_area\n 20.08%     0.21%  [kernel]        [k] find_unlink_vmap_area\n  2.34%     0.61%  [kernel]        [k] free_vmap_area_noflush\n<default perf>\n   vs\n<patch-series perf>\n 82.32%     0.22%  [test_vmalloc]  [k] long_busy_list_alloc_test\n 63.36%     0.02%  [kernel]        [k] vmalloc\n 63.34%     2.64%  [kernel]        [k] __vmalloc_node_range\n 30.42%     4.46%  [kernel]        [k] vfree.part.0\n 28.98%     2.51%  [kernel]        [k] __alloc_pages_bulk\n 27.28%     0.19%  [kernel]        [k] __get_vm_area_node\n 26.13%     1.50%  [kernel]        [k] alloc_vmap_area\n 21.72%    21.67%  [kernel]        [k] clear_page_rep\n 19.51%     2.43%  [kernel]        [k] _raw_spin_lock\n 16.61%    16.51%  [kernel]        [k] native_queued_spin_lock_slowpath\n 13.40%     2.07%  [kernel]        [k] free_unref_page\n 10.62%     0.01%  [kernel]        [k] remove_vm_area\n  9.02%     8.73%  [kernel]        [k] insert_vmap_area\n  8.94%     0.00%  [kernel]        [k] ret_from_fork_asm\n  8.94%     0.00%  [kernel]        [k] ret_from_fork\n  8.94%     0.00%  [kernel]        [k] kthread\n  8.29%     0.00%  [test_vmalloc]  [k] test_func\n  7.81%     0.05%  [test_vmalloc]  [k] full_fit_alloc_test\n  5.30%     4.73%  [kernel]        [k] purge_vmap_node\n  4.47%     2.65%  [kernel]        [k] free_vmap_area_noflush\n<patch-series perf>\n\nconfirms that a native_queued_spin_lock_slowpath goes down to\n16.51% percent from 93.07%.\n\nThe throughput is ~12x higher:\n\nurezki@pc638:~$ time sudo ./test_vmalloc.sh run_test_mask=7 nr_threads=64\nRun the test with following parameters: run_test_mask=7 nr_threads=64\nDone.\nCheck the kernel ring buffer to see the summary.\n\nreal    10m51.271s\nuser    0m0.013s\nsys     0m0.187s\nurezki@pc638:~$\n\nurezki@pc638:~$ time sudo ./test_vmalloc.sh run_test_mask=7 nr_threads=64\nRun the test with following parameters: run_test_mask=7 nr_threads=64\nDone.\nCheck the kernel ring buffer to see the summary.\n\nreal    0m51.301s\nuser    0m0.015s\nsys     0m0.040s\nurezki@pc638:~$\n\nLink: https://lkml.kernel.org/r/20240102184633.748113-11-urezki@gmail.com\nSigned-off-by: Uladzislau Rezki (Sony) <urezki@gmail.com>\nCc: Baoquan He <bhe@redhat.com>\nCc: Christoph Hellwig <hch@lst.de>\nCc: Dave Chinner <david@fromorbit.com>\nCc: Joel Fernandes (Google) <joel@joelfernandes.org>\nCc: Kazuhito Hagio <k-hagio-ab@nec.com>\nCc: Liam R. Howlett <Liam.Howlett@oracle.com>\nCc: Lorenzo Stoakes <lstoakes@gmail.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Oleksiy Avramchenko <oleksiy.avramchenko@sony.com>\nCc: Paul E. McKenney <paulmck@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm: vmalloc: add va_alloc() helper",
            "commit_message": "Patch series \"Mitigate a vmap lock contention\", v3.\n\n1. Motivation\n\n- Offload global vmap locks making it scaled to number of CPUS;\n\n- If possible and there is an agreement, we can remove the \"Per cpu kva\n  allocator\" to make the vmap code to be more simple;\n\n- There were complaints from XFS folk that a vmalloc might be contented\n  on their workloads.\n\n2. Design(high level overview)\n\nWe introduce an effective vmap node logic.  A node behaves as independent\nentity to serve an allocation request directly(if possible) from its pool.\nThat way it bypasses a global vmap space that is protected by its own\nlock.\n\nAn access to pools are serialized by CPUs.  Number of nodes are equal to\nnumber of CPUs in a system.  Please note the high threshold is bound to\n128 nodes.\n\nPools are size segregated and populated based on system demand.  The\nmaximum alloc request that can be stored into a segregated storage is 256\npages.  The lazily drain path decays a pool by 25% as a first step and as\nsecond populates it by fresh freed VAs for reuse instead of returning them\ninto a global space.\n\nWhen a VA is obtained(alloc path), it is stored in separate nodes.  A\nva->va_start address is converted into a correct node where it should be\nplaced and resided.  Doing so we balance VAs across the nodes as a result\nan access becomes scalable.  The addr_to_node() function does a proper\naddress conversion to a correct node.\n\nA vmap space is divided on segments with fixed size, it is 16 pages.  That\nway any address can be associated with a segment number.  Number of\nsegments are equal to num_possible_cpus() but not grater then 128.  The\nnumeration starts from 0.  See below how it is converted:\n\nstatic inline unsigned int\naddr_to_node_id(unsigned long addr)\n{\n\treturn (addr / zone_size) % nr_nodes;\n}\n\nOn a free path, a VA can be easily found by converting its \"va_start\"\naddress to a certain node it resides.  It is moved from \"busy\" data to\n\"lazy\" data structure.  Later on, as noted earlier, the lazy kworker\ndecays each node pool and populates it by fresh incoming VAs.  Please\nnote, a VA is returned to a node that did an alloc request.\n\n3. Test on AMD Ryzen Threadripper 3970X 32-Core Processor\n\nsudo ./test_vmalloc.sh run_test_mask=7 nr_threads=64\n\n<default perf>\n 94.41%     0.89%  [kernel]        [k] _raw_spin_lock\n 93.35%    93.07%  [kernel]        [k] native_queued_spin_lock_slowpath\n 76.13%     0.28%  [kernel]        [k] __vmalloc_node_range\n 72.96%     0.81%  [kernel]        [k] alloc_vmap_area\n 56.94%     0.00%  [kernel]        [k] __get_vm_area_node\n 41.95%     0.00%  [kernel]        [k] vmalloc\n 37.15%     0.01%  [test_vmalloc]  [k] full_fit_alloc_test\n 35.17%     0.00%  [kernel]        [k] ret_from_fork_asm\n 35.17%     0.00%  [kernel]        [k] ret_from_fork\n 35.17%     0.00%  [kernel]        [k] kthread\n 35.08%     0.00%  [test_vmalloc]  [k] test_func\n 34.45%     0.00%  [test_vmalloc]  [k] fix_size_alloc_test\n 28.09%     0.01%  [test_vmalloc]  [k] long_busy_list_alloc_test\n 23.53%     0.25%  [kernel]        [k] vfree.part.0\n 21.72%     0.00%  [kernel]        [k] remove_vm_area\n 20.08%     0.21%  [kernel]        [k] find_unlink_vmap_area\n  2.34%     0.61%  [kernel]        [k] free_vmap_area_noflush\n<default perf>\n   vs\n<patch-series perf>\n 82.32%     0.22%  [test_vmalloc]  [k] long_busy_list_alloc_test\n 63.36%     0.02%  [kernel]        [k] vmalloc\n 63.34%     2.64%  [kernel]        [k] __vmalloc_node_range\n 30.42%     4.46%  [kernel]        [k] vfree.part.0\n 28.98%     2.51%  [kernel]        [k] __alloc_pages_bulk\n 27.28%     0.19%  [kernel]        [k] __get_vm_area_node\n 26.13%     1.50%  [kernel]        [k] alloc_vmap_area\n 21.72%    21.67%  [kernel]        [k] clear_page_rep\n 19.51%     2.43%  [kernel]        [k] _raw_spin_lock\n 16.61%    16.51%  [kernel]        [k] native_queued_spin_lock_slowpath\n 13.40%     2.07%  [kernel]        [k] free_unref_page\n 10.62%     0.01%  [kernel]        [k] remove_vm_area\n  9.02%     8.73%  [kernel]        [k] insert_vmap_area\n  8.94%     0.00%  [kernel]        [k] ret_from_fork_asm\n  8.94%     0.00%  [kernel]        [k] ret_from_fork\n  8.94%     0.00%  [kernel]        [k] kthread\n  8.29%     0.00%  [test_vmalloc]  [k] test_func\n  7.81%     0.05%  [test_vmalloc]  [k] full_fit_alloc_test\n  5.30%     4.73%  [kernel]        [k] purge_vmap_node\n  4.47%     2.65%  [kernel]        [k] free_vmap_area_noflush\n<patch-series perf>\n\nconfirms that a native_queued_spin_lock_slowpath goes down to\n16.51% percent from 93.07%.\n\nThe throughput is ~12x higher:\n\nurezki@pc638:~$ time sudo ./test_vmalloc.sh run_test_mask=7 nr_threads=64\nRun the test with following parameters: run_test_mask=7 nr_threads=64\nDone.\nCheck the kernel ring buffer to see the summary.\n\nreal    10m51.271s\nuser    0m0.013s\nsys     0m0.187s\nurezki@pc638:~$\n\nurezki@pc638:~$ time sudo ./test_vmalloc.sh run_test_mask=7 nr_threads=64\nRun the test with following parameters: run_test_mask=7 nr_threads=64\nDone.\nCheck the kernel ring buffer to see the summary.\n\nreal    0m51.301s\nuser    0m0.015s\nsys     0m0.040s\nurezki@pc638:~$\n\n\nThis patch (of 11):\n\nCurrently __alloc_vmap_area() function contains an open codded logic that\nfinds and adjusts a VA based on allocation request.\n\nIntroduce a va_alloc() helper that adjusts found VA only.  There is no a\nfunctional change as a result of this patch.\n\nLink: https://lkml.kernel.org/r/20240102184633.748113-1-urezki@gmail.com\nLink: https://lkml.kernel.org/r/20240102184633.748113-2-urezki@gmail.com\nSigned-off-by: Uladzislau Rezki (Sony) <urezki@gmail.com>\nReviewed-by: Baoquan He <bhe@redhat.com>\nReviewed-by: Christoph Hellwig <hch@lst.de>\nReviewed-by: Lorenzo Stoakes <lstoakes@gmail.com>\nCc: Dave Chinner <david@fromorbit.com>\nCc: Joel Fernandes (Google) <joel@joelfernandes.org>\nCc: Liam R. Howlett <Liam.Howlett@oracle.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Oleksiy Avramchenko <oleksiy.avramchenko@sony.com>\nCc: Paul E. McKenney <paulmck@kernel.org>\nCc: Kazuhito Hagio <k-hagio-ab@nec.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm: vmalloc: support multiple nodes in vread_iter",
            "commit_message": "Extend the vread_iter() to be able to perform a sequential reading of VAs\nwhich are spread among multiple nodes.  So a data read over the /dev/kmem\ncorrectly reflects a vmalloc memory layout.\n\nLink: https://lkml.kernel.org/r/20240102184633.748113-9-urezki@gmail.com\nSigned-off-by: Uladzislau Rezki (Sony) <urezki@gmail.com>\nReviewed-by: Baoquan He <bhe@redhat.com>\nCc: Christoph Hellwig <hch@lst.de>\nCc: Dave Chinner <david@fromorbit.com>\nCc: Joel Fernandes (Google) <joel@joelfernandes.org>\nCc: Kazuhito Hagio <k-hagio-ab@nec.com>\nCc: Liam R. Howlett <Liam.Howlett@oracle.com>\nCc: Lorenzo Stoakes <lstoakes@gmail.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Oleksiy Avramchenko <oleksiy.avramchenko@sony.com>\nCc: Paul E. McKenney <paulmck@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm/vmalloc: remove vmap_area_list",
            "commit_message": "Earlier, vmap_area_list is exported to vmcoreinfo so that makedumpfile get\nthe base address of vmalloc area.  Now, vmap_area_list is empty, so export\nVMALLOC_START to vmcoreinfo instead, and remove vmap_area_list.\n\n[urezki@gmail.com: fix a warning in the crash_save_vmcoreinfo_init()]\n  Link: https://lkml.kernel.org/r/20240111192329.449189-1-urezki@gmail.com\nLink: https://lkml.kernel.org/r/20240102184633.748113-6-urezki@gmail.com\nSigned-off-by: Baoquan He <bhe@redhat.com>\nSigned-off-by: Uladzislau Rezki (Sony) <urezki@gmail.com>\nAcked-by: Lorenzo Stoakes <lstoakes@gmail.com>\nCc: Christoph Hellwig <hch@lst.de>\nCc: Dave Chinner <david@fromorbit.com>\nCc: Joel Fernandes (Google) <joel@joelfernandes.org>\nCc: Kazuhito Hagio <k-hagio-ab@nec.com>\nCc: Liam R. Howlett <Liam.Howlett@oracle.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Oleksiy Avramchenko <oleksiy.avramchenko@sony.com>\nCc: Paul E. McKenney <paulmck@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm: vmalloc: remove global vmap_area_root rb-tree",
            "commit_message": "Store allocated objects in a separate nodes.  A va->va_start address is\nconverted into a correct node where it should be placed and resided.  An\naddr_to_node() function is used to do a proper address conversion to\ndetermine a node that contains a VA.\n\nSuch approach balances VAs across nodes as a result an access becomes\nscalable.  Number of nodes in a system depends on number of CPUs.\n\nPlease note:\n\n1. As of now allocated VAs are bound to a node-0. It means the\n   patch does not give any difference comparing with a current\n   behavior;\n\n2. The global vmap_area_lock, vmap_area_root are removed as there\n   is no need in it anymore. The vmap_area_list is still kept and\n   is _empty_. It is exported for a kexec only;\n\n3. The vmallocinfo and vread() have to be reworked to be able to\n   handle multiple nodes.\n\n[urezki@gmail.com: mark vmap_init_free_space() with __init tag]\n  Link: https://lkml.kernel.org/r/20240111132628.299644-1-urezki@gmail.com\n[urezki@gmail.com: fix a wrong value passed to __find_vmap_area()]\n  Link: https://lkml.kernel.org/r/20240111121104.180993-1-urezki@gmail.com\nLink: https://lkml.kernel.org/r/20240102184633.748113-5-urezki@gmail.com\nSigned-off-by: Uladzislau Rezki (Sony) <urezki@gmail.com>\nReviewed-by: Baoquan He <bhe@redhat.com>\nReviewed-by: Christoph Hellwig <hch@lst.de>\nReviewed-by: Lorenzo Stoakes <lstoakes@gmail.com>\nReviewed-by: Anshuman Khandual <anshuman.khandual@arm.com>\nCc: Dave Chinner <david@fromorbit.com>\nCc: Joel Fernandes (Google) <joel@joelfernandes.org>\nCc: Kazuhito Hagio <k-hagio-ab@nec.com>\nCc: Liam R. Howlett <Liam.Howlett@oracle.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Oleksiy Avramchenko <oleksiy.avramchenko@sony.com>\nCc: Paul E. McKenney <paulmck@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm: vmalloc: support multiple nodes in vmallocinfo",
            "commit_message": "Allocated areas are spread among nodes, it implies that the scanning has\nto be performed individually of each node in order to dump all existing\nVAs.\n\nLink: https://lkml.kernel.org/r/20240102184633.748113-10-urezki@gmail.com\nSigned-off-by: Uladzislau Rezki (Sony) <urezki@gmail.com>\nCc: Baoquan He <bhe@redhat.com>\nCc: Christoph Hellwig <hch@lst.de>\nCc: Dave Chinner <david@fromorbit.com>\nCc: Joel Fernandes (Google) <joel@joelfernandes.org>\nCc: Kazuhito Hagio <k-hagio-ab@nec.com>\nCc: Liam R. Howlett <Liam.Howlett@oracle.com>\nCc: Lorenzo Stoakes <lstoakes@gmail.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Oleksiy Avramchenko <oleksiy.avramchenko@sony.com>\nCc: Paul E. McKenney <paulmck@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        }
    ],
    "proper_nouns": [
        {
            "vmap_area": "A 'vmap_area' is a data structure in the Linux kernel used to manage virtual memory mappings. It represents a region of virtual memory that has been allocated and mapped, typically for use by the vmalloc subsystem. Each vmap_area contains metadata about the allocated region, such as its start and end addresses, and is stored in a red-black tree for efficient lookup and management."
        },
        {
            "red-black tree": "A 'red-black tree' is a type of self-balancing binary search tree used in the Linux kernel for efficient data storage and retrieval. In the context of vmalloc, the red-black tree is used to store vmap_area structures, allowing for quick lookups and insertions of virtual memory regions."
        },
        {
            "vmalloc": "The 'vmalloc' subsystem in the Linux kernel is responsible for allocating large contiguous blocks of virtual memory, which may be mapped to non-contiguous physical pages. This is useful for allocating large buffers or data structures that require a contiguous address space but do not need to be physically contiguous."
        },
        {
            "spinlock": "A 'spinlock' is a synchronization primitive used in the Linux kernel to protect shared resources from concurrent access. In the context of vmalloc, spinlocks are used to protect the global data structures that manage virtual memory mappings, ensuring that only one process can modify these structures at a time."
        }
    ],
    "background_knowledge": [
        "The vmalloc subsystem in the Linux kernel is designed to allocate large blocks of virtual memory, which can be mapped to non-contiguous physical pages. This is particularly useful for allocating large buffers or data structures that require a contiguous address space but do not need to be physically contiguous.",
        "The vmap_area structure is used to manage these allocated regions of virtual memory. Each vmap_area is stored in a red-black tree for efficient lookup and management.",
        "The global purge-lock and purge-tree were previously used to manage lazily-freed areas in the vmalloc space. These structures required a global locking primitive, which could lead to contention and reduced scalability, especially on systems with many CPUs."
    ],
    "kernel_changes": [
        "Before the implementation of this feature, the vmalloc subsystem used a global purge-lock and purge-tree to manage lazily-freed areas. This approach required a global locking primitive, which could lead to contention and reduced scalability on multi-CPU systems.",
        "After the implementation, the kernel was modified to store lazily-freed areas in a node-based structure, where each node corresponds to a CPU. This approach eliminates the need for a global locking primitive, making the access to these areas more scalable and reducing contention.",
        "Additionally, the global purge-lock, purge-tree, and purge list were removed, further reducing the overhead associated with managing lazily-freed areas in the vmalloc space.",
        "The introduction of a shrinker mechanism was also added to return cached VAs back to the global vmalloc space when the system enters a low memory mode, helping to manage memory more efficiently."
    ],
    "comprehensive_understanding": "This feature aims to mitigate vmap lock contention by improving the scalability of the vmalloc subsystem. By removing the global purge-lock and purge-tree, and instead storing lazily-freed areas in a node-based structure, the kernel reduces contention and improves scalability, especially on systems with many CPUs. Additionally, the introduction of a shrinker mechanism helps to manage memory more efficiently by returning cached VAs to the global vmalloc space when necessary. Overall, this feature enhances the performance and scalability of the vmalloc subsystem, making it more suitable for modern, multi-CPU systems.",
    "concepts": [
        "vmap_area",
        "red-black tree",
        "vmalloc",
        "spinlock",
        "scalability",
        "contention",
        "memory management",
        "node-based structure",
        "shrinker mechanism"
    ]
}
2024-12-03 22:33:02,346 - extractor - INFO - ---------Processing feature 15/815---------
2024-12-03 22:33:30,533 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个 linux 内核的专家。给你一个 linux 内核特性的描述以及其对应的若干 commit 的标题和提交信息，你的任务是为这个特性写一个多方面的扩写版描述。你的扩写需包含以下几个方面：1. 该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)；2. 为了让开发者理解该特性，需要哪些具体的，与这个特性直接相关的背景知识 (background_knowledge)；3. 该特性被实现前后，linux 内核发生了怎样的变化 (kernel_changes)；4. 你对该特性的综合理解 (comprehensive_understanding)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}]
        },
        "output": {
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
        }
    ],
    "input": {
        "feature_description": "print stacks and their outstanding allocations",
        "commits": [{"commit_subject": "mm,page_owner: filter out stacks by a threshold", "commit_message": "We want to be able to filter out the stacks based on a threshold we can\ncan tune.  By writing to 'count_threshold' file, we can adjust the\nthreshold value.\n\nLink: https://lkml.kernel.org/r/20240215215907.20121-7-osalvador@suse.de\nSigned-off-by: Oscar Salvador <osalvador@suse.de>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\nAcked-by: Andrey Konovalov <andreyknvl@gmail.com>\nCc: Alexander Potapenko <glider@google.com>\nCc: Marco Elver <elver@google.com>\nCc: Michal Hocko <mhocko@suse.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm,page_owner: update Documentation regarding page_owner_stacks", "commit_message": "Update page_owner documentation including the new page_owner_stacks\nfeature to show how it can be used.\n\nLink: https://lkml.kernel.org/r/20240215215907.20121-8-osalvador@suse.de\nSigned-off-by: Oscar Salvador <osalvador@suse.de>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\nReviewed-by: Marco Elver <elver@google.com>\nAcked-by: Andrey Konovalov <andreyknvl@gmail.com>\nCc: Alexander Potapenko <glider@google.com>\nCc: Michal Hocko <mhocko@suse.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "lib/stackdepot: move stack_record struct definition into the header", "commit_message": "In order to move the heavy lifting into page_owner code, this one needs to\nhave access to the stack_record structure, which right now sits in\nlib/stackdepot.c.  Move it to the stackdepot.h header so page_owner can\naccess stack_record's struct fields.\n\nLink: https://lkml.kernel.org/r/20240215215907.20121-3-osalvador@suse.de\nSigned-off-by: Oscar Salvador <osalvador@suse.de>\nReviewed-by: Marco Elver <elver@google.com>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\nAcked-by: Andrey Konovalov <andreyknvl@gmail.com>\nCc: Alexander Potapenko <glider@google.com>\nCc: Michal Hocko <mhocko@suse.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm,page_owner: maintain own list of stack_records structs", "commit_message": "page_owner needs to increment a stack_record refcount when a new\nallocation occurs, and decrement it on a free operation.  In order to do\nthat, we need to have a way to get a stack_record from a handle. \nImplement __stack_depot_get_stack_record() which just does that, and make\nit public so page_owner can use it.\n\nAlso, traversing all stackdepot buckets comes with its own complexity,\nplus we would have to implement a way to mark only those stack_records\nthat were originated from page_owner, as those are the ones we are\ninterested in.  For that reason, page_owner maintains its own list of\nstack_records, because traversing that list is faster than traversing all\nbuckets while keeping at the same time a low complexity.\n\nFor now, add to stack_list only the stack_records of dummy_handle and\nfailure_handle, and set their refcount of 1.\n\nFurther patches will add code to increment or decrement stack_records\ncount on allocation and free operation.\n\nLink: https://lkml.kernel.org/r/20240215215907.20121-4-osalvador@suse.de\nSigned-off-by: Oscar Salvador <osalvador@suse.de>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\nReviewed-by: Marco Elver <elver@google.com>\nAcked-by: Andrey Konovalov <andreyknvl@gmail.com>\nCc: Alexander Potapenko <glider@google.com>\nCc: Michal Hocko <mhocko@suse.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "lib/stackdepot: fix first entry having a 0-handle", "commit_message": "Patch series \"page_owner: print stacks and their outstanding allocations\",\nv10.\n\npage_owner is a great debug functionality tool that lets us know about all\npages that have been allocated/freed and their specific stacktrace.  This\ncomes very handy when debugging memory leaks, since with some scripting we\ncan see the outstanding allocations, which might point to a memory leak.\n\nIn my experience, that is one of the most useful cases, but it can get\nreally tedious to screen through all pages and try to reconstruct the\nstack <-> allocated/freed relationship, becoming most of the time a\ndaunting and slow process when we have tons of allocation/free operations.\n \n\nThis patchset aims to ease that by adding a new functionality into\npage_owner.  This functionality creates a new directory called\n'page_owner_stacks' under 'sys/kernel//debug' with a read-only file called\n'show_stacks', which prints out all the stacks followed by their\noutstanding number of allocations (being that the times the stacktrace has\nallocated but not freed yet).  This gives us a clear and a quick overview\nof stacks <-> allocated/free.\n\nWe take advantage of the new refcount_f field that stack_record struct\ngained, and increment/decrement the stack refcount on every\n__set_page_owner() (alloc operation) and __reset_page_owner (free\noperation) call.\n\nUnfortunately, we cannot use the new stackdepot api STACK_DEPOT_FLAG_GET\nbecause it does not fulfill page_owner needs, meaning we would have to\nspecial case things, at which point makes more sense for page_owner to do\nits own {dec,inc}rementing of the stacks.  E.g: Using\nSTACK_DEPOT_FLAG_PUT, once the refcount reaches 0, such stack gets\nevicted, so page_owner would lose information.\n\nThis patchset also creates a new file called 'set_threshold' within\n'page_owner_stacks' directory, and by writing a value to it, the stacks\nwhich refcount is below such value will be filtered out.\n\nA PoC can be found below:\n\n # cat /sys/kernel/debug/page_owner_stacks/show_stacks > page_owner_full_stacks.txt\n # head -40 page_owner_full_stacks.txt \n  prep_new_page+0xa9/0x120\n  get_page_from_freelist+0x801/0x2210\n  __alloc_pages+0x18b/0x350\n  alloc_pages_mpol+0x91/0x1f0\n  folio_alloc+0x14/0x50\n  filemap_alloc_folio+0xb2/0x100\n  page_cache_ra_unbounded+0x96/0x180\n  filemap_get_pages+0xfd/0x590\n  filemap_read+0xcc/0x330\n  blkdev_read_iter+0xb8/0x150\n  vfs_read+0x285/0x320\n  ksys_read+0xa5/0xe0\n  do_syscall_64+0x80/0x160\n  entry_SYSCALL_64_after_hwframe+0x6e/0x76\n stack_count: 521\n\n\n\n  prep_new_page+0xa9/0x120\n  get_page_from_freelist+0x801/0x2210\n  __alloc_pages+0x18b/0x350\n  alloc_pages_mpol+0x91/0x1f0\n  folio_alloc+0x14/0x50\n  filemap_alloc_folio+0xb2/0x100\n  __filemap_get_folio+0x14a/0x490\n  ext4_write_begin+0xbd/0x4b0 [ext4]\n  generic_perform_write+0xc1/0x1e0\n  ext4_buffered_write_iter+0x68/0xe0 [ext4]\n  ext4_file_write_iter+0x70/0x740 [ext4]\n  vfs_write+0x33d/0x420\n  ksys_write+0xa5/0xe0\n  do_syscall_64+0x80/0x160\n  entry_SYSCALL_64_after_hwframe+0x6e/0x76\n stack_count: 4609\n...\n...\n\n # echo 5000 > /sys/kernel/debug/page_owner_stacks/set_threshold \n # cat /sys/kernel/debug/page_owner_stacks/show_stacks > page_owner_full_stacks_5000.txt\n # head -40 page_owner_full_stacks_5000.txt \n  prep_new_page+0xa9/0x120\n  get_page_from_freelist+0x801/0x2210\n  __alloc_pages+0x18b/0x350\n  alloc_pages_mpol+0x91/0x1f0\n  folio_alloc+0x14/0x50\n  filemap_alloc_folio+0xb2/0x100\n  __filemap_get_folio+0x14a/0x490\n  ext4_write_begin+0xbd/0x4b0 [ext4]\n  generic_perform_write+0xc1/0x1e0\n  ext4_buffered_write_iter+0x68/0xe0 [ext4]\n  ext4_file_write_iter+0x70/0x740 [ext4]\n  vfs_write+0x33d/0x420\n  ksys_pwrite64+0x75/0x90\n  do_syscall_64+0x80/0x160\n  entry_SYSCALL_64_after_hwframe+0x6e/0x76\n stack_count: 6781\n\n\n\n  prep_new_page+0xa9/0x120\n  get_page_from_freelist+0x801/0x2210\n  __alloc_pages+0x18b/0x350\n  pcpu_populate_chunk+0xec/0x350\n  pcpu_balance_workfn+0x2d1/0x4a0\n  process_scheduled_works+0x84/0x380\n  worker_thread+0x12a/0x2a0\n  kthread+0xe3/0x110\n  ret_from_fork+0x30/0x50\n  ret_from_fork_asm+0x1b/0x30\n stack_count: 8641\n\n\nThis patch (of 7):\n\nThe very first entry of stack_record gets a handle of 0, but this is wrong\nbecause stackdepot treats a 0-handle as a non-valid one.  E.g: See the\ncheck in stack_depot_fetch()\n\nFix this by adding and offset of 1.\n\nThis bug has been lurking since the very beginning of stackdepot, but no\none really cared as it seems.  Because of that I am not adding a Fixes\ntag.\n\nLink: https://lkml.kernel.org/r/20240215215907.20121-1-osalvador@suse.de\nLink: https://lkml.kernel.org/r/20240215215907.20121-2-osalvador@suse.de\nCo-developed-by: Marco Elver <elver@google.com>\nSigned-off-by: Marco Elver <elver@google.com>\nSigned-off-by: Oscar Salvador <osalvador@suse.de>\nAcked-by: Vlastimil Babka <vbabka@suse.cz>\nAcked-by: Andrey Konovalov <andreyknvl@gmail.com>\nCc: Alexander Potapenko <glider@google.com>\nCc: Michal Hocko <mhocko@suse.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm,page_owner: display all stacks and their count", "commit_message": "This patch adds a new directory called 'page_owner_stacks' under\n/sys/kernel/debug/, with a file called 'show_stacks' in it.  Reading from\nthat file will show all stacks that were added by page_owner followed by\ntheir counting, giving us a clear overview of stack <-> count\nrelationship.\n\nE.g:\n\n  prep_new_page+0xa9/0x120\n  get_page_from_freelist+0x801/0x2210\n  __alloc_pages+0x18b/0x350\n  alloc_pages_mpol+0x91/0x1f0\n  folio_alloc+0x14/0x50\n  filemap_alloc_folio+0xb2/0x100\n  __filemap_get_folio+0x14a/0x490\n  ext4_write_begin+0xbd/0x4b0 [ext4]\n  generic_perform_write+0xc1/0x1e0\n  ext4_buffered_write_iter+0x68/0xe0 [ext4]\n  ext4_file_write_iter+0x70/0x740 [ext4]\n  vfs_write+0x33d/0x420\n  ksys_write+0xa5/0xe0\n  do_syscall_64+0x80/0x160\n  entry_SYSCALL_64_after_hwframe+0x6e/0x76\n stack_count: 4578\n\nThe seq stack_{start,next} functions will iterate through the list\nstack_list in order to print all stacks.\n\nLink: https://lkml.kernel.org/r/20240215215907.20121-6-osalvador@suse.de\nSigned-off-by: Oscar Salvador <osalvador@suse.de>\nAcked-by: Marco Elver <elver@google.com>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\nAcked-by: Andrey Konovalov <andreyknvl@gmail.com>\nCc: Alexander Potapenko <glider@google.com>\nCc: Michal Hocko <mhocko@suse.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm,page_owner: implement the tracking of the stacks count", "commit_message": "Implement {inc,dec}_stack_record_count() which increments or decrements on\nrespective allocation and free operations, via __reset_page_owner() (free\noperation) and __set_page_owner() (alloc operation).\n\nNewly allocated stack_record structs will be added to the list stack_list\nvia add_stack_record_to_list().  Modifications on the list are protected\nvia a spinlock with irqs disabled, since this code can also be reached\nfrom IRQ context.\n\nLink: https://lkml.kernel.org/r/20240215215907.20121-5-osalvador@suse.de\nSigned-off-by: Oscar Salvador <osalvador@suse.de>\nReviewed-by: Marco Elver <elver@google.com>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\nAcked-by: Andrey Konovalov <andreyknvl@gmail.com>\nCc: Alexander Potapenko <glider@google.com>\nCc: Michal Hocko <mhocko@suse.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}]
    }
}

2024-12-03 22:34:04,689 - extractor - DEBUG - deepseek response:
ChatCompletion(id='df85855d-726e-48df-b6b6-9b14c34c4b81', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "proper_nouns": [\n        {\n            "page_owner": "The \'page_owner\' is a debugging feature in the Linux kernel that tracks the allocation and deallocation of memory pages. It records metadata about each page, including the stack trace of the code that allocated or freed the page. This information is useful for diagnosing memory leaks and understanding memory usage patterns."\n        },\n        {\n            "stack_record": "A \'stack_record\' is a data structure used in the Linux kernel to store information about a stack trace. Each stack_record contains details about a specific stack trace, including the sequence of function calls that led to the allocation or deallocation of a memory page."\n        },\n        {\n            "stackdepot": "The \'stackdepot\' is a subsystem in the Linux kernel that manages and stores stack traces. It provides an efficient way to store and retrieve stack traces, which are used for debugging purposes, such as tracking memory allocations and deallocations."\n        }\n    ],\n    "background_knowledge": [\n        "Understanding how memory allocation and deallocation are tracked in the Linux kernel is crucial. The \'page_owner\' feature records metadata about each memory page, including the stack trace of the code that allocated or freed the page. This information is stored in \'stack_records\' and managed by the \'stackdepot\' subsystem.",\n        "The ability to filter and display stack traces based on a threshold is important for analyzing memory usage patterns. This feature allows developers to focus on stack traces that have a significant number of outstanding allocations, which can help identify memory leaks or inefficient memory usage."\n    ],\n    "kernel_changes": [\n        "Before this feature, the Linux kernel did not have a mechanism to filter and display stack traces based on the number of outstanding allocations. Developers had to manually analyze all stack traces, which was time-consuming and inefficient.",\n        "With the introduction of this feature, the kernel now allows developers to filter stack traces based on a configurable threshold. This makes it easier to identify stack traces with a high number of outstanding allocations, which can be indicative of memory leaks or inefficient memory usage.",\n        "Additionally, the kernel now maintains its own list of \'stack_records\' to track the number of allocations and deallocations for each stack trace. This list is used to display the count of outstanding allocations for each stack trace, providing a clear overview of memory usage patterns."\n    ],\n    "comprehensive_understanding": "This feature enhances the \'page_owner\' debugging tool by allowing developers to filter and display stack traces based on the number of outstanding allocations. By maintaining a list of \'stack_records\' and providing a mechanism to filter stack traces based on a configurable threshold, the kernel makes it easier to identify memory leaks and inefficient memory usage. This improvement streamlines the debugging process and provides valuable insights into memory allocation patterns, ultimately helping developers optimize memory management in the Linux kernel."\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733236410, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=627, prompt_tokens=5918, total_tokens=6545, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=1472, prompt_cache_miss_tokens=4446))
2024-12-03 22:34:04,689 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个命名实体抽取 (NER) 任务的专家，同时也是 linux 内核的专家。给你一个 linux 内核特性的描述 (feature_description)、对应的若干 commit 的标题和提交信息 (commits)、该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)、为了让开发者理解该特性所需的与这个特性直接相关的背景知识 (background_knowledge)、该特性被实现前后，linux 内核发生的变化 (kernel_changes)、对该特性的综合理解 (comprehensive_understanding)。你的任务是：根据这些信息提取出在 linux 内核中涉及到的和这个特性相关的概念实体 (concepts)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}],
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": [
                "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
            ]
        },
        "output": {
            "concepts": ["Copy-on-Write (CoW)", "mTHP", "folio", "memory reclamation", "memory availability", "anti-fragmentation"]
        }
    ],
    "input": {
        "feature_description": "print stacks and their outstanding allocations",
        "commits": [{"commit_subject": "mm,page_owner: filter out stacks by a threshold", "commit_message": "We want to be able to filter out the stacks based on a threshold we can\ncan tune.  By writing to 'count_threshold' file, we can adjust the\nthreshold value.\n\nLink: https://lkml.kernel.org/r/20240215215907.20121-7-osalvador@suse.de\nSigned-off-by: Oscar Salvador <osalvador@suse.de>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\nAcked-by: Andrey Konovalov <andreyknvl@gmail.com>\nCc: Alexander Potapenko <glider@google.com>\nCc: Marco Elver <elver@google.com>\nCc: Michal Hocko <mhocko@suse.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm,page_owner: update Documentation regarding page_owner_stacks", "commit_message": "Update page_owner documentation including the new page_owner_stacks\nfeature to show how it can be used.\n\nLink: https://lkml.kernel.org/r/20240215215907.20121-8-osalvador@suse.de\nSigned-off-by: Oscar Salvador <osalvador@suse.de>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\nReviewed-by: Marco Elver <elver@google.com>\nAcked-by: Andrey Konovalov <andreyknvl@gmail.com>\nCc: Alexander Potapenko <glider@google.com>\nCc: Michal Hocko <mhocko@suse.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "lib/stackdepot: move stack_record struct definition into the header", "commit_message": "In order to move the heavy lifting into page_owner code, this one needs to\nhave access to the stack_record structure, which right now sits in\nlib/stackdepot.c.  Move it to the stackdepot.h header so page_owner can\naccess stack_record's struct fields.\n\nLink: https://lkml.kernel.org/r/20240215215907.20121-3-osalvador@suse.de\nSigned-off-by: Oscar Salvador <osalvador@suse.de>\nReviewed-by: Marco Elver <elver@google.com>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\nAcked-by: Andrey Konovalov <andreyknvl@gmail.com>\nCc: Alexander Potapenko <glider@google.com>\nCc: Michal Hocko <mhocko@suse.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm,page_owner: maintain own list of stack_records structs", "commit_message": "page_owner needs to increment a stack_record refcount when a new\nallocation occurs, and decrement it on a free operation.  In order to do\nthat, we need to have a way to get a stack_record from a handle. \nImplement __stack_depot_get_stack_record() which just does that, and make\nit public so page_owner can use it.\n\nAlso, traversing all stackdepot buckets comes with its own complexity,\nplus we would have to implement a way to mark only those stack_records\nthat were originated from page_owner, as those are the ones we are\ninterested in.  For that reason, page_owner maintains its own list of\nstack_records, because traversing that list is faster than traversing all\nbuckets while keeping at the same time a low complexity.\n\nFor now, add to stack_list only the stack_records of dummy_handle and\nfailure_handle, and set their refcount of 1.\n\nFurther patches will add code to increment or decrement stack_records\ncount on allocation and free operation.\n\nLink: https://lkml.kernel.org/r/20240215215907.20121-4-osalvador@suse.de\nSigned-off-by: Oscar Salvador <osalvador@suse.de>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\nReviewed-by: Marco Elver <elver@google.com>\nAcked-by: Andrey Konovalov <andreyknvl@gmail.com>\nCc: Alexander Potapenko <glider@google.com>\nCc: Michal Hocko <mhocko@suse.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "lib/stackdepot: fix first entry having a 0-handle", "commit_message": "Patch series \"page_owner: print stacks and their outstanding allocations\",\nv10.\n\npage_owner is a great debug functionality tool that lets us know about all\npages that have been allocated/freed and their specific stacktrace.  This\ncomes very handy when debugging memory leaks, since with some scripting we\ncan see the outstanding allocations, which might point to a memory leak.\n\nIn my experience, that is one of the most useful cases, but it can get\nreally tedious to screen through all pages and try to reconstruct the\nstack <-> allocated/freed relationship, becoming most of the time a\ndaunting and slow process when we have tons of allocation/free operations.\n \n\nThis patchset aims to ease that by adding a new functionality into\npage_owner.  This functionality creates a new directory called\n'page_owner_stacks' under 'sys/kernel//debug' with a read-only file called\n'show_stacks', which prints out all the stacks followed by their\noutstanding number of allocations (being that the times the stacktrace has\nallocated but not freed yet).  This gives us a clear and a quick overview\nof stacks <-> allocated/free.\n\nWe take advantage of the new refcount_f field that stack_record struct\ngained, and increment/decrement the stack refcount on every\n__set_page_owner() (alloc operation) and __reset_page_owner (free\noperation) call.\n\nUnfortunately, we cannot use the new stackdepot api STACK_DEPOT_FLAG_GET\nbecause it does not fulfill page_owner needs, meaning we would have to\nspecial case things, at which point makes more sense for page_owner to do\nits own {dec,inc}rementing of the stacks.  E.g: Using\nSTACK_DEPOT_FLAG_PUT, once the refcount reaches 0, such stack gets\nevicted, so page_owner would lose information.\n\nThis patchset also creates a new file called 'set_threshold' within\n'page_owner_stacks' directory, and by writing a value to it, the stacks\nwhich refcount is below such value will be filtered out.\n\nA PoC can be found below:\n\n # cat /sys/kernel/debug/page_owner_stacks/show_stacks > page_owner_full_stacks.txt\n # head -40 page_owner_full_stacks.txt \n  prep_new_page+0xa9/0x120\n  get_page_from_freelist+0x801/0x2210\n  __alloc_pages+0x18b/0x350\n  alloc_pages_mpol+0x91/0x1f0\n  folio_alloc+0x14/0x50\n  filemap_alloc_folio+0xb2/0x100\n  page_cache_ra_unbounded+0x96/0x180\n  filemap_get_pages+0xfd/0x590\n  filemap_read+0xcc/0x330\n  blkdev_read_iter+0xb8/0x150\n  vfs_read+0x285/0x320\n  ksys_read+0xa5/0xe0\n  do_syscall_64+0x80/0x160\n  entry_SYSCALL_64_after_hwframe+0x6e/0x76\n stack_count: 521\n\n\n\n  prep_new_page+0xa9/0x120\n  get_page_from_freelist+0x801/0x2210\n  __alloc_pages+0x18b/0x350\n  alloc_pages_mpol+0x91/0x1f0\n  folio_alloc+0x14/0x50\n  filemap_alloc_folio+0xb2/0x100\n  __filemap_get_folio+0x14a/0x490\n  ext4_write_begin+0xbd/0x4b0 [ext4]\n  generic_perform_write+0xc1/0x1e0\n  ext4_buffered_write_iter+0x68/0xe0 [ext4]\n  ext4_file_write_iter+0x70/0x740 [ext4]\n  vfs_write+0x33d/0x420\n  ksys_write+0xa5/0xe0\n  do_syscall_64+0x80/0x160\n  entry_SYSCALL_64_after_hwframe+0x6e/0x76\n stack_count: 4609\n...\n...\n\n # echo 5000 > /sys/kernel/debug/page_owner_stacks/set_threshold \n # cat /sys/kernel/debug/page_owner_stacks/show_stacks > page_owner_full_stacks_5000.txt\n # head -40 page_owner_full_stacks_5000.txt \n  prep_new_page+0xa9/0x120\n  get_page_from_freelist+0x801/0x2210\n  __alloc_pages+0x18b/0x350\n  alloc_pages_mpol+0x91/0x1f0\n  folio_alloc+0x14/0x50\n  filemap_alloc_folio+0xb2/0x100\n  __filemap_get_folio+0x14a/0x490\n  ext4_write_begin+0xbd/0x4b0 [ext4]\n  generic_perform_write+0xc1/0x1e0\n  ext4_buffered_write_iter+0x68/0xe0 [ext4]\n  ext4_file_write_iter+0x70/0x740 [ext4]\n  vfs_write+0x33d/0x420\n  ksys_pwrite64+0x75/0x90\n  do_syscall_64+0x80/0x160\n  entry_SYSCALL_64_after_hwframe+0x6e/0x76\n stack_count: 6781\n\n\n\n  prep_new_page+0xa9/0x120\n  get_page_from_freelist+0x801/0x2210\n  __alloc_pages+0x18b/0x350\n  pcpu_populate_chunk+0xec/0x350\n  pcpu_balance_workfn+0x2d1/0x4a0\n  process_scheduled_works+0x84/0x380\n  worker_thread+0x12a/0x2a0\n  kthread+0xe3/0x110\n  ret_from_fork+0x30/0x50\n  ret_from_fork_asm+0x1b/0x30\n stack_count: 8641\n\n\nThis patch (of 7):\n\nThe very first entry of stack_record gets a handle of 0, but this is wrong\nbecause stackdepot treats a 0-handle as a non-valid one.  E.g: See the\ncheck in stack_depot_fetch()\n\nFix this by adding and offset of 1.\n\nThis bug has been lurking since the very beginning of stackdepot, but no\none really cared as it seems.  Because of that I am not adding a Fixes\ntag.\n\nLink: https://lkml.kernel.org/r/20240215215907.20121-1-osalvador@suse.de\nLink: https://lkml.kernel.org/r/20240215215907.20121-2-osalvador@suse.de\nCo-developed-by: Marco Elver <elver@google.com>\nSigned-off-by: Marco Elver <elver@google.com>\nSigned-off-by: Oscar Salvador <osalvador@suse.de>\nAcked-by: Vlastimil Babka <vbabka@suse.cz>\nAcked-by: Andrey Konovalov <andreyknvl@gmail.com>\nCc: Alexander Potapenko <glider@google.com>\nCc: Michal Hocko <mhocko@suse.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm,page_owner: display all stacks and their count", "commit_message": "This patch adds a new directory called 'page_owner_stacks' under\n/sys/kernel/debug/, with a file called 'show_stacks' in it.  Reading from\nthat file will show all stacks that were added by page_owner followed by\ntheir counting, giving us a clear overview of stack <-> count\nrelationship.\n\nE.g:\n\n  prep_new_page+0xa9/0x120\n  get_page_from_freelist+0x801/0x2210\n  __alloc_pages+0x18b/0x350\n  alloc_pages_mpol+0x91/0x1f0\n  folio_alloc+0x14/0x50\n  filemap_alloc_folio+0xb2/0x100\n  __filemap_get_folio+0x14a/0x490\n  ext4_write_begin+0xbd/0x4b0 [ext4]\n  generic_perform_write+0xc1/0x1e0\n  ext4_buffered_write_iter+0x68/0xe0 [ext4]\n  ext4_file_write_iter+0x70/0x740 [ext4]\n  vfs_write+0x33d/0x420\n  ksys_write+0xa5/0xe0\n  do_syscall_64+0x80/0x160\n  entry_SYSCALL_64_after_hwframe+0x6e/0x76\n stack_count: 4578\n\nThe seq stack_{start,next} functions will iterate through the list\nstack_list in order to print all stacks.\n\nLink: https://lkml.kernel.org/r/20240215215907.20121-6-osalvador@suse.de\nSigned-off-by: Oscar Salvador <osalvador@suse.de>\nAcked-by: Marco Elver <elver@google.com>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\nAcked-by: Andrey Konovalov <andreyknvl@gmail.com>\nCc: Alexander Potapenko <glider@google.com>\nCc: Michal Hocko <mhocko@suse.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm,page_owner: implement the tracking of the stacks count", "commit_message": "Implement {inc,dec}_stack_record_count() which increments or decrements on\nrespective allocation and free operations, via __reset_page_owner() (free\noperation) and __set_page_owner() (alloc operation).\n\nNewly allocated stack_record structs will be added to the list stack_list\nvia add_stack_record_to_list().  Modifications on the list are protected\nvia a spinlock with irqs disabled, since this code can also be reached\nfrom IRQ context.\n\nLink: https://lkml.kernel.org/r/20240215215907.20121-5-osalvador@suse.de\nSigned-off-by: Oscar Salvador <osalvador@suse.de>\nReviewed-by: Marco Elver <elver@google.com>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\nAcked-by: Andrey Konovalov <andreyknvl@gmail.com>\nCc: Alexander Potapenko <glider@google.com>\nCc: Michal Hocko <mhocko@suse.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}],
        "proper_nouns": [{"page_owner": "The 'page_owner' is a debugging feature in the Linux kernel that tracks the allocation and deallocation of memory pages. It records metadata about each page, including the stack trace of the code that allocated or freed the page. This information is useful for diagnosing memory leaks and understanding memory usage patterns."}, {"stack_record": "A 'stack_record' is a data structure used in the Linux kernel to store information about a stack trace. Each stack_record contains details about a specific stack trace, including the sequence of function calls that led to the allocation or deallocation of a memory page."}, {"stackdepot": "The 'stackdepot' is a subsystem in the Linux kernel that manages and stores stack traces. It provides an efficient way to store and retrieve stack traces, which are used for debugging purposes, such as tracking memory allocations and deallocations."}],
        "background_knowledge": ["Understanding how memory allocation and deallocation are tracked in the Linux kernel is crucial. The 'page_owner' feature records metadata about each memory page, including the stack trace of the code that allocated or freed the page. This information is stored in 'stack_records' and managed by the 'stackdepot' subsystem.", "The ability to filter and display stack traces based on a threshold is important for analyzing memory usage patterns. This feature allows developers to focus on stack traces that have a significant number of outstanding allocations, which can help identify memory leaks or inefficient memory usage."],
        "kernel_changes": ["Before this feature, the Linux kernel did not have a mechanism to filter and display stack traces based on the number of outstanding allocations. Developers had to manually analyze all stack traces, which was time-consuming and inefficient.", "With the introduction of this feature, the kernel now allows developers to filter stack traces based on a configurable threshold. This makes it easier to identify stack traces with a high number of outstanding allocations, which can be indicative of memory leaks or inefficient memory usage.", "Additionally, the kernel now maintains its own list of 'stack_records' to track the number of allocations and deallocations for each stack trace. This list is used to display the count of outstanding allocations for each stack trace, providing a clear overview of memory usage patterns."],
        "comprehensive_understanding": "This feature enhances the 'page_owner' debugging tool by allowing developers to filter and display stack traces based on the number of outstanding allocations. By maintaining a list of 'stack_records' and providing a mechanism to filter stack traces based on a configurable threshold, the kernel makes it easier to identify memory leaks and inefficient memory usage. This improvement streamlines the debugging process and provides valuable insights into memory allocation patterns, ultimately helping developers optimize memory management in the Linux kernel."
    }
}

2024-12-03 22:34:08,289 - extractor - DEBUG - deepseek response:
ChatCompletion(id='72cd9abb-eb76-4bb3-a8d5-90a4bed044dc', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "concepts": ["page_owner", "stack_record", "stackdepot", "memory allocation", "memory deallocation", "stack trace", "memory leaks", "outstanding allocations", "threshold filtering"]\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733236444, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=55, prompt_tokens=6553, total_tokens=6608, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=1536, prompt_cache_miss_tokens=5017))
2024-12-03 22:34:08,289 - extractor - DEBUG - feature_extracted of feature_id=33225:
{
    "feature_id": 33225,
    "feature_description": "print stacks and their outstanding allocations",
    "version": "6.9",
    "commits": [
        {
            "commit_subject": "mm,page_owner: filter out stacks by a threshold",
            "commit_message": "We want to be able to filter out the stacks based on a threshold we can\ncan tune.  By writing to 'count_threshold' file, we can adjust the\nthreshold value.\n\nLink: https://lkml.kernel.org/r/20240215215907.20121-7-osalvador@suse.de\nSigned-off-by: Oscar Salvador <osalvador@suse.de>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\nAcked-by: Andrey Konovalov <andreyknvl@gmail.com>\nCc: Alexander Potapenko <glider@google.com>\nCc: Marco Elver <elver@google.com>\nCc: Michal Hocko <mhocko@suse.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm,page_owner: update Documentation regarding page_owner_stacks",
            "commit_message": "Update page_owner documentation including the new page_owner_stacks\nfeature to show how it can be used.\n\nLink: https://lkml.kernel.org/r/20240215215907.20121-8-osalvador@suse.de\nSigned-off-by: Oscar Salvador <osalvador@suse.de>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\nReviewed-by: Marco Elver <elver@google.com>\nAcked-by: Andrey Konovalov <andreyknvl@gmail.com>\nCc: Alexander Potapenko <glider@google.com>\nCc: Michal Hocko <mhocko@suse.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "lib/stackdepot: move stack_record struct definition into the header",
            "commit_message": "In order to move the heavy lifting into page_owner code, this one needs to\nhave access to the stack_record structure, which right now sits in\nlib/stackdepot.c.  Move it to the stackdepot.h header so page_owner can\naccess stack_record's struct fields.\n\nLink: https://lkml.kernel.org/r/20240215215907.20121-3-osalvador@suse.de\nSigned-off-by: Oscar Salvador <osalvador@suse.de>\nReviewed-by: Marco Elver <elver@google.com>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\nAcked-by: Andrey Konovalov <andreyknvl@gmail.com>\nCc: Alexander Potapenko <glider@google.com>\nCc: Michal Hocko <mhocko@suse.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm,page_owner: maintain own list of stack_records structs",
            "commit_message": "page_owner needs to increment a stack_record refcount when a new\nallocation occurs, and decrement it on a free operation.  In order to do\nthat, we need to have a way to get a stack_record from a handle. \nImplement __stack_depot_get_stack_record() which just does that, and make\nit public so page_owner can use it.\n\nAlso, traversing all stackdepot buckets comes with its own complexity,\nplus we would have to implement a way to mark only those stack_records\nthat were originated from page_owner, as those are the ones we are\ninterested in.  For that reason, page_owner maintains its own list of\nstack_records, because traversing that list is faster than traversing all\nbuckets while keeping at the same time a low complexity.\n\nFor now, add to stack_list only the stack_records of dummy_handle and\nfailure_handle, and set their refcount of 1.\n\nFurther patches will add code to increment or decrement stack_records\ncount on allocation and free operation.\n\nLink: https://lkml.kernel.org/r/20240215215907.20121-4-osalvador@suse.de\nSigned-off-by: Oscar Salvador <osalvador@suse.de>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\nReviewed-by: Marco Elver <elver@google.com>\nAcked-by: Andrey Konovalov <andreyknvl@gmail.com>\nCc: Alexander Potapenko <glider@google.com>\nCc: Michal Hocko <mhocko@suse.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "lib/stackdepot: fix first entry having a 0-handle",
            "commit_message": "Patch series \"page_owner: print stacks and their outstanding allocations\",\nv10.\n\npage_owner is a great debug functionality tool that lets us know about all\npages that have been allocated/freed and their specific stacktrace.  This\ncomes very handy when debugging memory leaks, since with some scripting we\ncan see the outstanding allocations, which might point to a memory leak.\n\nIn my experience, that is one of the most useful cases, but it can get\nreally tedious to screen through all pages and try to reconstruct the\nstack <-> allocated/freed relationship, becoming most of the time a\ndaunting and slow process when we have tons of allocation/free operations.\n \n\nThis patchset aims to ease that by adding a new functionality into\npage_owner.  This functionality creates a new directory called\n'page_owner_stacks' under 'sys/kernel//debug' with a read-only file called\n'show_stacks', which prints out all the stacks followed by their\noutstanding number of allocations (being that the times the stacktrace has\nallocated but not freed yet).  This gives us a clear and a quick overview\nof stacks <-> allocated/free.\n\nWe take advantage of the new refcount_f field that stack_record struct\ngained, and increment/decrement the stack refcount on every\n__set_page_owner() (alloc operation) and __reset_page_owner (free\noperation) call.\n\nUnfortunately, we cannot use the new stackdepot api STACK_DEPOT_FLAG_GET\nbecause it does not fulfill page_owner needs, meaning we would have to\nspecial case things, at which point makes more sense for page_owner to do\nits own {dec,inc}rementing of the stacks.  E.g: Using\nSTACK_DEPOT_FLAG_PUT, once the refcount reaches 0, such stack gets\nevicted, so page_owner would lose information.\n\nThis patchset also creates a new file called 'set_threshold' within\n'page_owner_stacks' directory, and by writing a value to it, the stacks\nwhich refcount is below such value will be filtered out.\n\nA PoC can be found below:\n\n # cat /sys/kernel/debug/page_owner_stacks/show_stacks > page_owner_full_stacks.txt\n # head -40 page_owner_full_stacks.txt \n  prep_new_page+0xa9/0x120\n  get_page_from_freelist+0x801/0x2210\n  __alloc_pages+0x18b/0x350\n  alloc_pages_mpol+0x91/0x1f0\n  folio_alloc+0x14/0x50\n  filemap_alloc_folio+0xb2/0x100\n  page_cache_ra_unbounded+0x96/0x180\n  filemap_get_pages+0xfd/0x590\n  filemap_read+0xcc/0x330\n  blkdev_read_iter+0xb8/0x150\n  vfs_read+0x285/0x320\n  ksys_read+0xa5/0xe0\n  do_syscall_64+0x80/0x160\n  entry_SYSCALL_64_after_hwframe+0x6e/0x76\n stack_count: 521\n\n\n\n  prep_new_page+0xa9/0x120\n  get_page_from_freelist+0x801/0x2210\n  __alloc_pages+0x18b/0x350\n  alloc_pages_mpol+0x91/0x1f0\n  folio_alloc+0x14/0x50\n  filemap_alloc_folio+0xb2/0x100\n  __filemap_get_folio+0x14a/0x490\n  ext4_write_begin+0xbd/0x4b0 [ext4]\n  generic_perform_write+0xc1/0x1e0\n  ext4_buffered_write_iter+0x68/0xe0 [ext4]\n  ext4_file_write_iter+0x70/0x740 [ext4]\n  vfs_write+0x33d/0x420\n  ksys_write+0xa5/0xe0\n  do_syscall_64+0x80/0x160\n  entry_SYSCALL_64_after_hwframe+0x6e/0x76\n stack_count: 4609\n...\n...\n\n # echo 5000 > /sys/kernel/debug/page_owner_stacks/set_threshold \n # cat /sys/kernel/debug/page_owner_stacks/show_stacks > page_owner_full_stacks_5000.txt\n # head -40 page_owner_full_stacks_5000.txt \n  prep_new_page+0xa9/0x120\n  get_page_from_freelist+0x801/0x2210\n  __alloc_pages+0x18b/0x350\n  alloc_pages_mpol+0x91/0x1f0\n  folio_alloc+0x14/0x50\n  filemap_alloc_folio+0xb2/0x100\n  __filemap_get_folio+0x14a/0x490\n  ext4_write_begin+0xbd/0x4b0 [ext4]\n  generic_perform_write+0xc1/0x1e0\n  ext4_buffered_write_iter+0x68/0xe0 [ext4]\n  ext4_file_write_iter+0x70/0x740 [ext4]\n  vfs_write+0x33d/0x420\n  ksys_pwrite64+0x75/0x90\n  do_syscall_64+0x80/0x160\n  entry_SYSCALL_64_after_hwframe+0x6e/0x76\n stack_count: 6781\n\n\n\n  prep_new_page+0xa9/0x120\n  get_page_from_freelist+0x801/0x2210\n  __alloc_pages+0x18b/0x350\n  pcpu_populate_chunk+0xec/0x350\n  pcpu_balance_workfn+0x2d1/0x4a0\n  process_scheduled_works+0x84/0x380\n  worker_thread+0x12a/0x2a0\n  kthread+0xe3/0x110\n  ret_from_fork+0x30/0x50\n  ret_from_fork_asm+0x1b/0x30\n stack_count: 8641\n\n\nThis patch (of 7):\n\nThe very first entry of stack_record gets a handle of 0, but this is wrong\nbecause stackdepot treats a 0-handle as a non-valid one.  E.g: See the\ncheck in stack_depot_fetch()\n\nFix this by adding and offset of 1.\n\nThis bug has been lurking since the very beginning of stackdepot, but no\none really cared as it seems.  Because of that I am not adding a Fixes\ntag.\n\nLink: https://lkml.kernel.org/r/20240215215907.20121-1-osalvador@suse.de\nLink: https://lkml.kernel.org/r/20240215215907.20121-2-osalvador@suse.de\nCo-developed-by: Marco Elver <elver@google.com>\nSigned-off-by: Marco Elver <elver@google.com>\nSigned-off-by: Oscar Salvador <osalvador@suse.de>\nAcked-by: Vlastimil Babka <vbabka@suse.cz>\nAcked-by: Andrey Konovalov <andreyknvl@gmail.com>\nCc: Alexander Potapenko <glider@google.com>\nCc: Michal Hocko <mhocko@suse.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm,page_owner: display all stacks and their count",
            "commit_message": "This patch adds a new directory called 'page_owner_stacks' under\n/sys/kernel/debug/, with a file called 'show_stacks' in it.  Reading from\nthat file will show all stacks that were added by page_owner followed by\ntheir counting, giving us a clear overview of stack <-> count\nrelationship.\n\nE.g:\n\n  prep_new_page+0xa9/0x120\n  get_page_from_freelist+0x801/0x2210\n  __alloc_pages+0x18b/0x350\n  alloc_pages_mpol+0x91/0x1f0\n  folio_alloc+0x14/0x50\n  filemap_alloc_folio+0xb2/0x100\n  __filemap_get_folio+0x14a/0x490\n  ext4_write_begin+0xbd/0x4b0 [ext4]\n  generic_perform_write+0xc1/0x1e0\n  ext4_buffered_write_iter+0x68/0xe0 [ext4]\n  ext4_file_write_iter+0x70/0x740 [ext4]\n  vfs_write+0x33d/0x420\n  ksys_write+0xa5/0xe0\n  do_syscall_64+0x80/0x160\n  entry_SYSCALL_64_after_hwframe+0x6e/0x76\n stack_count: 4578\n\nThe seq stack_{start,next} functions will iterate through the list\nstack_list in order to print all stacks.\n\nLink: https://lkml.kernel.org/r/20240215215907.20121-6-osalvador@suse.de\nSigned-off-by: Oscar Salvador <osalvador@suse.de>\nAcked-by: Marco Elver <elver@google.com>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\nAcked-by: Andrey Konovalov <andreyknvl@gmail.com>\nCc: Alexander Potapenko <glider@google.com>\nCc: Michal Hocko <mhocko@suse.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm,page_owner: implement the tracking of the stacks count",
            "commit_message": "Implement {inc,dec}_stack_record_count() which increments or decrements on\nrespective allocation and free operations, via __reset_page_owner() (free\noperation) and __set_page_owner() (alloc operation).\n\nNewly allocated stack_record structs will be added to the list stack_list\nvia add_stack_record_to_list().  Modifications on the list are protected\nvia a spinlock with irqs disabled, since this code can also be reached\nfrom IRQ context.\n\nLink: https://lkml.kernel.org/r/20240215215907.20121-5-osalvador@suse.de\nSigned-off-by: Oscar Salvador <osalvador@suse.de>\nReviewed-by: Marco Elver <elver@google.com>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\nAcked-by: Andrey Konovalov <andreyknvl@gmail.com>\nCc: Alexander Potapenko <glider@google.com>\nCc: Michal Hocko <mhocko@suse.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        }
    ],
    "proper_nouns": [
        {
            "page_owner": "The 'page_owner' is a debugging feature in the Linux kernel that tracks the allocation and deallocation of memory pages. It records metadata about each page, including the stack trace of the code that allocated or freed the page. This information is useful for diagnosing memory leaks and understanding memory usage patterns."
        },
        {
            "stack_record": "A 'stack_record' is a data structure used in the Linux kernel to store information about a stack trace. Each stack_record contains details about a specific stack trace, including the sequence of function calls that led to the allocation or deallocation of a memory page."
        },
        {
            "stackdepot": "The 'stackdepot' is a subsystem in the Linux kernel that manages and stores stack traces. It provides an efficient way to store and retrieve stack traces, which are used for debugging purposes, such as tracking memory allocations and deallocations."
        }
    ],
    "background_knowledge": [
        "Understanding how memory allocation and deallocation are tracked in the Linux kernel is crucial. The 'page_owner' feature records metadata about each memory page, including the stack trace of the code that allocated or freed the page. This information is stored in 'stack_records' and managed by the 'stackdepot' subsystem.",
        "The ability to filter and display stack traces based on a threshold is important for analyzing memory usage patterns. This feature allows developers to focus on stack traces that have a significant number of outstanding allocations, which can help identify memory leaks or inefficient memory usage."
    ],
    "kernel_changes": [
        "Before this feature, the Linux kernel did not have a mechanism to filter and display stack traces based on the number of outstanding allocations. Developers had to manually analyze all stack traces, which was time-consuming and inefficient.",
        "With the introduction of this feature, the kernel now allows developers to filter stack traces based on a configurable threshold. This makes it easier to identify stack traces with a high number of outstanding allocations, which can be indicative of memory leaks or inefficient memory usage.",
        "Additionally, the kernel now maintains its own list of 'stack_records' to track the number of allocations and deallocations for each stack trace. This list is used to display the count of outstanding allocations for each stack trace, providing a clear overview of memory usage patterns."
    ],
    "comprehensive_understanding": "This feature enhances the 'page_owner' debugging tool by allowing developers to filter and display stack traces based on the number of outstanding allocations. By maintaining a list of 'stack_records' and providing a mechanism to filter stack traces based on a configurable threshold, the kernel makes it easier to identify memory leaks and inefficient memory usage. This improvement streamlines the debugging process and provides valuable insights into memory allocation patterns, ultimately helping developers optimize memory management in the Linux kernel.",
    "concepts": [
        "page_owner",
        "stack_record",
        "stackdepot",
        "memory allocation",
        "memory deallocation",
        "stack trace",
        "memory leaks",
        "outstanding allocations",
        "threshold filtering"
    ]
}
2024-12-03 22:34:08,289 - extractor - INFO - ---------Processing feature 16/815---------
2024-12-03 22:34:27,450 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个 linux 内核的专家。给你一个 linux 内核特性的描述以及其对应的若干 commit 的标题和提交信息，你的任务是为这个特性写一个多方面的扩写版描述。你的扩写需包含以下几个方面：1. 该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)；2. 为了让开发者理解该特性，需要哪些具体的，与这个特性直接相关的背景知识 (background_knowledge)；3. 该特性被实现前后，linux 内核发生了怎样的变化 (kernel_changes)；4. 你对该特性的综合理解 (comprehensive_understanding)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}]
        },
        "output": {
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
        }
    ],
    "input": {
        "feature_description": "userfaultfd: per-vma locks in userfaultfd",
        "commits": [{"commit_subject": "userfaultfd: use per-vma locks in userfaultfd operations", "commit_message": "All userfaultfd operations, except write-protect, opportunistically use\nper-vma locks to lock vmas.  On failure, attempt again inside mmap_lock\ncritical section.\n\nWrite-protect operation requires mmap_lock as it iterates over multiple\nvmas.\n\nLink: https://lkml.kernel.org/r/20240215182756.3448972-5-lokeshgidra@google.com\nSigned-off-by: Lokesh Gidra <lokeshgidra@google.com>\nReviewed-by: Liam R. Howlett <Liam.Howlett@oracle.com>\nCc: Andrea Arcangeli <aarcange@redhat.com>\nCc: Axel Rasmussen <axelrasmussen@google.com>\nCc: Brian Geffon <bgeffon@google.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Jann Horn <jannh@google.com>\nCc: Kalesh Singh <kaleshsingh@google.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Mike Rapoport (IBM) <rppt@kernel.org>\nCc: Nicolas Geoffray <ngeoffray@google.com>\nCc: Peter Xu <peterx@redhat.com>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Suren Baghdasaryan <surenb@google.com>\nCc: Tim Murray <timmurray@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: add vma_assert_locked() for !CONFIG_PER_VMA_LOCK", "commit_message": "vma_assert_locked() is needed to replace mmap_assert_locked() once we\nstart using per-vma locks in userfaultfd operations.\n\nIn !CONFIG_PER_VMA_LOCK case when mm is locked, it implies that the given\nVMA is locked.\n\nLink: https://lkml.kernel.org/r/20240215182756.3448972-4-lokeshgidra@google.com\nSigned-off-by: Lokesh Gidra <lokeshgidra@google.com>\nReviewed-by: Suren Baghdasaryan <surenb@google.com>\nCc: Andrea Arcangeli <aarcange@redhat.com>\nCc: Axel Rasmussen <axelrasmussen@google.com>\nCc: Brian Geffon <bgeffon@google.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Jann Horn <jannh@google.com>\nCc: Kalesh Singh <kaleshsingh@google.com>\nCc: Liam R. Howlett <Liam.Howlett@oracle.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Mike Rapoport (IBM) <rppt@kernel.org>\nCc: Nicolas Geoffray <ngeoffray@google.com>\nCc: Peter Xu <peterx@redhat.com>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Tim Murray <timmurray@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "userfaultfd: protect mmap_changing with rw_sem in userfaulfd_ctx", "commit_message": "Increments and loads to mmap_changing are always in mmap_lock critical\nsection.  This ensures that if userspace requests event notification for\nnon-cooperative operations (e.g.  mremap), userfaultfd operations don't\noccur concurrently.\n\nThis can be achieved by using a separate read-write semaphore in\nuserfaultfd_ctx such that increments are done in write-mode and loads in\nread-mode, thereby eliminating the dependency on mmap_lock for this\npurpose.\n\nThis is a preparatory step before we replace mmap_lock usage with per-vma\nlocks in fill/move ioctls.\n\nLink: https://lkml.kernel.org/r/20240215182756.3448972-3-lokeshgidra@google.com\nSigned-off-by: Lokesh Gidra <lokeshgidra@google.com>\nReviewed-by: Mike Rapoport (IBM) <rppt@kernel.org>\nReviewed-by: Liam R. Howlett <Liam.Howlett@oracle.com>\nCc: Andrea Arcangeli <aarcange@redhat.com>\nCc: Axel Rasmussen <axelrasmussen@google.com>\nCc: Brian Geffon <bgeffon@google.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Jann Horn <jannh@google.com>\nCc: Kalesh Singh <kaleshsingh@google.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Nicolas Geoffray <ngeoffray@google.com>\nCc: Peter Xu <peterx@redhat.com>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Suren Baghdasaryan <surenb@google.com>\nCc: Tim Murray <timmurray@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "userfaultfd: move userfaultfd_ctx struct to header file", "commit_message": "Patch series \"per-vma locks in userfaultfd\", v7.\n\nPerforming userfaultfd operations (like copy/move etc.) in critical\nsection of mmap_lock (read-mode) causes significant contention on the lock\nwhen operations requiring the lock in write-mode are taking place\nconcurrently.  We can use per-vma locks instead to significantly reduce\nthe contention issue.\n\nAndroid runtime's Garbage Collector uses userfaultfd for concurrent\ncompaction.  mmap-lock contention during compaction potentially causes\njittery experience for the user.  During one such reproducible scenario,\nwe observed the following improvements with this patch-set:\n\n- Wall clock time of compaction phase came down from ~3s to <500ms\n- Uninterruptible sleep time (across all threads in the process) was\n  ~10ms (none in mmap_lock) during compaction, instead of >20s\n\n\nThis patch (of 4):\n\nMove the struct to userfaultfd_k.h to be accessible from mm/userfaultfd.c.\nThere are no other changes in the struct.\n\nThis is required to prepare for using per-vma locks in userfaultfd\noperations.\n\nLink: https://lkml.kernel.org/r/20240215182756.3448972-1-lokeshgidra@google.com\nLink: https://lkml.kernel.org/r/20240215182756.3448972-2-lokeshgidra@google.com\nSigned-off-by: Lokesh Gidra <lokeshgidra@google.com>\nReviewed-by: Mike Rapoport (IBM) <rppt@kernel.org>\nReviewed-by: Liam R. Howlett <Liam.Howlett@oracle.com>\nCc: Andrea Arcangeli <aarcange@redhat.com>\nCc: Axel Rasmussen <axelrasmussen@google.com>\nCc: Brian Geffon <bgeffon@google.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Jann Horn <jannh@google.com>\nCc: Kalesh Singh <kaleshsingh@google.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Nicolas Geoffray <ngeoffray@google.com>\nCc: Peter Xu <peterx@redhat.com>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Suren Baghdasaryan <surenb@google.com>\nCc: Tim Murray <timmurray@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}]
    }
}

2024-12-03 22:35:09,046 - extractor - DEBUG - deepseek response:
ChatCompletion(id='cbeeaff4-adde-43bd-8148-1d817d85af9d', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "proper_nouns": [\n        {\n            "userfaultfd": "userfaultfd is a Linux kernel feature that allows user-space applications to handle page faults in user space. This is particularly useful for implementing advanced memory management techniques, such as lazy page allocation or memory overcommitment, where the kernel can delegate the handling of page faults to user-space code."\n        },\n        {\n            "VMA (Virtual Memory Area)": "A Virtual Memory Area (VMA) is a contiguous range of virtual addresses in a process\'s address space. Each VMA is associated with a specific memory region, such as a file mapping, anonymous memory, or shared memory. VMAs are managed by the kernel and are used to track the properties and permissions of different memory regions."\n        },\n        {\n            "mmap_lock": "mmap_lock is a lock in the Linux kernel used to protect the memory mapping data structures, including the list of VMAs. This lock is critical for ensuring consistency when performing operations that modify the memory mappings, such as mmap, munmap, or mremap."\n        },\n        {\n            "per-VMA locks": "Per-VMA locks are fine-grained locks introduced to lock individual VMAs instead of the entire memory mapping data structure. This allows for more concurrent operations on different VMAs, reducing contention on the mmap_lock and improving performance in scenarios where multiple VMAs are accessed concurrently."\n        },\n        {\n            "rw_semaphore": "A read-write semaphore (rw_semaphore) is a synchronization primitive in the Linux kernel that allows multiple readers or a single writer to access a resource concurrently. This is useful in scenarios where read-heavy operations need to be performed concurrently without blocking each other, while ensuring that write operations have exclusive access."\n        }\n    ],\n    "background_knowledge": [\n        "The userfaultfd feature relies on the ability to handle page faults in user space, which requires careful management of memory mappings and VMAs. Before the introduction of per-VMA locks, all operations on VMAs were protected by the mmap_lock, which could lead to significant contention in scenarios with high concurrency.",\n        "Per-VMA locks were introduced to reduce this contention by allowing concurrent operations on different VMAs. However, some operations, such as write-protect, still require the mmap_lock due to the need to iterate over multiple VMAs."\n    ],\n    "kernel_changes": [\n        "Before the implementation of per-VMA locks in userfaultfd, all userfaultfd operations were protected by the mmap_lock, leading to high contention in scenarios with concurrent memory operations. After the implementation, userfaultfd operations now opportunistically use per-VMA locks, reducing contention and improving performance.",\n        "The write-protect operation remains dependent on the mmap_lock due to its need to iterate over multiple VMAs. Additionally, a new read-write semaphore was introduced in the userfaultfd_ctx structure to protect the mmap_changing field, further reducing dependencies on the mmap_lock."\n    ],\n    "comprehensive_understanding": "The introduction of per-VMA locks in userfaultfd operations represents a significant improvement in the Linux kernel\'s memory management capabilities. By reducing contention on the mmap_lock and allowing more concurrent operations on different VMAs, this feature enhances the performance of userfaultfd operations, particularly in scenarios with high concurrency. The use of read-write semaphores to protect critical sections further optimizes the handling of memory operations, ensuring that userfaultfd can operate efficiently even under heavy load."\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733236468, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=782, prompt_tokens=3613, total_tokens=4395, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=1472, prompt_cache_miss_tokens=2141))
2024-12-03 22:35:09,046 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个命名实体抽取 (NER) 任务的专家，同时也是 linux 内核的专家。给你一个 linux 内核特性的描述 (feature_description)、对应的若干 commit 的标题和提交信息 (commits)、该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)、为了让开发者理解该特性所需的与这个特性直接相关的背景知识 (background_knowledge)、该特性被实现前后，linux 内核发生的变化 (kernel_changes)、对该特性的综合理解 (comprehensive_understanding)。你的任务是：根据这些信息提取出在 linux 内核中涉及到的和这个特性相关的概念实体 (concepts)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}],
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": [
                "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
            ]
        },
        "output": {
            "concepts": ["Copy-on-Write (CoW)", "mTHP", "folio", "memory reclamation", "memory availability", "anti-fragmentation"]
        }
    ],
    "input": {
        "feature_description": "userfaultfd: per-vma locks in userfaultfd",
        "commits": [{"commit_subject": "userfaultfd: use per-vma locks in userfaultfd operations", "commit_message": "All userfaultfd operations, except write-protect, opportunistically use\nper-vma locks to lock vmas.  On failure, attempt again inside mmap_lock\ncritical section.\n\nWrite-protect operation requires mmap_lock as it iterates over multiple\nvmas.\n\nLink: https://lkml.kernel.org/r/20240215182756.3448972-5-lokeshgidra@google.com\nSigned-off-by: Lokesh Gidra <lokeshgidra@google.com>\nReviewed-by: Liam R. Howlett <Liam.Howlett@oracle.com>\nCc: Andrea Arcangeli <aarcange@redhat.com>\nCc: Axel Rasmussen <axelrasmussen@google.com>\nCc: Brian Geffon <bgeffon@google.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Jann Horn <jannh@google.com>\nCc: Kalesh Singh <kaleshsingh@google.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Mike Rapoport (IBM) <rppt@kernel.org>\nCc: Nicolas Geoffray <ngeoffray@google.com>\nCc: Peter Xu <peterx@redhat.com>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Suren Baghdasaryan <surenb@google.com>\nCc: Tim Murray <timmurray@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: add vma_assert_locked() for !CONFIG_PER_VMA_LOCK", "commit_message": "vma_assert_locked() is needed to replace mmap_assert_locked() once we\nstart using per-vma locks in userfaultfd operations.\n\nIn !CONFIG_PER_VMA_LOCK case when mm is locked, it implies that the given\nVMA is locked.\n\nLink: https://lkml.kernel.org/r/20240215182756.3448972-4-lokeshgidra@google.com\nSigned-off-by: Lokesh Gidra <lokeshgidra@google.com>\nReviewed-by: Suren Baghdasaryan <surenb@google.com>\nCc: Andrea Arcangeli <aarcange@redhat.com>\nCc: Axel Rasmussen <axelrasmussen@google.com>\nCc: Brian Geffon <bgeffon@google.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Jann Horn <jannh@google.com>\nCc: Kalesh Singh <kaleshsingh@google.com>\nCc: Liam R. Howlett <Liam.Howlett@oracle.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Mike Rapoport (IBM) <rppt@kernel.org>\nCc: Nicolas Geoffray <ngeoffray@google.com>\nCc: Peter Xu <peterx@redhat.com>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Tim Murray <timmurray@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "userfaultfd: protect mmap_changing with rw_sem in userfaulfd_ctx", "commit_message": "Increments and loads to mmap_changing are always in mmap_lock critical\nsection.  This ensures that if userspace requests event notification for\nnon-cooperative operations (e.g.  mremap), userfaultfd operations don't\noccur concurrently.\n\nThis can be achieved by using a separate read-write semaphore in\nuserfaultfd_ctx such that increments are done in write-mode and loads in\nread-mode, thereby eliminating the dependency on mmap_lock for this\npurpose.\n\nThis is a preparatory step before we replace mmap_lock usage with per-vma\nlocks in fill/move ioctls.\n\nLink: https://lkml.kernel.org/r/20240215182756.3448972-3-lokeshgidra@google.com\nSigned-off-by: Lokesh Gidra <lokeshgidra@google.com>\nReviewed-by: Mike Rapoport (IBM) <rppt@kernel.org>\nReviewed-by: Liam R. Howlett <Liam.Howlett@oracle.com>\nCc: Andrea Arcangeli <aarcange@redhat.com>\nCc: Axel Rasmussen <axelrasmussen@google.com>\nCc: Brian Geffon <bgeffon@google.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Jann Horn <jannh@google.com>\nCc: Kalesh Singh <kaleshsingh@google.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Nicolas Geoffray <ngeoffray@google.com>\nCc: Peter Xu <peterx@redhat.com>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Suren Baghdasaryan <surenb@google.com>\nCc: Tim Murray <timmurray@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "userfaultfd: move userfaultfd_ctx struct to header file", "commit_message": "Patch series \"per-vma locks in userfaultfd\", v7.\n\nPerforming userfaultfd operations (like copy/move etc.) in critical\nsection of mmap_lock (read-mode) causes significant contention on the lock\nwhen operations requiring the lock in write-mode are taking place\nconcurrently.  We can use per-vma locks instead to significantly reduce\nthe contention issue.\n\nAndroid runtime's Garbage Collector uses userfaultfd for concurrent\ncompaction.  mmap-lock contention during compaction potentially causes\njittery experience for the user.  During one such reproducible scenario,\nwe observed the following improvements with this patch-set:\n\n- Wall clock time of compaction phase came down from ~3s to <500ms\n- Uninterruptible sleep time (across all threads in the process) was\n  ~10ms (none in mmap_lock) during compaction, instead of >20s\n\n\nThis patch (of 4):\n\nMove the struct to userfaultfd_k.h to be accessible from mm/userfaultfd.c.\nThere are no other changes in the struct.\n\nThis is required to prepare for using per-vma locks in userfaultfd\noperations.\n\nLink: https://lkml.kernel.org/r/20240215182756.3448972-1-lokeshgidra@google.com\nLink: https://lkml.kernel.org/r/20240215182756.3448972-2-lokeshgidra@google.com\nSigned-off-by: Lokesh Gidra <lokeshgidra@google.com>\nReviewed-by: Mike Rapoport (IBM) <rppt@kernel.org>\nReviewed-by: Liam R. Howlett <Liam.Howlett@oracle.com>\nCc: Andrea Arcangeli <aarcange@redhat.com>\nCc: Axel Rasmussen <axelrasmussen@google.com>\nCc: Brian Geffon <bgeffon@google.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Jann Horn <jannh@google.com>\nCc: Kalesh Singh <kaleshsingh@google.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Nicolas Geoffray <ngeoffray@google.com>\nCc: Peter Xu <peterx@redhat.com>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Suren Baghdasaryan <surenb@google.com>\nCc: Tim Murray <timmurray@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}],
        "proper_nouns": [{"userfaultfd": "userfaultfd is a Linux kernel feature that allows user-space applications to handle page faults in user space. This is particularly useful for implementing advanced memory management techniques, such as lazy page allocation or memory overcommitment, where the kernel can delegate the handling of page faults to user-space code."}, {"VMA (Virtual Memory Area)": "A Virtual Memory Area (VMA) is a contiguous range of virtual addresses in a process's address space. Each VMA is associated with a specific memory region, such as a file mapping, anonymous memory, or shared memory. VMAs are managed by the kernel and are used to track the properties and permissions of different memory regions."}, {"mmap_lock": "mmap_lock is a lock in the Linux kernel used to protect the memory mapping data structures, including the list of VMAs. This lock is critical for ensuring consistency when performing operations that modify the memory mappings, such as mmap, munmap, or mremap."}, {"per-VMA locks": "Per-VMA locks are fine-grained locks introduced to lock individual VMAs instead of the entire memory mapping data structure. This allows for more concurrent operations on different VMAs, reducing contention on the mmap_lock and improving performance in scenarios where multiple VMAs are accessed concurrently."}, {"rw_semaphore": "A read-write semaphore (rw_semaphore) is a synchronization primitive in the Linux kernel that allows multiple readers or a single writer to access a resource concurrently. This is useful in scenarios where read-heavy operations need to be performed concurrently without blocking each other, while ensuring that write operations have exclusive access."}],
        "background_knowledge": ["The userfaultfd feature relies on the ability to handle page faults in user space, which requires careful management of memory mappings and VMAs. Before the introduction of per-VMA locks, all operations on VMAs were protected by the mmap_lock, which could lead to significant contention in scenarios with high concurrency.", "Per-VMA locks were introduced to reduce this contention by allowing concurrent operations on different VMAs. However, some operations, such as write-protect, still require the mmap_lock due to the need to iterate over multiple VMAs."],
        "kernel_changes": ["Before the implementation of per-VMA locks in userfaultfd, all userfaultfd operations were protected by the mmap_lock, leading to high contention in scenarios with concurrent memory operations. After the implementation, userfaultfd operations now opportunistically use per-VMA locks, reducing contention and improving performance.", "The write-protect operation remains dependent on the mmap_lock due to its need to iterate over multiple VMAs. Additionally, a new read-write semaphore was introduced in the userfaultfd_ctx structure to protect the mmap_changing field, further reducing dependencies on the mmap_lock."],
        "comprehensive_understanding": "The introduction of per-VMA locks in userfaultfd operations represents a significant improvement in the Linux kernel's memory management capabilities. By reducing contention on the mmap_lock and allowing more concurrent operations on different VMAs, this feature enhances the performance of userfaultfd operations, particularly in scenarios with high concurrency. The use of read-write semaphores to protect critical sections further optimizes the handling of memory operations, ensuring that userfaultfd can operate efficiently even under heavy load."
    }
}

2024-12-03 22:35:13,390 - extractor - DEBUG - deepseek response:
ChatCompletion(id='cc24c756-12b4-42c0-9539-a1a7499778ca', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "concepts": ["userfaultfd", "VMA (Virtual Memory Area)", "mmap_lock", "per-VMA locks", "rw_semaphore", "memory mapping", "page faults", "memory management", "concurrency", "performance optimization"]\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733236509, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=66, prompt_tokens=4391, total_tokens=4457, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=1536, prompt_cache_miss_tokens=2855))
2024-12-03 22:35:13,390 - extractor - DEBUG - feature_extracted of feature_id=33224:
{
    "feature_id": 33224,
    "feature_description": "userfaultfd: per-vma locks in userfaultfd",
    "version": "6.9",
    "commits": [
        {
            "commit_subject": "userfaultfd: use per-vma locks in userfaultfd operations",
            "commit_message": "All userfaultfd operations, except write-protect, opportunistically use\nper-vma locks to lock vmas.  On failure, attempt again inside mmap_lock\ncritical section.\n\nWrite-protect operation requires mmap_lock as it iterates over multiple\nvmas.\n\nLink: https://lkml.kernel.org/r/20240215182756.3448972-5-lokeshgidra@google.com\nSigned-off-by: Lokesh Gidra <lokeshgidra@google.com>\nReviewed-by: Liam R. Howlett <Liam.Howlett@oracle.com>\nCc: Andrea Arcangeli <aarcange@redhat.com>\nCc: Axel Rasmussen <axelrasmussen@google.com>\nCc: Brian Geffon <bgeffon@google.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Jann Horn <jannh@google.com>\nCc: Kalesh Singh <kaleshsingh@google.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Mike Rapoport (IBM) <rppt@kernel.org>\nCc: Nicolas Geoffray <ngeoffray@google.com>\nCc: Peter Xu <peterx@redhat.com>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Suren Baghdasaryan <surenb@google.com>\nCc: Tim Murray <timmurray@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm: add vma_assert_locked() for !CONFIG_PER_VMA_LOCK",
            "commit_message": "vma_assert_locked() is needed to replace mmap_assert_locked() once we\nstart using per-vma locks in userfaultfd operations.\n\nIn !CONFIG_PER_VMA_LOCK case when mm is locked, it implies that the given\nVMA is locked.\n\nLink: https://lkml.kernel.org/r/20240215182756.3448972-4-lokeshgidra@google.com\nSigned-off-by: Lokesh Gidra <lokeshgidra@google.com>\nReviewed-by: Suren Baghdasaryan <surenb@google.com>\nCc: Andrea Arcangeli <aarcange@redhat.com>\nCc: Axel Rasmussen <axelrasmussen@google.com>\nCc: Brian Geffon <bgeffon@google.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Jann Horn <jannh@google.com>\nCc: Kalesh Singh <kaleshsingh@google.com>\nCc: Liam R. Howlett <Liam.Howlett@oracle.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Mike Rapoport (IBM) <rppt@kernel.org>\nCc: Nicolas Geoffray <ngeoffray@google.com>\nCc: Peter Xu <peterx@redhat.com>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Tim Murray <timmurray@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "userfaultfd: protect mmap_changing with rw_sem in userfaulfd_ctx",
            "commit_message": "Increments and loads to mmap_changing are always in mmap_lock critical\nsection.  This ensures that if userspace requests event notification for\nnon-cooperative operations (e.g.  mremap), userfaultfd operations don't\noccur concurrently.\n\nThis can be achieved by using a separate read-write semaphore in\nuserfaultfd_ctx such that increments are done in write-mode and loads in\nread-mode, thereby eliminating the dependency on mmap_lock for this\npurpose.\n\nThis is a preparatory step before we replace mmap_lock usage with per-vma\nlocks in fill/move ioctls.\n\nLink: https://lkml.kernel.org/r/20240215182756.3448972-3-lokeshgidra@google.com\nSigned-off-by: Lokesh Gidra <lokeshgidra@google.com>\nReviewed-by: Mike Rapoport (IBM) <rppt@kernel.org>\nReviewed-by: Liam R. Howlett <Liam.Howlett@oracle.com>\nCc: Andrea Arcangeli <aarcange@redhat.com>\nCc: Axel Rasmussen <axelrasmussen@google.com>\nCc: Brian Geffon <bgeffon@google.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Jann Horn <jannh@google.com>\nCc: Kalesh Singh <kaleshsingh@google.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Nicolas Geoffray <ngeoffray@google.com>\nCc: Peter Xu <peterx@redhat.com>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Suren Baghdasaryan <surenb@google.com>\nCc: Tim Murray <timmurray@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "userfaultfd: move userfaultfd_ctx struct to header file",
            "commit_message": "Patch series \"per-vma locks in userfaultfd\", v7.\n\nPerforming userfaultfd operations (like copy/move etc.) in critical\nsection of mmap_lock (read-mode) causes significant contention on the lock\nwhen operations requiring the lock in write-mode are taking place\nconcurrently.  We can use per-vma locks instead to significantly reduce\nthe contention issue.\n\nAndroid runtime's Garbage Collector uses userfaultfd for concurrent\ncompaction.  mmap-lock contention during compaction potentially causes\njittery experience for the user.  During one such reproducible scenario,\nwe observed the following improvements with this patch-set:\n\n- Wall clock time of compaction phase came down from ~3s to <500ms\n- Uninterruptible sleep time (across all threads in the process) was\n  ~10ms (none in mmap_lock) during compaction, instead of >20s\n\n\nThis patch (of 4):\n\nMove the struct to userfaultfd_k.h to be accessible from mm/userfaultfd.c.\nThere are no other changes in the struct.\n\nThis is required to prepare for using per-vma locks in userfaultfd\noperations.\n\nLink: https://lkml.kernel.org/r/20240215182756.3448972-1-lokeshgidra@google.com\nLink: https://lkml.kernel.org/r/20240215182756.3448972-2-lokeshgidra@google.com\nSigned-off-by: Lokesh Gidra <lokeshgidra@google.com>\nReviewed-by: Mike Rapoport (IBM) <rppt@kernel.org>\nReviewed-by: Liam R. Howlett <Liam.Howlett@oracle.com>\nCc: Andrea Arcangeli <aarcange@redhat.com>\nCc: Axel Rasmussen <axelrasmussen@google.com>\nCc: Brian Geffon <bgeffon@google.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Jann Horn <jannh@google.com>\nCc: Kalesh Singh <kaleshsingh@google.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Nicolas Geoffray <ngeoffray@google.com>\nCc: Peter Xu <peterx@redhat.com>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Suren Baghdasaryan <surenb@google.com>\nCc: Tim Murray <timmurray@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        }
    ],
    "proper_nouns": [
        {
            "userfaultfd": "userfaultfd is a Linux kernel feature that allows user-space applications to handle page faults in user space. This is particularly useful for implementing advanced memory management techniques, such as lazy page allocation or memory overcommitment, where the kernel can delegate the handling of page faults to user-space code."
        },
        {
            "VMA (Virtual Memory Area)": "A Virtual Memory Area (VMA) is a contiguous range of virtual addresses in a process's address space. Each VMA is associated with a specific memory region, such as a file mapping, anonymous memory, or shared memory. VMAs are managed by the kernel and are used to track the properties and permissions of different memory regions."
        },
        {
            "mmap_lock": "mmap_lock is a lock in the Linux kernel used to protect the memory mapping data structures, including the list of VMAs. This lock is critical for ensuring consistency when performing operations that modify the memory mappings, such as mmap, munmap, or mremap."
        },
        {
            "per-VMA locks": "Per-VMA locks are fine-grained locks introduced to lock individual VMAs instead of the entire memory mapping data structure. This allows for more concurrent operations on different VMAs, reducing contention on the mmap_lock and improving performance in scenarios where multiple VMAs are accessed concurrently."
        },
        {
            "rw_semaphore": "A read-write semaphore (rw_semaphore) is a synchronization primitive in the Linux kernel that allows multiple readers or a single writer to access a resource concurrently. This is useful in scenarios where read-heavy operations need to be performed concurrently without blocking each other, while ensuring that write operations have exclusive access."
        }
    ],
    "background_knowledge": [
        "The userfaultfd feature relies on the ability to handle page faults in user space, which requires careful management of memory mappings and VMAs. Before the introduction of per-VMA locks, all operations on VMAs were protected by the mmap_lock, which could lead to significant contention in scenarios with high concurrency.",
        "Per-VMA locks were introduced to reduce this contention by allowing concurrent operations on different VMAs. However, some operations, such as write-protect, still require the mmap_lock due to the need to iterate over multiple VMAs."
    ],
    "kernel_changes": [
        "Before the implementation of per-VMA locks in userfaultfd, all userfaultfd operations were protected by the mmap_lock, leading to high contention in scenarios with concurrent memory operations. After the implementation, userfaultfd operations now opportunistically use per-VMA locks, reducing contention and improving performance.",
        "The write-protect operation remains dependent on the mmap_lock due to its need to iterate over multiple VMAs. Additionally, a new read-write semaphore was introduced in the userfaultfd_ctx structure to protect the mmap_changing field, further reducing dependencies on the mmap_lock."
    ],
    "comprehensive_understanding": "The introduction of per-VMA locks in userfaultfd operations represents a significant improvement in the Linux kernel's memory management capabilities. By reducing contention on the mmap_lock and allowing more concurrent operations on different VMAs, this feature enhances the performance of userfaultfd operations, particularly in scenarios with high concurrency. The use of read-write semaphores to protect critical sections further optimizes the handling of memory operations, ensuring that userfaultfd can operate efficiently even under heavy load.",
    "concepts": [
        "userfaultfd",
        "VMA (Virtual Memory Area)",
        "mmap_lock",
        "per-VMA locks",
        "rw_semaphore",
        "memory mapping",
        "page faults",
        "memory management",
        "concurrency",
        "performance optimization"
    ]
}
2024-12-03 22:35:13,390 - extractor - INFO - ---------Processing feature 17/815---------
2024-12-03 22:35:17,221 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个 linux 内核的专家。给你一个 linux 内核特性的描述以及其对应的若干 commit 的标题和提交信息，你的任务是为这个特性写一个多方面的扩写版描述。你的扩写需包含以下几个方面：1. 该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)；2. 为了让开发者理解该特性，需要哪些具体的，与这个特性直接相关的背景知识 (background_knowledge)；3. 该特性被实现前后，linux 内核发生了怎样的变化 (kernel_changes)；4. 你对该特性的综合理解 (comprehensive_understanding)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}]
        },
        "output": {
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
        }
    ],
    "input": {
        "feature_description": "memcg: use larger batches for proactive reclaim",
        "commits": [{"commit_subject": "mm: memcg: use larger batches for proactive reclaim", "commit_message": "Before 388536ac291 (\"mm:vmscan: fix inaccurate reclaim during proactive\nreclaim\") we passed the number of pages for the reclaim request directly\nto try_to_free_mem_cgroup_pages, which could lead to significant\noverreclaim.  After 0388536ac291 the number of pages was limited to a\nmaximum 32 (SWAP_CLUSTER_MAX) to reduce the amount of overreclaim. \nHowever such a small batch size caused a regression in reclaim performance\ndue to many more reclaim start/stop cycles inside memory_reclaim.  The\nrestart cost is amortized over more pages with larger batch sizes, and\nbecomes a significant component of the runtime if the batch size is too\nsmall.\n\nReclaim tries to balance nr_to_reclaim fidelity with fairness across nodes\nand cgroups over which the pages are spread.  As such, the bigger the\nrequest, the bigger the absolute overreclaim error.  Historic in-kernel\nusers of reclaim have used fixed, small sized requests to approach an\nappropriate reclaim rate over time.  When we reclaim a user request of\narbitrary size, use decaying batch sizes to manage error while maintaining\nreasonable throughput.\n\nMGLRU enabled - memcg LRU used\nroot - full reclaim       pages/sec   time (sec)\npre-0388536ac291      :    68047        10.46\npost-0388536ac291     :    13742        inf\n(reclaim-reclaimed)/4 :    67352        10.51\n\nMGLRU enabled - memcg LRU not used\n/uid_0 - 1G reclaim       pages/sec   time (sec)  overreclaim (MiB)\npre-0388536ac291      :    258822       1.12            107.8\npost-0388536ac291     :    105174       2.49            3.5\n(reclaim-reclaimed)/4 :    233396       1.12            -7.4\n\nMGLRU enabled - memcg LRU not used\n/uid_0 - full reclaim     pages/sec   time (sec)\npre-0388536ac291      :    72334        7.09\npost-0388536ac291     :    38105        14.45\n(reclaim-reclaimed)/4 :    72914        6.96\n\n[tjmercier@google.com: v4]\n  Link: https://lkml.kernel.org/r/20240206175251.3364296-1-tjmercier@google.com\nLink: https://lkml.kernel.org/r/20240202233855.1236422-1-tjmercier@google.com\nFixes: 0388536ac291 (\"mm:vmscan: fix inaccurate reclaim during proactive reclaim\")\nSigned-off-by: T.J. Mercier <tjmercier@google.com>\nReviewed-by: Yosry Ahmed <yosryahmed@google.com>\nAcked-by: Johannes Weiner <hannes@cmpxchg.org>\nReviewed-by: Michal Koutny <mkoutny@suse.com>\nAcked-by: Shakeel Butt <shakeelb@google.com>\nAcked-by: Michal Hocko <mhocko@suse.com>\nCc: Roman Gushchin <roman.gushchin@linux.dev>\nCc: Shakeel Butt <shakeelb@google.com>\nCc: Muchun Song <songmuchun@bytedance.com>\nCc: Efly Young <yangyifei03@kuaishou.com>\nCc: Yu Zhao <yuzhao@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}]
    }
}

2024-12-03 22:35:52,349 - extractor - DEBUG - deepseek response:
ChatCompletion(id='d86da113-e863-4d38-9eee-b2672c575502', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "proper_nouns": [\n        {\n            "memcg": "memcg, short for Memory Control Group, is a feature in the Linux kernel that allows for hierarchical memory resource management. It enables the kernel to enforce memory limits and policies on groups of processes, providing a way to manage memory usage more granularly."\n        },\n        {\n            "proactive reclaim": "Proactive reclaim is a memory management strategy in the Linux kernel where the system proactively reclaims memory before it becomes critically low. This helps in maintaining system responsiveness and preventing out-of-memory (OOM) conditions."\n        },\n        {\n            "MGLRU": "MGLRU, or Multi-Gen LRU, is an advanced page replacement algorithm in the Linux kernel. It aims to improve the efficiency of page reclaim by categorizing pages into multiple generations and prioritizing the eviction of older, less frequently used pages."\n        },\n        {\n            "SWAP_CLUSTER_MAX": "SWAP_CLUSTER_MAX is a constant in the Linux kernel that defines the maximum number of pages to be reclaimed in a single batch during memory reclaim operations. It helps in balancing the trade-off between the efficiency of reclaim operations and the overhead of managing multiple small batches."\n        }\n    ],\n    "background_knowledge": [\n        "The Linux kernel\'s memory management subsystem includes mechanisms like memcg for hierarchical memory resource management and proactive reclaim for maintaining system responsiveness. Proactive reclaim involves reclaiming memory before it becomes critically low, and it uses algorithms like MGLRU for efficient page replacement. The SWAP_CLUSTER_MAX constant defines the maximum number of pages to be reclaimed in a single batch, which affects the balance between reclaim efficiency and overhead."\n    ],\n    "kernel_changes": [\n        "Before the introduction of this feature, the proactive reclaim process used a fixed batch size of 32 pages (SWAP_CLUSTER_MAX) to reduce overreclaim. However, this small batch size led to performance regressions due to frequent reclaim start/stop cycles. After the implementation of this feature, the batch size for proactive reclaim was increased, which reduced the number of reclaim cycles and improved overall reclaim performance. This change helped in maintaining a balance between the fidelity of nr_to_reclaim and fairness across nodes and cgroups, thereby enhancing the efficiency of memory reclaim operations."\n    ],\n    "comprehensive_understanding": "This feature enhances the proactive reclaim process in the Linux kernel by using larger batch sizes for memory reclaim operations. By increasing the batch size from the previous fixed limit of 32 pages (SWAP_CLUSTER_MAX), the feature reduces the number of reclaim cycles, thereby improving reclaim performance and system responsiveness. This change is particularly beneficial in scenarios where memory is spread across multiple nodes and cgroups, as it helps in balancing the trade-off between the accuracy of reclaim requests and the fairness of memory distribution. Overall, the use of larger batches for proactive reclaim optimizes memory management, reduces overreclaim, and enhances system performance."\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733236517, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=648, prompt_tokens=2589, total_tokens=3237, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=1472, prompt_cache_miss_tokens=1117))
2024-12-03 22:35:52,350 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个命名实体抽取 (NER) 任务的专家，同时也是 linux 内核的专家。给你一个 linux 内核特性的描述 (feature_description)、对应的若干 commit 的标题和提交信息 (commits)、该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)、为了让开发者理解该特性所需的与这个特性直接相关的背景知识 (background_knowledge)、该特性被实现前后，linux 内核发生的变化 (kernel_changes)、对该特性的综合理解 (comprehensive_understanding)。你的任务是：根据这些信息提取出在 linux 内核中涉及到的和这个特性相关的概念实体 (concepts)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}],
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": [
                "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
            ]
        },
        "output": {
            "concepts": ["Copy-on-Write (CoW)", "mTHP", "folio", "memory reclamation", "memory availability", "anti-fragmentation"]
        }
    ],
    "input": {
        "feature_description": "memcg: use larger batches for proactive reclaim",
        "commits": [{"commit_subject": "mm: memcg: use larger batches for proactive reclaim", "commit_message": "Before 388536ac291 (\"mm:vmscan: fix inaccurate reclaim during proactive\nreclaim\") we passed the number of pages for the reclaim request directly\nto try_to_free_mem_cgroup_pages, which could lead to significant\noverreclaim.  After 0388536ac291 the number of pages was limited to a\nmaximum 32 (SWAP_CLUSTER_MAX) to reduce the amount of overreclaim. \nHowever such a small batch size caused a regression in reclaim performance\ndue to many more reclaim start/stop cycles inside memory_reclaim.  The\nrestart cost is amortized over more pages with larger batch sizes, and\nbecomes a significant component of the runtime if the batch size is too\nsmall.\n\nReclaim tries to balance nr_to_reclaim fidelity with fairness across nodes\nand cgroups over which the pages are spread.  As such, the bigger the\nrequest, the bigger the absolute overreclaim error.  Historic in-kernel\nusers of reclaim have used fixed, small sized requests to approach an\nappropriate reclaim rate over time.  When we reclaim a user request of\narbitrary size, use decaying batch sizes to manage error while maintaining\nreasonable throughput.\n\nMGLRU enabled - memcg LRU used\nroot - full reclaim       pages/sec   time (sec)\npre-0388536ac291      :    68047        10.46\npost-0388536ac291     :    13742        inf\n(reclaim-reclaimed)/4 :    67352        10.51\n\nMGLRU enabled - memcg LRU not used\n/uid_0 - 1G reclaim       pages/sec   time (sec)  overreclaim (MiB)\npre-0388536ac291      :    258822       1.12            107.8\npost-0388536ac291     :    105174       2.49            3.5\n(reclaim-reclaimed)/4 :    233396       1.12            -7.4\n\nMGLRU enabled - memcg LRU not used\n/uid_0 - full reclaim     pages/sec   time (sec)\npre-0388536ac291      :    72334        7.09\npost-0388536ac291     :    38105        14.45\n(reclaim-reclaimed)/4 :    72914        6.96\n\n[tjmercier@google.com: v4]\n  Link: https://lkml.kernel.org/r/20240206175251.3364296-1-tjmercier@google.com\nLink: https://lkml.kernel.org/r/20240202233855.1236422-1-tjmercier@google.com\nFixes: 0388536ac291 (\"mm:vmscan: fix inaccurate reclaim during proactive reclaim\")\nSigned-off-by: T.J. Mercier <tjmercier@google.com>\nReviewed-by: Yosry Ahmed <yosryahmed@google.com>\nAcked-by: Johannes Weiner <hannes@cmpxchg.org>\nReviewed-by: Michal Koutny <mkoutny@suse.com>\nAcked-by: Shakeel Butt <shakeelb@google.com>\nAcked-by: Michal Hocko <mhocko@suse.com>\nCc: Roman Gushchin <roman.gushchin@linux.dev>\nCc: Shakeel Butt <shakeelb@google.com>\nCc: Muchun Song <songmuchun@bytedance.com>\nCc: Efly Young <yangyifei03@kuaishou.com>\nCc: Yu Zhao <yuzhao@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}],
        "proper_nouns": [{"memcg": "memcg, short for Memory Control Group, is a feature in the Linux kernel that allows for hierarchical memory resource management. It enables the kernel to enforce memory limits and policies on groups of processes, providing a way to manage memory usage more granularly."}, {"proactive reclaim": "Proactive reclaim is a memory management strategy in the Linux kernel where the system proactively reclaims memory before it becomes critically low. This helps in maintaining system responsiveness and preventing out-of-memory (OOM) conditions."}, {"MGLRU": "MGLRU, or Multi-Gen LRU, is an advanced page replacement algorithm in the Linux kernel. It aims to improve the efficiency of page reclaim by categorizing pages into multiple generations and prioritizing the eviction of older, less frequently used pages."}, {"SWAP_CLUSTER_MAX": "SWAP_CLUSTER_MAX is a constant in the Linux kernel that defines the maximum number of pages to be reclaimed in a single batch during memory reclaim operations. It helps in balancing the trade-off between the efficiency of reclaim operations and the overhead of managing multiple small batches."}],
        "background_knowledge": ["The Linux kernel's memory management subsystem includes mechanisms like memcg for hierarchical memory resource management and proactive reclaim for maintaining system responsiveness. Proactive reclaim involves reclaiming memory before it becomes critically low, and it uses algorithms like MGLRU for efficient page replacement. The SWAP_CLUSTER_MAX constant defines the maximum number of pages to be reclaimed in a single batch, which affects the balance between reclaim efficiency and overhead."],
        "kernel_changes": ["Before the introduction of this feature, the proactive reclaim process used a fixed batch size of 32 pages (SWAP_CLUSTER_MAX) to reduce overreclaim. However, this small batch size led to performance regressions due to frequent reclaim start/stop cycles. After the implementation of this feature, the batch size for proactive reclaim was increased, which reduced the number of reclaim cycles and improved overall reclaim performance. This change helped in maintaining a balance between the fidelity of nr_to_reclaim and fairness across nodes and cgroups, thereby enhancing the efficiency of memory reclaim operations."],
        "comprehensive_understanding": "This feature enhances the proactive reclaim process in the Linux kernel by using larger batch sizes for memory reclaim operations. By increasing the batch size from the previous fixed limit of 32 pages (SWAP_CLUSTER_MAX), the feature reduces the number of reclaim cycles, thereby improving reclaim performance and system responsiveness. This change is particularly beneficial in scenarios where memory is spread across multiple nodes and cgroups, as it helps in balancing the trade-off between the accuracy of reclaim requests and the fairness of memory distribution. Overall, the use of larger batches for proactive reclaim optimizes memory management, reduces overreclaim, and enhances system performance."
    }
}

2024-12-03 22:35:57,152 - extractor - DEBUG - deepseek response:
ChatCompletion(id='1ed3de30-313c-4162-a042-e24edd784787', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "concepts": ["memcg", "proactive reclaim", "MGLRU", "SWAP_CLUSTER_MAX", "memory reclaim operations", "memory management", "system responsiveness", "overreclaim", "nr_to_reclaim fidelity", "fairness across nodes and cgroups"]\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733236552, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=73, prompt_tokens=3244, total_tokens=3317, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=1536, prompt_cache_miss_tokens=1708))
2024-12-03 22:35:57,152 - extractor - DEBUG - feature_extracted of feature_id=33223:
{
    "feature_id": 33223,
    "feature_description": "memcg: use larger batches for proactive reclaim",
    "version": "6.9",
    "commits": [
        {
            "commit_subject": "mm: memcg: use larger batches for proactive reclaim",
            "commit_message": "Before 388536ac291 (\"mm:vmscan: fix inaccurate reclaim during proactive\nreclaim\") we passed the number of pages for the reclaim request directly\nto try_to_free_mem_cgroup_pages, which could lead to significant\noverreclaim.  After 0388536ac291 the number of pages was limited to a\nmaximum 32 (SWAP_CLUSTER_MAX) to reduce the amount of overreclaim. \nHowever such a small batch size caused a regression in reclaim performance\ndue to many more reclaim start/stop cycles inside memory_reclaim.  The\nrestart cost is amortized over more pages with larger batch sizes, and\nbecomes a significant component of the runtime if the batch size is too\nsmall.\n\nReclaim tries to balance nr_to_reclaim fidelity with fairness across nodes\nand cgroups over which the pages are spread.  As such, the bigger the\nrequest, the bigger the absolute overreclaim error.  Historic in-kernel\nusers of reclaim have used fixed, small sized requests to approach an\nappropriate reclaim rate over time.  When we reclaim a user request of\narbitrary size, use decaying batch sizes to manage error while maintaining\nreasonable throughput.\n\nMGLRU enabled - memcg LRU used\nroot - full reclaim       pages/sec   time (sec)\npre-0388536ac291      :    68047        10.46\npost-0388536ac291     :    13742        inf\n(reclaim-reclaimed)/4 :    67352        10.51\n\nMGLRU enabled - memcg LRU not used\n/uid_0 - 1G reclaim       pages/sec   time (sec)  overreclaim (MiB)\npre-0388536ac291      :    258822       1.12            107.8\npost-0388536ac291     :    105174       2.49            3.5\n(reclaim-reclaimed)/4 :    233396       1.12            -7.4\n\nMGLRU enabled - memcg LRU not used\n/uid_0 - full reclaim     pages/sec   time (sec)\npre-0388536ac291      :    72334        7.09\npost-0388536ac291     :    38105        14.45\n(reclaim-reclaimed)/4 :    72914        6.96\n\n[tjmercier@google.com: v4]\n  Link: https://lkml.kernel.org/r/20240206175251.3364296-1-tjmercier@google.com\nLink: https://lkml.kernel.org/r/20240202233855.1236422-1-tjmercier@google.com\nFixes: 0388536ac291 (\"mm:vmscan: fix inaccurate reclaim during proactive reclaim\")\nSigned-off-by: T.J. Mercier <tjmercier@google.com>\nReviewed-by: Yosry Ahmed <yosryahmed@google.com>\nAcked-by: Johannes Weiner <hannes@cmpxchg.org>\nReviewed-by: Michal Koutny <mkoutny@suse.com>\nAcked-by: Shakeel Butt <shakeelb@google.com>\nAcked-by: Michal Hocko <mhocko@suse.com>\nCc: Roman Gushchin <roman.gushchin@linux.dev>\nCc: Shakeel Butt <shakeelb@google.com>\nCc: Muchun Song <songmuchun@bytedance.com>\nCc: Efly Young <yangyifei03@kuaishou.com>\nCc: Yu Zhao <yuzhao@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        }
    ],
    "proper_nouns": [
        {
            "memcg": "memcg, short for Memory Control Group, is a feature in the Linux kernel that allows for hierarchical memory resource management. It enables the kernel to enforce memory limits and policies on groups of processes, providing a way to manage memory usage more granularly."
        },
        {
            "proactive reclaim": "Proactive reclaim is a memory management strategy in the Linux kernel where the system proactively reclaims memory before it becomes critically low. This helps in maintaining system responsiveness and preventing out-of-memory (OOM) conditions."
        },
        {
            "MGLRU": "MGLRU, or Multi-Gen LRU, is an advanced page replacement algorithm in the Linux kernel. It aims to improve the efficiency of page reclaim by categorizing pages into multiple generations and prioritizing the eviction of older, less frequently used pages."
        },
        {
            "SWAP_CLUSTER_MAX": "SWAP_CLUSTER_MAX is a constant in the Linux kernel that defines the maximum number of pages to be reclaimed in a single batch during memory reclaim operations. It helps in balancing the trade-off between the efficiency of reclaim operations and the overhead of managing multiple small batches."
        }
    ],
    "background_knowledge": [
        "The Linux kernel's memory management subsystem includes mechanisms like memcg for hierarchical memory resource management and proactive reclaim for maintaining system responsiveness. Proactive reclaim involves reclaiming memory before it becomes critically low, and it uses algorithms like MGLRU for efficient page replacement. The SWAP_CLUSTER_MAX constant defines the maximum number of pages to be reclaimed in a single batch, which affects the balance between reclaim efficiency and overhead."
    ],
    "kernel_changes": [
        "Before the introduction of this feature, the proactive reclaim process used a fixed batch size of 32 pages (SWAP_CLUSTER_MAX) to reduce overreclaim. However, this small batch size led to performance regressions due to frequent reclaim start/stop cycles. After the implementation of this feature, the batch size for proactive reclaim was increased, which reduced the number of reclaim cycles and improved overall reclaim performance. This change helped in maintaining a balance between the fidelity of nr_to_reclaim and fairness across nodes and cgroups, thereby enhancing the efficiency of memory reclaim operations."
    ],
    "comprehensive_understanding": "This feature enhances the proactive reclaim process in the Linux kernel by using larger batch sizes for memory reclaim operations. By increasing the batch size from the previous fixed limit of 32 pages (SWAP_CLUSTER_MAX), the feature reduces the number of reclaim cycles, thereby improving reclaim performance and system responsiveness. This change is particularly beneficial in scenarios where memory is spread across multiple nodes and cgroups, as it helps in balancing the trade-off between the accuracy of reclaim requests and the fairness of memory distribution. Overall, the use of larger batches for proactive reclaim optimizes memory management, reduces overreclaim, and enhances system performance.",
    "concepts": [
        "memcg",
        "proactive reclaim",
        "MGLRU",
        "SWAP_CLUSTER_MAX",
        "memory reclaim operations",
        "memory management",
        "system responsiveness",
        "overreclaim",
        "nr_to_reclaim fidelity",
        "fairness across nodes and cgroups"
    ]
}
2024-12-03 22:35:57,153 - extractor - INFO - ---------Processing feature 18/815---------
2024-12-03 22:36:51,750 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个 linux 内核的专家。给你一个 linux 内核特性的描述以及其对应的若干 commit 的标题和提交信息，你的任务是为这个特性写一个多方面的扩写版描述。你的扩写需包含以下几个方面：1. 该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)；2. 为了让开发者理解该特性，需要哪些具体的，与这个特性直接相关的背景知识 (background_knowledge)；3. 该特性被实现前后，linux 内核发生了怎样的变化 (kernel_changes)；4. 你对该特性的综合理解 (comprehensive_understanding)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}]
        },
        "output": {
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
        }
    ],
    "input": {
        "feature_description": "Optimize unmap/zap with PTE-mapped THP",
        "commits": [{"commit_subject": "mm/mmu_gather: define ENCODED_PAGE_FLAG_DELAY_RMAP", "commit_message": "Nowadays, encoded pages are only used in mmu_gather handling.  Let's\nupdate the documentation, and define ENCODED_PAGE_BIT_DELAY_RMAP.  While\nat it, rename ENCODE_PAGE_BITS to ENCODED_PAGE_BITS.\n\nIf encoded page pointers would ever be used in other context again, we'd\nlikely want to change the defines to reflect their context (e.g.,\nENCODED_PAGE_FLAG_MMU_GATHER_DELAY_RMAP).  For now, let's keep it simple.\n\nThis is a preparation for using the remaining spare bit to indicate that\nthe next item in an array of encoded pages is a \"nr_pages\" argument and\nnot an encoded page.\n\nLink: https://lkml.kernel.org/r/20240214204435.167852-7-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>\nCc: Arnd Bergmann <arnd@arndb.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Michal Hocko <mhocko@suse.com>\nCc: \"Naveen N. Rao\" <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Peter Zijlstra (Intel) <peterz@infradead.org>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nCc: Yin Fengwei <fengwei.yin@intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/mmu_gather: add tlb_remove_tlb_entries()", "commit_message": "Let's add a helper that lets us batch-process multiple consecutive PTEs.\n\nNote that the loop will get optimized out on all architectures except on\npowerpc.  We have to add an early define of __tlb_remove_tlb_entry() on\nppc to make the compiler happy (and avoid making tlb_remove_tlb_entries()\na macro).\n\n[arnd@kernel.org: change __tlb_remove_tlb_entry() to an inline function]\n  Link: https://lkml.kernel.org/r/20240221154549.2026073-1-arnd@kernel.org\nLink: https://lkml.kernel.org/r/20240214204435.167852-8-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nSigned-off-by: Arnd Bergmann <arnd@arndb.de>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>\nCc: Arnd Bergmann <arnd@arndb.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Michal Hocko <mhocko@suse.com>\nCc: \"Naveen N. Rao\" <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Peter Zijlstra (Intel) <peterz@infradead.org>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nCc: Yin Fengwei <fengwei.yin@intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/memory: handle !page case in zap_present_pte() separately", "commit_message": "We don't need uptodate accessed/dirty bits, so in theory we could replace\nptep_get_and_clear_full() by an optimized ptep_clear_full() function. \nLet's rely on the provided pte.\n\nFurther, there is no scenario where we would have to insert uffd-wp\nmarkers when zapping something that is not a normal page (i.e., zeropage).\nAdd a sanity check to make sure this remains true.\n\nshould_zap_folio() no longer has to handle NULL pointers.  This change\nreplaces 2/3 \"!page/!folio\" checks by a single \"!page\" one.\n\nNote that arch_check_zapped_pte() on x86-64 checks the HW-dirty bit to\ndetect shadow stack entries.  But for shadow stack entries, the HW dirty\nbit (in combination with non-writable PTEs) is set by software.  So for\nthe arch_check_zapped_pte() check, we don't have to sync against HW\nsetting the HW dirty bit concurrently, it is always set.\n\nLink: https://lkml.kernel.org/r/20240214204435.167852-3-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>\nCc: Arnd Bergmann <arnd@arndb.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Michal Hocko <mhocko@suse.com>\nCc: \"Naveen N. Rao\" <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Peter Zijlstra (Intel) <peterz@infradead.org>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nCc: Yin Fengwei <fengwei.yin@intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/memory: further separate anon and pagecache folio handling in zap_present_pte()", "commit_message": "We don't need up-to-date accessed-dirty information for anon folios and\ncan simply work with the ptent we already have.  Also, we know the RSS\ncounter we want to update.\n\nWe can safely move arch_check_zapped_pte() + tlb_remove_tlb_entry() +\nzap_install_uffd_wp_if_needed() after updating the folio and RSS.\n\nWhile at it, only call zap_install_uffd_wp_if_needed() if there is even\nany chance that pte_install_uffd_wp_if_needed() would do *something*. \nThat is, just don't bother if uffd-wp does not apply.\n\nLink: https://lkml.kernel.org/r/20240214204435.167852-4-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>\nCc: Arnd Bergmann <arnd@arndb.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Michal Hocko <mhocko@suse.com>\nCc: \"Naveen N. Rao\" <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Peter Zijlstra (Intel) <peterz@infradead.org>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nCc: Yin Fengwei <fengwei.yin@intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/memory: optimize unmap/zap with PTE-mapped THP", "commit_message": "Similar to how we optimized fork(), let's implement PTE batching when\nconsecutive (present) PTEs map consecutive pages of the same large folio.\n\nMost infrastructure we need for batching (mmu gather, rmap) is already\nthere.  We only have to add get_and_clear_full_ptes() and\nclear_full_ptes().  Similarly, extend zap_install_uffd_wp_if_needed() to\nprocess a PTE range.\n\nWe won't bother sanity-checking the mapcount of all subpages, but only\ncheck the mapcount of the first subpage we process.  If there is a real\nproblem hiding somewhere, we can trigger it simply by using small folios,\nor when we zap single pages of a large folio.  Ideally, we had that check\nin rmap code (including for delayed rmap), but then we cannot print the\nPTE.  Let's keep it simple for now.  If we ever have a cheap\nfolio_mapcount(), we might just want to check for underflows there.\n\nTo keep small folios as fast as possible force inlining of a specialized\nvariant using __always_inline with nr=1.\n\nLink: https://lkml.kernel.org/r/20240214204435.167852-11-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>\nCc: Arnd Bergmann <arnd@arndb.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Michal Hocko <mhocko@suse.com>\nCc: \"Naveen N. Rao\" <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Peter Zijlstra (Intel) <peterz@infradead.org>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nCc: Yin Fengwei <fengwei.yin@intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/memory: factor out zapping of present pte into zap_present_pte()", "commit_message": "Patch series \"mm/memory: optimize unmap/zap with PTE-mapped THP\", v3.\n\nThis series is based on [1].  Similar to what we did with fork(), let's\nimplement PTE batching during unmap/zap when processing PTE-mapped THPs.\n\nWe collect consecutive PTEs that map consecutive pages of the same large\nfolio, making sure that the other PTE bits are compatible, and (a) adjust\nthe refcount only once per batch, (b) call rmap handling functions only\nonce per batch, (c) perform batch PTE setting/updates and (d) perform TLB\nentry removal once per batch.\n\nRyan was previously working on this in the context of cont-pte for arm64,\nint latest iteration [2] with a focus on arm6 with cont-pte only.  This\nseries implements the optimization for all architectures, independent of\nsuch PTE bits, teaches MMU gather/TLB code to be fully aware of such\nlarge-folio-pages batches as well, and amkes use of our new rmap batching\nfunction when removing the rmap.\n\nTo achieve that, we have to enlighten MMU gather / page freeing code\n(i.e., everything that consumes encoded_page) to process unmapping of\nconsecutive pages that all belong to the same large folio.  I'm being very\ncareful to not degrade order-0 performance, and it looks like I managed to\nachieve that.\n\nWhile this series should -- similar to [1] -- be beneficial for adding\ncont-pte support on arm64[2], it's one of the requirements for maintaining\na total mapcount[3] for large folios with minimal added overhead and\nfurther changes[4] that build up on top of the total mapcount.\n\nIndependent of all that, this series results in a speedup during munmap()\nand similar unmapping (process teardown, MADV_DONTNEED on larger ranges)\nwith PTE-mapped THP, which is the default with THPs that are smaller than\na PMD (for example, 16KiB to 1024KiB mTHPs for anonymous memory[5]).\n\nOn an Intel Xeon Silver 4210R CPU, munmap'ing a 1GiB VMA backed by\nPTE-mapped folios of the same size (stddev < 1%) results in the following\nruntimes for munmap() in seconds (shorter is better):\n\nFolio Size | mm-unstable |      New | Change\n---------------------------------------------\n      4KiB |    0.058110 | 0.057715 |   - 1%\n     16KiB |    0.044198 | 0.035469 |   -20%\n     32KiB |    0.034216 | 0.023522 |   -31%\n     64KiB |    0.029207 | 0.018434 |   -37%\n    128KiB |    0.026579 | 0.014026 |   -47%\n    256KiB |    0.025130 | 0.011756 |   -53%\n    512KiB |    0.024292 | 0.010703 |   -56%\n   1024KiB |    0.023812 | 0.010294 |   -57%\n   2048KiB |    0.023785 | 0.009910 |   -58%\n\n[1] https://lkml.kernel.org/r/20240129124649.189745-1-david@redhat.com\n[2] https://lkml.kernel.org/r/20231218105100.172635-1-ryan.roberts@arm.com\n[3] https://lkml.kernel.org/r/20230809083256.699513-1-david@redhat.com\n[4] https://lkml.kernel.org/r/20231124132626.235350-1-david@redhat.com\n[5] https://lkml.kernel.org/r/20231207161211.2374093-1-ryan.roberts@arm.com\n\n\nThis patch (of 10):\n\nLet's prepare for further changes by factoring out processing of present\nPTEs.\n\nLink: https://lkml.kernel.org/r/20240214204435.167852-1-david@redhat.com\nLink: https://lkml.kernel.org/r/20240214204435.167852-2-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>\nCc: Arnd Bergmann <arnd@arndb.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: linuxppc-dev@lists.ozlabs.org\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Michal Hocko <mhocko@suse.com>\nCc: \"Naveen N. Rao\" <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Peter Zijlstra (Intel) <peterz@infradead.org>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nCc: Yin Fengwei <fengwei.yin@intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/mmu_gather: add __tlb_remove_folio_pages()", "commit_message": "Add __tlb_remove_folio_pages(), which will remove multiple consecutive\npages that belong to the same large folio, instead of only a single page. \nWe'll be using this function when optimizing unmapping/zapping of large\nfolios that are mapped by PTEs.\n\nWe're using the remaining spare bit in an encoded_page to indicate that\nthe next enoced page in an array contains actually shifted \"nr_pages\". \nTeach swap/freeing code about putting multiple folio references, and\ndelayed rmap handling to remove page ranges of a folio.\n\nThis extension allows for still gathering almost as many small folios as\nwe used to (-1, because we have to prepare for a possibly bigger next\nentry), but still allows for gathering consecutive pages that belong to\nthe same large folio.\n\nNote that we don't pass the folio pointer, because it is not required for\nnow.  Further, we don't support page_size != PAGE_SIZE, it won't be\nrequired for simple PTE batching.\n\nWe have to provide a separate s390 implementation, but it's fairly\nstraight forward.\n\nAnother, more invasive and likely more expensive, approach would be to use\nfolio+range or a PFN range instead of page+nr_pages.  But, we should do\nthat consistently for the whole mmu_gather.  For now, let's keep it simple\nand add \"nr_pages\" only.\n\nNote that it is now possible to gather significantly more pages: In the\npast, we were able to gather ~10000 pages, now we can also gather ~5000\nfolio fragments that span multiple pages.  A folio fragment on x86-64 can\nspan up to 512 pages (2 MiB THP) and on arm64 with 64k in theory 8192\npages (512 MiB THP).  Gathering more memory is not considered something we\nshould worry about, especially because these are already corner cases.\n\nWhile we can gather more total memory, we won't free more folio fragments.\nAs long as page freeing time primarily only depends on the number of\ninvolved folios, there is no effective change for !preempt configurations.\nHowever, we'll adjust tlb_batch_pages_flush() separately to handle corner\ncases where page freeing time grows proportionally with the actual memory\nsize.\n\nLink: https://lkml.kernel.org/r/20240214204435.167852-9-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>\nCc: Arnd Bergmann <arnd@arndb.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Michal Hocko <mhocko@suse.com>\nCc: \"Naveen N. Rao\" <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Peter Zijlstra (Intel) <peterz@infradead.org>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nCc: Yin Fengwei <fengwei.yin@intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/mmu_gather: pass \"delay_rmap\" instead of encoded page to __tlb_remove_page_size()", "commit_message": "We have two bits available in the encoded page pointer to store additional\ninformation.  Currently, we use one bit to request delay of the rmap\nremoval until after a TLB flush.\n\nWe want to make use of the remaining bit internally for batching of\nmultiple pages of the same folio, specifying that the next encoded page\npointer in an array is actually \"nr_pages\".  So pass page + delay_rmap\nflag instead of an encoded page, to handle the encoding internally.\n\nLink: https://lkml.kernel.org/r/20240214204435.167852-6-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>\nCc: Arnd Bergmann <arnd@arndb.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Michal Hocko <mhocko@suse.com>\nCc: \"Naveen N. Rao\" <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Peter Zijlstra (Intel) <peterz@infradead.org>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nCc: Yin Fengwei <fengwei.yin@intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/memory: factor out zapping folio pte into zap_present_folio_pte()", "commit_message": "Let's prepare for further changes by factoring it out into a separate\nfunction.\n\nLink: https://lkml.kernel.org/r/20240214204435.167852-5-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>\nCc: Arnd Bergmann <arnd@arndb.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Michal Hocko <mhocko@suse.com>\nCc: \"Naveen N. Rao\" <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Peter Zijlstra (Intel) <peterz@infradead.org>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nCc: Yin Fengwei <fengwei.yin@intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/mmu_gather: improve cond_resched() handling with large folios and expensive page freeing", "commit_message": "In tlb_batch_pages_flush(), we can end up freeing up to 512 pages or now\nup to 256 folio fragments that span more than one page, before we\nconditionally reschedule.\n\nIt's a pain that we have to handle cond_resched() in\ntlb_batch_pages_flush() manually and cannot simply handle it in\nrelease_pages() -- release_pages() can be called from atomic context. \nWell, in a perfect world we wouldn't have to make our code more\ncomplicated at all.\n\nWith page poisoning and init_on_free, we might now run into soft lockups\nwhen we free a lot of rather large folio fragments, because page freeing\ntime then depends on the actual memory size we are freeing instead of on\nthe number of folios that are involved.\n\nIn the absolute (unlikely) worst case, on arm64 with 64k we will be able\nto free up to 256 folio fragments that each span 512 MiB: zeroing out 128\nGiB does sound like it might take a while.  But instead of ignoring this\nunlikely case, let's just handle it.\n\nSo, let's teach tlb_batch_pages_flush() that there are some configurations\nwhere page freeing is horribly slow, and let's reschedule more frequently\n-- similarly like we did for now before we had large folio fragments in\nthere.  Avoid yet another loop over all encoded pages in the common case\nby handling that separately.\n\nNote that with page poisoning/zeroing, we might now end up freeing only a\nsingle folio fragment at a time that might exceed the old 512 pages limit:\nbut if we cannot even free a single MAX_ORDER page on a system without\nrunning into soft lockups, something else is already completely bogus. \nFreeing a PMD-mapped THP would similarly cause trouble.\n\nIn theory, we might even free 511 order-0 pages + a single MAX_ORDER page,\neffectively having to zero out 8703 pages on arm64 with 64k, translating\nto ~544 MiB of memory: however, if 512 MiB doesn't result in soft lockups,\n544 MiB is unlikely to result in soft lockups, so we won't care about that\nfor the time being.\n\nIn the future, we might want to detect if handling cond_resched() is\nrequired at all, and just not do any of that with full preemption enabled.\n\nLink: https://lkml.kernel.org/r/20240214204435.167852-10-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>\nCc: Arnd Bergmann <arnd@arndb.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Michal Hocko <mhocko@suse.com>\nCc: \"Naveen N. Rao\" <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Peter Zijlstra (Intel) <peterz@infradead.org>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nCc: Yin Fengwei <fengwei.yin@intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}]
    }
}

2024-12-03 22:37:33,621 - extractor - DEBUG - deepseek response:
ChatCompletion(id='626d79da-1d5a-4d0f-be7b-5c1a3b83178e', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "proper_nouns": [\n        {\n            "PTE": "PTE stands for Page Table Entry, which is a data structure used in the Linux kernel to manage virtual-to-physical memory mappings. Each PTE corresponds to a single memory page and contains information such as the physical address of the page, access permissions, and status bits."\n        },\n        {\n            "THP": "THP, or Transparent Huge Pages, is a feature in the Linux kernel that allows the use of larger memory pages (typically 2MB or 1GB) to reduce the overhead of managing smaller pages (usually 4KB). This can improve performance by reducing the number of page table entries and TLB (Translation Lookaside Buffer) misses."\n        },\n        {\n            "MMU": "MMU stands for Memory Management Unit, a hardware component responsible for handling memory access requests and translating virtual addresses to physical addresses. The MMU is crucial for implementing features like virtual memory, memory protection, and caching."\n        },\n        {\n            "TLB": "TLB, or Translation Lookaside Buffer, is a cache used by the MMU to store recent translations of virtual addresses to physical addresses. This reduces the time needed to access memory by avoiding the need to consult the full page table for every memory access."\n        },\n        {\n            "RMAP": "RMAP is a reverse mapping mechanism used in the Linux kernel to keep track of which processes are using a particular page of memory. This is essential for efficient memory management, especially during operations like page freeing and migration."\n        }\n    ],\n    "background_knowledge": [\n        "The optimization of unmap/zap operations with PTE-mapped THP involves batching consecutive PTEs that map consecutive pages of the same large folio. This batching reduces the number of operations required to manage these pages, thereby improving performance.",\n        "Understanding the existing infrastructure for batching in the Linux kernel, such as MMU gather and RMAP, is crucial for grasping how this optimization works. These mechanisms are already in place to handle large-scale memory operations efficiently.",\n        "The concept of \'encoded pages\' is used in the MMU gather handling to manage memory pages more efficiently. This involves encoding additional information into page pointers to optimize memory management operations."\n    ],\n    "kernel_changes": [\n        "Before this optimization, unmap/zap operations with PTE-mapped THP were less efficient due to the need for multiple operations to manage each page individually. This led to increased overhead and reduced performance.",\n        "After the implementation, the kernel was modified to batch consecutive PTEs that map consecutive pages of the same large folio. This batching reduces the number of operations required, leading to faster unmap/zap operations and improved overall performance.",\n        "The introduction of new helper functions like tlb_remove_tlb_entries() and __tlb_remove_folio_pages() allows for more efficient handling of large folios, further enhancing the performance of memory management operations."\n    ],\n    "comprehensive_understanding": "The optimization of unmap/zap operations with PTE-mapped THP significantly improves the efficiency of memory management in the Linux kernel. By batching consecutive PTEs that map consecutive pages of the same large folio, the kernel reduces the number of operations required to manage these pages. This not only speeds up unmap/zap operations but also improves overall system performance by minimizing overhead and reducing the load on the MMU and TLB. The introduction of new helper functions and the use of encoded pages further enhance the efficiency of these operations, making the Linux kernel\'s memory management more robust and performant."\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733236611, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=791, prompt_tokens=9631, total_tokens=10422, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=1472, prompt_cache_miss_tokens=8159))
2024-12-03 22:37:33,621 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个命名实体抽取 (NER) 任务的专家，同时也是 linux 内核的专家。给你一个 linux 内核特性的描述 (feature_description)、对应的若干 commit 的标题和提交信息 (commits)、该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)、为了让开发者理解该特性所需的与这个特性直接相关的背景知识 (background_knowledge)、该特性被实现前后，linux 内核发生的变化 (kernel_changes)、对该特性的综合理解 (comprehensive_understanding)。你的任务是：根据这些信息提取出在 linux 内核中涉及到的和这个特性相关的概念实体 (concepts)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}],
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": [
                "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
            ]
        },
        "output": {
            "concepts": ["Copy-on-Write (CoW)", "mTHP", "folio", "memory reclamation", "memory availability", "anti-fragmentation"]
        }
    ],
    "input": {
        "feature_description": "Optimize unmap/zap with PTE-mapped THP",
        "commits": [{"commit_subject": "mm/mmu_gather: define ENCODED_PAGE_FLAG_DELAY_RMAP", "commit_message": "Nowadays, encoded pages are only used in mmu_gather handling.  Let's\nupdate the documentation, and define ENCODED_PAGE_BIT_DELAY_RMAP.  While\nat it, rename ENCODE_PAGE_BITS to ENCODED_PAGE_BITS.\n\nIf encoded page pointers would ever be used in other context again, we'd\nlikely want to change the defines to reflect their context (e.g.,\nENCODED_PAGE_FLAG_MMU_GATHER_DELAY_RMAP).  For now, let's keep it simple.\n\nThis is a preparation for using the remaining spare bit to indicate that\nthe next item in an array of encoded pages is a \"nr_pages\" argument and\nnot an encoded page.\n\nLink: https://lkml.kernel.org/r/20240214204435.167852-7-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>\nCc: Arnd Bergmann <arnd@arndb.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Michal Hocko <mhocko@suse.com>\nCc: \"Naveen N. Rao\" <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Peter Zijlstra (Intel) <peterz@infradead.org>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nCc: Yin Fengwei <fengwei.yin@intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/mmu_gather: add tlb_remove_tlb_entries()", "commit_message": "Let's add a helper that lets us batch-process multiple consecutive PTEs.\n\nNote that the loop will get optimized out on all architectures except on\npowerpc.  We have to add an early define of __tlb_remove_tlb_entry() on\nppc to make the compiler happy (and avoid making tlb_remove_tlb_entries()\na macro).\n\n[arnd@kernel.org: change __tlb_remove_tlb_entry() to an inline function]\n  Link: https://lkml.kernel.org/r/20240221154549.2026073-1-arnd@kernel.org\nLink: https://lkml.kernel.org/r/20240214204435.167852-8-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nSigned-off-by: Arnd Bergmann <arnd@arndb.de>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>\nCc: Arnd Bergmann <arnd@arndb.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Michal Hocko <mhocko@suse.com>\nCc: \"Naveen N. Rao\" <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Peter Zijlstra (Intel) <peterz@infradead.org>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nCc: Yin Fengwei <fengwei.yin@intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/memory: handle !page case in zap_present_pte() separately", "commit_message": "We don't need uptodate accessed/dirty bits, so in theory we could replace\nptep_get_and_clear_full() by an optimized ptep_clear_full() function. \nLet's rely on the provided pte.\n\nFurther, there is no scenario where we would have to insert uffd-wp\nmarkers when zapping something that is not a normal page (i.e., zeropage).\nAdd a sanity check to make sure this remains true.\n\nshould_zap_folio() no longer has to handle NULL pointers.  This change\nreplaces 2/3 \"!page/!folio\" checks by a single \"!page\" one.\n\nNote that arch_check_zapped_pte() on x86-64 checks the HW-dirty bit to\ndetect shadow stack entries.  But for shadow stack entries, the HW dirty\nbit (in combination with non-writable PTEs) is set by software.  So for\nthe arch_check_zapped_pte() check, we don't have to sync against HW\nsetting the HW dirty bit concurrently, it is always set.\n\nLink: https://lkml.kernel.org/r/20240214204435.167852-3-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>\nCc: Arnd Bergmann <arnd@arndb.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Michal Hocko <mhocko@suse.com>\nCc: \"Naveen N. Rao\" <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Peter Zijlstra (Intel) <peterz@infradead.org>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nCc: Yin Fengwei <fengwei.yin@intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/memory: further separate anon and pagecache folio handling in zap_present_pte()", "commit_message": "We don't need up-to-date accessed-dirty information for anon folios and\ncan simply work with the ptent we already have.  Also, we know the RSS\ncounter we want to update.\n\nWe can safely move arch_check_zapped_pte() + tlb_remove_tlb_entry() +\nzap_install_uffd_wp_if_needed() after updating the folio and RSS.\n\nWhile at it, only call zap_install_uffd_wp_if_needed() if there is even\nany chance that pte_install_uffd_wp_if_needed() would do *something*. \nThat is, just don't bother if uffd-wp does not apply.\n\nLink: https://lkml.kernel.org/r/20240214204435.167852-4-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>\nCc: Arnd Bergmann <arnd@arndb.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Michal Hocko <mhocko@suse.com>\nCc: \"Naveen N. Rao\" <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Peter Zijlstra (Intel) <peterz@infradead.org>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nCc: Yin Fengwei <fengwei.yin@intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/memory: optimize unmap/zap with PTE-mapped THP", "commit_message": "Similar to how we optimized fork(), let's implement PTE batching when\nconsecutive (present) PTEs map consecutive pages of the same large folio.\n\nMost infrastructure we need for batching (mmu gather, rmap) is already\nthere.  We only have to add get_and_clear_full_ptes() and\nclear_full_ptes().  Similarly, extend zap_install_uffd_wp_if_needed() to\nprocess a PTE range.\n\nWe won't bother sanity-checking the mapcount of all subpages, but only\ncheck the mapcount of the first subpage we process.  If there is a real\nproblem hiding somewhere, we can trigger it simply by using small folios,\nor when we zap single pages of a large folio.  Ideally, we had that check\nin rmap code (including for delayed rmap), but then we cannot print the\nPTE.  Let's keep it simple for now.  If we ever have a cheap\nfolio_mapcount(), we might just want to check for underflows there.\n\nTo keep small folios as fast as possible force inlining of a specialized\nvariant using __always_inline with nr=1.\n\nLink: https://lkml.kernel.org/r/20240214204435.167852-11-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>\nCc: Arnd Bergmann <arnd@arndb.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Michal Hocko <mhocko@suse.com>\nCc: \"Naveen N. Rao\" <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Peter Zijlstra (Intel) <peterz@infradead.org>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nCc: Yin Fengwei <fengwei.yin@intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/memory: factor out zapping of present pte into zap_present_pte()", "commit_message": "Patch series \"mm/memory: optimize unmap/zap with PTE-mapped THP\", v3.\n\nThis series is based on [1].  Similar to what we did with fork(), let's\nimplement PTE batching during unmap/zap when processing PTE-mapped THPs.\n\nWe collect consecutive PTEs that map consecutive pages of the same large\nfolio, making sure that the other PTE bits are compatible, and (a) adjust\nthe refcount only once per batch, (b) call rmap handling functions only\nonce per batch, (c) perform batch PTE setting/updates and (d) perform TLB\nentry removal once per batch.\n\nRyan was previously working on this in the context of cont-pte for arm64,\nint latest iteration [2] with a focus on arm6 with cont-pte only.  This\nseries implements the optimization for all architectures, independent of\nsuch PTE bits, teaches MMU gather/TLB code to be fully aware of such\nlarge-folio-pages batches as well, and amkes use of our new rmap batching\nfunction when removing the rmap.\n\nTo achieve that, we have to enlighten MMU gather / page freeing code\n(i.e., everything that consumes encoded_page) to process unmapping of\nconsecutive pages that all belong to the same large folio.  I'm being very\ncareful to not degrade order-0 performance, and it looks like I managed to\nachieve that.\n\nWhile this series should -- similar to [1] -- be beneficial for adding\ncont-pte support on arm64[2], it's one of the requirements for maintaining\na total mapcount[3] for large folios with minimal added overhead and\nfurther changes[4] that build up on top of the total mapcount.\n\nIndependent of all that, this series results in a speedup during munmap()\nand similar unmapping (process teardown, MADV_DONTNEED on larger ranges)\nwith PTE-mapped THP, which is the default with THPs that are smaller than\na PMD (for example, 16KiB to 1024KiB mTHPs for anonymous memory[5]).\n\nOn an Intel Xeon Silver 4210R CPU, munmap'ing a 1GiB VMA backed by\nPTE-mapped folios of the same size (stddev < 1%) results in the following\nruntimes for munmap() in seconds (shorter is better):\n\nFolio Size | mm-unstable |      New | Change\n---------------------------------------------\n      4KiB |    0.058110 | 0.057715 |   - 1%\n     16KiB |    0.044198 | 0.035469 |   -20%\n     32KiB |    0.034216 | 0.023522 |   -31%\n     64KiB |    0.029207 | 0.018434 |   -37%\n    128KiB |    0.026579 | 0.014026 |   -47%\n    256KiB |    0.025130 | 0.011756 |   -53%\n    512KiB |    0.024292 | 0.010703 |   -56%\n   1024KiB |    0.023812 | 0.010294 |   -57%\n   2048KiB |    0.023785 | 0.009910 |   -58%\n\n[1] https://lkml.kernel.org/r/20240129124649.189745-1-david@redhat.com\n[2] https://lkml.kernel.org/r/20231218105100.172635-1-ryan.roberts@arm.com\n[3] https://lkml.kernel.org/r/20230809083256.699513-1-david@redhat.com\n[4] https://lkml.kernel.org/r/20231124132626.235350-1-david@redhat.com\n[5] https://lkml.kernel.org/r/20231207161211.2374093-1-ryan.roberts@arm.com\n\n\nThis patch (of 10):\n\nLet's prepare for further changes by factoring out processing of present\nPTEs.\n\nLink: https://lkml.kernel.org/r/20240214204435.167852-1-david@redhat.com\nLink: https://lkml.kernel.org/r/20240214204435.167852-2-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>\nCc: Arnd Bergmann <arnd@arndb.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: linuxppc-dev@lists.ozlabs.org\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Michal Hocko <mhocko@suse.com>\nCc: \"Naveen N. Rao\" <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Peter Zijlstra (Intel) <peterz@infradead.org>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nCc: Yin Fengwei <fengwei.yin@intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/mmu_gather: add __tlb_remove_folio_pages()", "commit_message": "Add __tlb_remove_folio_pages(), which will remove multiple consecutive\npages that belong to the same large folio, instead of only a single page. \nWe'll be using this function when optimizing unmapping/zapping of large\nfolios that are mapped by PTEs.\n\nWe're using the remaining spare bit in an encoded_page to indicate that\nthe next enoced page in an array contains actually shifted \"nr_pages\". \nTeach swap/freeing code about putting multiple folio references, and\ndelayed rmap handling to remove page ranges of a folio.\n\nThis extension allows for still gathering almost as many small folios as\nwe used to (-1, because we have to prepare for a possibly bigger next\nentry), but still allows for gathering consecutive pages that belong to\nthe same large folio.\n\nNote that we don't pass the folio pointer, because it is not required for\nnow.  Further, we don't support page_size != PAGE_SIZE, it won't be\nrequired for simple PTE batching.\n\nWe have to provide a separate s390 implementation, but it's fairly\nstraight forward.\n\nAnother, more invasive and likely more expensive, approach would be to use\nfolio+range or a PFN range instead of page+nr_pages.  But, we should do\nthat consistently for the whole mmu_gather.  For now, let's keep it simple\nand add \"nr_pages\" only.\n\nNote that it is now possible to gather significantly more pages: In the\npast, we were able to gather ~10000 pages, now we can also gather ~5000\nfolio fragments that span multiple pages.  A folio fragment on x86-64 can\nspan up to 512 pages (2 MiB THP) and on arm64 with 64k in theory 8192\npages (512 MiB THP).  Gathering more memory is not considered something we\nshould worry about, especially because these are already corner cases.\n\nWhile we can gather more total memory, we won't free more folio fragments.\nAs long as page freeing time primarily only depends on the number of\ninvolved folios, there is no effective change for !preempt configurations.\nHowever, we'll adjust tlb_batch_pages_flush() separately to handle corner\ncases where page freeing time grows proportionally with the actual memory\nsize.\n\nLink: https://lkml.kernel.org/r/20240214204435.167852-9-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>\nCc: Arnd Bergmann <arnd@arndb.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Michal Hocko <mhocko@suse.com>\nCc: \"Naveen N. Rao\" <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Peter Zijlstra (Intel) <peterz@infradead.org>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nCc: Yin Fengwei <fengwei.yin@intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/mmu_gather: pass \"delay_rmap\" instead of encoded page to __tlb_remove_page_size()", "commit_message": "We have two bits available in the encoded page pointer to store additional\ninformation.  Currently, we use one bit to request delay of the rmap\nremoval until after a TLB flush.\n\nWe want to make use of the remaining bit internally for batching of\nmultiple pages of the same folio, specifying that the next encoded page\npointer in an array is actually \"nr_pages\".  So pass page + delay_rmap\nflag instead of an encoded page, to handle the encoding internally.\n\nLink: https://lkml.kernel.org/r/20240214204435.167852-6-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>\nCc: Arnd Bergmann <arnd@arndb.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Michal Hocko <mhocko@suse.com>\nCc: \"Naveen N. Rao\" <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Peter Zijlstra (Intel) <peterz@infradead.org>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nCc: Yin Fengwei <fengwei.yin@intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/memory: factor out zapping folio pte into zap_present_folio_pte()", "commit_message": "Let's prepare for further changes by factoring it out into a separate\nfunction.\n\nLink: https://lkml.kernel.org/r/20240214204435.167852-5-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>\nCc: Arnd Bergmann <arnd@arndb.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Michal Hocko <mhocko@suse.com>\nCc: \"Naveen N. Rao\" <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Peter Zijlstra (Intel) <peterz@infradead.org>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nCc: Yin Fengwei <fengwei.yin@intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/mmu_gather: improve cond_resched() handling with large folios and expensive page freeing", "commit_message": "In tlb_batch_pages_flush(), we can end up freeing up to 512 pages or now\nup to 256 folio fragments that span more than one page, before we\nconditionally reschedule.\n\nIt's a pain that we have to handle cond_resched() in\ntlb_batch_pages_flush() manually and cannot simply handle it in\nrelease_pages() -- release_pages() can be called from atomic context. \nWell, in a perfect world we wouldn't have to make our code more\ncomplicated at all.\n\nWith page poisoning and init_on_free, we might now run into soft lockups\nwhen we free a lot of rather large folio fragments, because page freeing\ntime then depends on the actual memory size we are freeing instead of on\nthe number of folios that are involved.\n\nIn the absolute (unlikely) worst case, on arm64 with 64k we will be able\nto free up to 256 folio fragments that each span 512 MiB: zeroing out 128\nGiB does sound like it might take a while.  But instead of ignoring this\nunlikely case, let's just handle it.\n\nSo, let's teach tlb_batch_pages_flush() that there are some configurations\nwhere page freeing is horribly slow, and let's reschedule more frequently\n-- similarly like we did for now before we had large folio fragments in\nthere.  Avoid yet another loop over all encoded pages in the common case\nby handling that separately.\n\nNote that with page poisoning/zeroing, we might now end up freeing only a\nsingle folio fragment at a time that might exceed the old 512 pages limit:\nbut if we cannot even free a single MAX_ORDER page on a system without\nrunning into soft lockups, something else is already completely bogus. \nFreeing a PMD-mapped THP would similarly cause trouble.\n\nIn theory, we might even free 511 order-0 pages + a single MAX_ORDER page,\neffectively having to zero out 8703 pages on arm64 with 64k, translating\nto ~544 MiB of memory: however, if 512 MiB doesn't result in soft lockups,\n544 MiB is unlikely to result in soft lockups, so we won't care about that\nfor the time being.\n\nIn the future, we might want to detect if handling cond_resched() is\nrequired at all, and just not do any of that with full preemption enabled.\n\nLink: https://lkml.kernel.org/r/20240214204435.167852-10-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>\nCc: Arnd Bergmann <arnd@arndb.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Michal Hocko <mhocko@suse.com>\nCc: \"Naveen N. Rao\" <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Peter Zijlstra (Intel) <peterz@infradead.org>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nCc: Yin Fengwei <fengwei.yin@intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}],
        "proper_nouns": [{"PTE": "PTE stands for Page Table Entry, which is a data structure used in the Linux kernel to manage virtual-to-physical memory mappings. Each PTE corresponds to a single memory page and contains information such as the physical address of the page, access permissions, and status bits."}, {"THP": "THP, or Transparent Huge Pages, is a feature in the Linux kernel that allows the use of larger memory pages (typically 2MB or 1GB) to reduce the overhead of managing smaller pages (usually 4KB). This can improve performance by reducing the number of page table entries and TLB (Translation Lookaside Buffer) misses."}, {"MMU": "MMU stands for Memory Management Unit, a hardware component responsible for handling memory access requests and translating virtual addresses to physical addresses. The MMU is crucial for implementing features like virtual memory, memory protection, and caching."}, {"TLB": "TLB, or Translation Lookaside Buffer, is a cache used by the MMU to store recent translations of virtual addresses to physical addresses. This reduces the time needed to access memory by avoiding the need to consult the full page table for every memory access."}, {"RMAP": "RMAP is a reverse mapping mechanism used in the Linux kernel to keep track of which processes are using a particular page of memory. This is essential for efficient memory management, especially during operations like page freeing and migration."}],
        "background_knowledge": ["The optimization of unmap/zap operations with PTE-mapped THP involves batching consecutive PTEs that map consecutive pages of the same large folio. This batching reduces the number of operations required to manage these pages, thereby improving performance.", "Understanding the existing infrastructure for batching in the Linux kernel, such as MMU gather and RMAP, is crucial for grasping how this optimization works. These mechanisms are already in place to handle large-scale memory operations efficiently.", "The concept of 'encoded pages' is used in the MMU gather handling to manage memory pages more efficiently. This involves encoding additional information into page pointers to optimize memory management operations."],
        "kernel_changes": ["Before this optimization, unmap/zap operations with PTE-mapped THP were less efficient due to the need for multiple operations to manage each page individually. This led to increased overhead and reduced performance.", "After the implementation, the kernel was modified to batch consecutive PTEs that map consecutive pages of the same large folio. This batching reduces the number of operations required, leading to faster unmap/zap operations and improved overall performance.", "The introduction of new helper functions like tlb_remove_tlb_entries() and __tlb_remove_folio_pages() allows for more efficient handling of large folios, further enhancing the performance of memory management operations."],
        "comprehensive_understanding": "The optimization of unmap/zap operations with PTE-mapped THP significantly improves the efficiency of memory management in the Linux kernel. By batching consecutive PTEs that map consecutive pages of the same large folio, the kernel reduces the number of operations required to manage these pages. This not only speeds up unmap/zap operations but also improves overall system performance by minimizing overhead and reducing the load on the MMU and TLB. The introduction of new helper functions and the use of encoded pages further enhance the efficiency of these operations, making the Linux kernel's memory management more robust and performant."
    }
}

2024-12-03 22:37:38,106 - extractor - DEBUG - deepseek response:
ChatCompletion(id='bee5cb6d-ad1b-47e2-805b-148a9931b8cb', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "concepts": ["PTE", "THP", "MMU", "TLB", "RMAP", "encoded pages", "batching", "large folio", "unmap/zap operations", "memory management", "performance optimization"]\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733236653, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=62, prompt_tokens=10414, total_tokens=10476, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=1536, prompt_cache_miss_tokens=8878))
2024-12-03 22:37:38,107 - extractor - DEBUG - feature_extracted of feature_id=33222:
{
    "feature_id": 33222,
    "feature_description": "Optimize unmap/zap with PTE-mapped THP",
    "version": "6.9",
    "commits": [
        {
            "commit_subject": "mm/mmu_gather: define ENCODED_PAGE_FLAG_DELAY_RMAP",
            "commit_message": "Nowadays, encoded pages are only used in mmu_gather handling.  Let's\nupdate the documentation, and define ENCODED_PAGE_BIT_DELAY_RMAP.  While\nat it, rename ENCODE_PAGE_BITS to ENCODED_PAGE_BITS.\n\nIf encoded page pointers would ever be used in other context again, we'd\nlikely want to change the defines to reflect their context (e.g.,\nENCODED_PAGE_FLAG_MMU_GATHER_DELAY_RMAP).  For now, let's keep it simple.\n\nThis is a preparation for using the remaining spare bit to indicate that\nthe next item in an array of encoded pages is a \"nr_pages\" argument and\nnot an encoded page.\n\nLink: https://lkml.kernel.org/r/20240214204435.167852-7-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>\nCc: Arnd Bergmann <arnd@arndb.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Michal Hocko <mhocko@suse.com>\nCc: \"Naveen N. Rao\" <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Peter Zijlstra (Intel) <peterz@infradead.org>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nCc: Yin Fengwei <fengwei.yin@intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm/mmu_gather: add tlb_remove_tlb_entries()",
            "commit_message": "Let's add a helper that lets us batch-process multiple consecutive PTEs.\n\nNote that the loop will get optimized out on all architectures except on\npowerpc.  We have to add an early define of __tlb_remove_tlb_entry() on\nppc to make the compiler happy (and avoid making tlb_remove_tlb_entries()\na macro).\n\n[arnd@kernel.org: change __tlb_remove_tlb_entry() to an inline function]\n  Link: https://lkml.kernel.org/r/20240221154549.2026073-1-arnd@kernel.org\nLink: https://lkml.kernel.org/r/20240214204435.167852-8-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nSigned-off-by: Arnd Bergmann <arnd@arndb.de>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>\nCc: Arnd Bergmann <arnd@arndb.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Michal Hocko <mhocko@suse.com>\nCc: \"Naveen N. Rao\" <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Peter Zijlstra (Intel) <peterz@infradead.org>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nCc: Yin Fengwei <fengwei.yin@intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm/memory: handle !page case in zap_present_pte() separately",
            "commit_message": "We don't need uptodate accessed/dirty bits, so in theory we could replace\nptep_get_and_clear_full() by an optimized ptep_clear_full() function. \nLet's rely on the provided pte.\n\nFurther, there is no scenario where we would have to insert uffd-wp\nmarkers when zapping something that is not a normal page (i.e., zeropage).\nAdd a sanity check to make sure this remains true.\n\nshould_zap_folio() no longer has to handle NULL pointers.  This change\nreplaces 2/3 \"!page/!folio\" checks by a single \"!page\" one.\n\nNote that arch_check_zapped_pte() on x86-64 checks the HW-dirty bit to\ndetect shadow stack entries.  But for shadow stack entries, the HW dirty\nbit (in combination with non-writable PTEs) is set by software.  So for\nthe arch_check_zapped_pte() check, we don't have to sync against HW\nsetting the HW dirty bit concurrently, it is always set.\n\nLink: https://lkml.kernel.org/r/20240214204435.167852-3-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>\nCc: Arnd Bergmann <arnd@arndb.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Michal Hocko <mhocko@suse.com>\nCc: \"Naveen N. Rao\" <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Peter Zijlstra (Intel) <peterz@infradead.org>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nCc: Yin Fengwei <fengwei.yin@intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm/memory: further separate anon and pagecache folio handling in zap_present_pte()",
            "commit_message": "We don't need up-to-date accessed-dirty information for anon folios and\ncan simply work with the ptent we already have.  Also, we know the RSS\ncounter we want to update.\n\nWe can safely move arch_check_zapped_pte() + tlb_remove_tlb_entry() +\nzap_install_uffd_wp_if_needed() after updating the folio and RSS.\n\nWhile at it, only call zap_install_uffd_wp_if_needed() if there is even\nany chance that pte_install_uffd_wp_if_needed() would do *something*. \nThat is, just don't bother if uffd-wp does not apply.\n\nLink: https://lkml.kernel.org/r/20240214204435.167852-4-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>\nCc: Arnd Bergmann <arnd@arndb.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Michal Hocko <mhocko@suse.com>\nCc: \"Naveen N. Rao\" <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Peter Zijlstra (Intel) <peterz@infradead.org>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nCc: Yin Fengwei <fengwei.yin@intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm/memory: optimize unmap/zap with PTE-mapped THP",
            "commit_message": "Similar to how we optimized fork(), let's implement PTE batching when\nconsecutive (present) PTEs map consecutive pages of the same large folio.\n\nMost infrastructure we need for batching (mmu gather, rmap) is already\nthere.  We only have to add get_and_clear_full_ptes() and\nclear_full_ptes().  Similarly, extend zap_install_uffd_wp_if_needed() to\nprocess a PTE range.\n\nWe won't bother sanity-checking the mapcount of all subpages, but only\ncheck the mapcount of the first subpage we process.  If there is a real\nproblem hiding somewhere, we can trigger it simply by using small folios,\nor when we zap single pages of a large folio.  Ideally, we had that check\nin rmap code (including for delayed rmap), but then we cannot print the\nPTE.  Let's keep it simple for now.  If we ever have a cheap\nfolio_mapcount(), we might just want to check for underflows there.\n\nTo keep small folios as fast as possible force inlining of a specialized\nvariant using __always_inline with nr=1.\n\nLink: https://lkml.kernel.org/r/20240214204435.167852-11-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>\nCc: Arnd Bergmann <arnd@arndb.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Michal Hocko <mhocko@suse.com>\nCc: \"Naveen N. Rao\" <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Peter Zijlstra (Intel) <peterz@infradead.org>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nCc: Yin Fengwei <fengwei.yin@intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm/memory: factor out zapping of present pte into zap_present_pte()",
            "commit_message": "Patch series \"mm/memory: optimize unmap/zap with PTE-mapped THP\", v3.\n\nThis series is based on [1].  Similar to what we did with fork(), let's\nimplement PTE batching during unmap/zap when processing PTE-mapped THPs.\n\nWe collect consecutive PTEs that map consecutive pages of the same large\nfolio, making sure that the other PTE bits are compatible, and (a) adjust\nthe refcount only once per batch, (b) call rmap handling functions only\nonce per batch, (c) perform batch PTE setting/updates and (d) perform TLB\nentry removal once per batch.\n\nRyan was previously working on this in the context of cont-pte for arm64,\nint latest iteration [2] with a focus on arm6 with cont-pte only.  This\nseries implements the optimization for all architectures, independent of\nsuch PTE bits, teaches MMU gather/TLB code to be fully aware of such\nlarge-folio-pages batches as well, and amkes use of our new rmap batching\nfunction when removing the rmap.\n\nTo achieve that, we have to enlighten MMU gather / page freeing code\n(i.e., everything that consumes encoded_page) to process unmapping of\nconsecutive pages that all belong to the same large folio.  I'm being very\ncareful to not degrade order-0 performance, and it looks like I managed to\nachieve that.\n\nWhile this series should -- similar to [1] -- be beneficial for adding\ncont-pte support on arm64[2], it's one of the requirements for maintaining\na total mapcount[3] for large folios with minimal added overhead and\nfurther changes[4] that build up on top of the total mapcount.\n\nIndependent of all that, this series results in a speedup during munmap()\nand similar unmapping (process teardown, MADV_DONTNEED on larger ranges)\nwith PTE-mapped THP, which is the default with THPs that are smaller than\na PMD (for example, 16KiB to 1024KiB mTHPs for anonymous memory[5]).\n\nOn an Intel Xeon Silver 4210R CPU, munmap'ing a 1GiB VMA backed by\nPTE-mapped folios of the same size (stddev < 1%) results in the following\nruntimes for munmap() in seconds (shorter is better):\n\nFolio Size | mm-unstable |      New | Change\n---------------------------------------------\n      4KiB |    0.058110 | 0.057715 |   - 1%\n     16KiB |    0.044198 | 0.035469 |   -20%\n     32KiB |    0.034216 | 0.023522 |   -31%\n     64KiB |    0.029207 | 0.018434 |   -37%\n    128KiB |    0.026579 | 0.014026 |   -47%\n    256KiB |    0.025130 | 0.011756 |   -53%\n    512KiB |    0.024292 | 0.010703 |   -56%\n   1024KiB |    0.023812 | 0.010294 |   -57%\n   2048KiB |    0.023785 | 0.009910 |   -58%\n\n[1] https://lkml.kernel.org/r/20240129124649.189745-1-david@redhat.com\n[2] https://lkml.kernel.org/r/20231218105100.172635-1-ryan.roberts@arm.com\n[3] https://lkml.kernel.org/r/20230809083256.699513-1-david@redhat.com\n[4] https://lkml.kernel.org/r/20231124132626.235350-1-david@redhat.com\n[5] https://lkml.kernel.org/r/20231207161211.2374093-1-ryan.roberts@arm.com\n\n\nThis patch (of 10):\n\nLet's prepare for further changes by factoring out processing of present\nPTEs.\n\nLink: https://lkml.kernel.org/r/20240214204435.167852-1-david@redhat.com\nLink: https://lkml.kernel.org/r/20240214204435.167852-2-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>\nCc: Arnd Bergmann <arnd@arndb.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: linuxppc-dev@lists.ozlabs.org\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Michal Hocko <mhocko@suse.com>\nCc: \"Naveen N. Rao\" <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Peter Zijlstra (Intel) <peterz@infradead.org>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nCc: Yin Fengwei <fengwei.yin@intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm/mmu_gather: add __tlb_remove_folio_pages()",
            "commit_message": "Add __tlb_remove_folio_pages(), which will remove multiple consecutive\npages that belong to the same large folio, instead of only a single page. \nWe'll be using this function when optimizing unmapping/zapping of large\nfolios that are mapped by PTEs.\n\nWe're using the remaining spare bit in an encoded_page to indicate that\nthe next enoced page in an array contains actually shifted \"nr_pages\". \nTeach swap/freeing code about putting multiple folio references, and\ndelayed rmap handling to remove page ranges of a folio.\n\nThis extension allows for still gathering almost as many small folios as\nwe used to (-1, because we have to prepare for a possibly bigger next\nentry), but still allows for gathering consecutive pages that belong to\nthe same large folio.\n\nNote that we don't pass the folio pointer, because it is not required for\nnow.  Further, we don't support page_size != PAGE_SIZE, it won't be\nrequired for simple PTE batching.\n\nWe have to provide a separate s390 implementation, but it's fairly\nstraight forward.\n\nAnother, more invasive and likely more expensive, approach would be to use\nfolio+range or a PFN range instead of page+nr_pages.  But, we should do\nthat consistently for the whole mmu_gather.  For now, let's keep it simple\nand add \"nr_pages\" only.\n\nNote that it is now possible to gather significantly more pages: In the\npast, we were able to gather ~10000 pages, now we can also gather ~5000\nfolio fragments that span multiple pages.  A folio fragment on x86-64 can\nspan up to 512 pages (2 MiB THP) and on arm64 with 64k in theory 8192\npages (512 MiB THP).  Gathering more memory is not considered something we\nshould worry about, especially because these are already corner cases.\n\nWhile we can gather more total memory, we won't free more folio fragments.\nAs long as page freeing time primarily only depends on the number of\ninvolved folios, there is no effective change for !preempt configurations.\nHowever, we'll adjust tlb_batch_pages_flush() separately to handle corner\ncases where page freeing time grows proportionally with the actual memory\nsize.\n\nLink: https://lkml.kernel.org/r/20240214204435.167852-9-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>\nCc: Arnd Bergmann <arnd@arndb.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Michal Hocko <mhocko@suse.com>\nCc: \"Naveen N. Rao\" <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Peter Zijlstra (Intel) <peterz@infradead.org>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nCc: Yin Fengwei <fengwei.yin@intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm/mmu_gather: pass \"delay_rmap\" instead of encoded page to __tlb_remove_page_size()",
            "commit_message": "We have two bits available in the encoded page pointer to store additional\ninformation.  Currently, we use one bit to request delay of the rmap\nremoval until after a TLB flush.\n\nWe want to make use of the remaining bit internally for batching of\nmultiple pages of the same folio, specifying that the next encoded page\npointer in an array is actually \"nr_pages\".  So pass page + delay_rmap\nflag instead of an encoded page, to handle the encoding internally.\n\nLink: https://lkml.kernel.org/r/20240214204435.167852-6-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>\nCc: Arnd Bergmann <arnd@arndb.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Michal Hocko <mhocko@suse.com>\nCc: \"Naveen N. Rao\" <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Peter Zijlstra (Intel) <peterz@infradead.org>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nCc: Yin Fengwei <fengwei.yin@intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm/memory: factor out zapping folio pte into zap_present_folio_pte()",
            "commit_message": "Let's prepare for further changes by factoring it out into a separate\nfunction.\n\nLink: https://lkml.kernel.org/r/20240214204435.167852-5-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>\nCc: Arnd Bergmann <arnd@arndb.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Michal Hocko <mhocko@suse.com>\nCc: \"Naveen N. Rao\" <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Peter Zijlstra (Intel) <peterz@infradead.org>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nCc: Yin Fengwei <fengwei.yin@intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm/mmu_gather: improve cond_resched() handling with large folios and expensive page freeing",
            "commit_message": "In tlb_batch_pages_flush(), we can end up freeing up to 512 pages or now\nup to 256 folio fragments that span more than one page, before we\nconditionally reschedule.\n\nIt's a pain that we have to handle cond_resched() in\ntlb_batch_pages_flush() manually and cannot simply handle it in\nrelease_pages() -- release_pages() can be called from atomic context. \nWell, in a perfect world we wouldn't have to make our code more\ncomplicated at all.\n\nWith page poisoning and init_on_free, we might now run into soft lockups\nwhen we free a lot of rather large folio fragments, because page freeing\ntime then depends on the actual memory size we are freeing instead of on\nthe number of folios that are involved.\n\nIn the absolute (unlikely) worst case, on arm64 with 64k we will be able\nto free up to 256 folio fragments that each span 512 MiB: zeroing out 128\nGiB does sound like it might take a while.  But instead of ignoring this\nunlikely case, let's just handle it.\n\nSo, let's teach tlb_batch_pages_flush() that there are some configurations\nwhere page freeing is horribly slow, and let's reschedule more frequently\n-- similarly like we did for now before we had large folio fragments in\nthere.  Avoid yet another loop over all encoded pages in the common case\nby handling that separately.\n\nNote that with page poisoning/zeroing, we might now end up freeing only a\nsingle folio fragment at a time that might exceed the old 512 pages limit:\nbut if we cannot even free a single MAX_ORDER page on a system without\nrunning into soft lockups, something else is already completely bogus. \nFreeing a PMD-mapped THP would similarly cause trouble.\n\nIn theory, we might even free 511 order-0 pages + a single MAX_ORDER page,\neffectively having to zero out 8703 pages on arm64 with 64k, translating\nto ~544 MiB of memory: however, if 512 MiB doesn't result in soft lockups,\n544 MiB is unlikely to result in soft lockups, so we won't care about that\nfor the time being.\n\nIn the future, we might want to detect if handling cond_resched() is\nrequired at all, and just not do any of that with full preemption enabled.\n\nLink: https://lkml.kernel.org/r/20240214204435.167852-10-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>\nCc: Arnd Bergmann <arnd@arndb.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Michal Hocko <mhocko@suse.com>\nCc: \"Naveen N. Rao\" <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Peter Zijlstra (Intel) <peterz@infradead.org>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nCc: Yin Fengwei <fengwei.yin@intel.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        }
    ],
    "proper_nouns": [
        {
            "PTE": "PTE stands for Page Table Entry, which is a data structure used in the Linux kernel to manage virtual-to-physical memory mappings. Each PTE corresponds to a single memory page and contains information such as the physical address of the page, access permissions, and status bits."
        },
        {
            "THP": "THP, or Transparent Huge Pages, is a feature in the Linux kernel that allows the use of larger memory pages (typically 2MB or 1GB) to reduce the overhead of managing smaller pages (usually 4KB). This can improve performance by reducing the number of page table entries and TLB (Translation Lookaside Buffer) misses."
        },
        {
            "MMU": "MMU stands for Memory Management Unit, a hardware component responsible for handling memory access requests and translating virtual addresses to physical addresses. The MMU is crucial for implementing features like virtual memory, memory protection, and caching."
        },
        {
            "TLB": "TLB, or Translation Lookaside Buffer, is a cache used by the MMU to store recent translations of virtual addresses to physical addresses. This reduces the time needed to access memory by avoiding the need to consult the full page table for every memory access."
        },
        {
            "RMAP": "RMAP is a reverse mapping mechanism used in the Linux kernel to keep track of which processes are using a particular page of memory. This is essential for efficient memory management, especially during operations like page freeing and migration."
        }
    ],
    "background_knowledge": [
        "The optimization of unmap/zap operations with PTE-mapped THP involves batching consecutive PTEs that map consecutive pages of the same large folio. This batching reduces the number of operations required to manage these pages, thereby improving performance.",
        "Understanding the existing infrastructure for batching in the Linux kernel, such as MMU gather and RMAP, is crucial for grasping how this optimization works. These mechanisms are already in place to handle large-scale memory operations efficiently.",
        "The concept of 'encoded pages' is used in the MMU gather handling to manage memory pages more efficiently. This involves encoding additional information into page pointers to optimize memory management operations."
    ],
    "kernel_changes": [
        "Before this optimization, unmap/zap operations with PTE-mapped THP were less efficient due to the need for multiple operations to manage each page individually. This led to increased overhead and reduced performance.",
        "After the implementation, the kernel was modified to batch consecutive PTEs that map consecutive pages of the same large folio. This batching reduces the number of operations required, leading to faster unmap/zap operations and improved overall performance.",
        "The introduction of new helper functions like tlb_remove_tlb_entries() and __tlb_remove_folio_pages() allows for more efficient handling of large folios, further enhancing the performance of memory management operations."
    ],
    "comprehensive_understanding": "The optimization of unmap/zap operations with PTE-mapped THP significantly improves the efficiency of memory management in the Linux kernel. By batching consecutive PTEs that map consecutive pages of the same large folio, the kernel reduces the number of operations required to manage these pages. This not only speeds up unmap/zap operations but also improves overall system performance by minimizing overhead and reducing the load on the MMU and TLB. The introduction of new helper functions and the use of encoded pages further enhance the efficiency of these operations, making the Linux kernel's memory management more robust and performant.",
    "concepts": [
        "PTE",
        "THP",
        "MMU",
        "TLB",
        "RMAP",
        "encoded pages",
        "batching",
        "large folio",
        "unmap/zap operations",
        "memory management",
        "performance optimization"
    ]
}
2024-12-03 22:37:38,108 - extractor - INFO - ---------Processing feature 19/815---------
2024-12-03 22:38:07,181 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个 linux 内核的专家。给你一个 linux 内核特性的描述以及其对应的若干 commit 的标题和提交信息，你的任务是为这个特性写一个多方面的扩写版描述。你的扩写需包含以下几个方面：1. 该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)；2. 为了让开发者理解该特性，需要哪些具体的，与这个特性直接相关的背景知识 (background_knowledge)；3. 该特性被实现前后，linux 内核发生了怎样的变化 (kernel_changes)；4. 你对该特性的综合理解 (comprehensive_understanding)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}]
        },
        "output": {
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
        }
    ],
    "input": {
        "feature_description": "zswap: optimize zswap lru list",
        "commits": [{"commit_subject": "mm/zswap: zswap entry doesn't need refcount anymore", "commit_message": "Since we don't need to leave zswap entry on the zswap tree anymore,\nwe should remove it from tree once we find it from the tree.\n\nThen after using it, we can directly free it, no concurrent path\ncan find it from tree. Only the shrinker can see it from lru list,\nwhich will also double check under tree lock, so no race problem.\n\nSo we don't need refcount in zswap entry anymore and don't need to\ntake the spinlock for the second time to invalidate it.\n\nThe side effect is that zswap_entry_free() maybe not happen in tree\nspinlock, but it's ok since nothing need to be protected by the lock.\n\nLink: https://lkml.kernel.org/r/20240201-b4-zswap-invalidate-entry-v2-6-99d4084260a0@bytedance.com\nSigned-off-by: Chengming Zhou <zhouchengming@bytedance.com>\nReviewed-by: Nhat Pham <nphamcs@gmail.com>\nAcked-by: Johannes Weiner <hannes@cmpxchg.org>\nCc: Yosry Ahmed <yosryahmed@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/zswap: invalidate zswap entry when swap entry free", "commit_message": "During testing I found there are some times the zswap_writeback_entry()\nreturn -ENOMEM, which is not we expected:\n\nbpftrace -e 'kr:zswap_writeback_entry {@[(int32)retval]=count()}'\n@[-12]: 1563\n@[0]: 277221\n\nThe reason is that __read_swap_cache_async() return NULL because\nswapcache_prepare() failed.  The reason is that we won't invalidate zswap\nentry when swap entry freed to the per-cpu pool, these zswap entries are\nstill on the zswap tree and lru list.\n\nThis patch moves the invalidation ahead to when swap entry freed to the\nper-cpu pool, since there is no any benefit to leave trashy zswap entry on\nthe tree and lru list.\n\nWith this patch:\nbpftrace -e 'kr:zswap_writeback_entry {@[(int32)retval]=count()}'\n@[0]: 259744\n\nNote: large folio can't have zswap entry for now, so don't bother\nto add zswap entry invalidation in the large folio swap free path.\n\nLink: https://lkml.kernel.org/r/20240201-b4-zswap-invalidate-entry-v2-2-99d4084260a0@bytedance.com\nSigned-off-by: Chengming Zhou <zhouchengming@bytedance.com>\nReviewed-by: Nhat Pham <nphamcs@gmail.com>\nAcked-by: Johannes Weiner <hannes@cmpxchg.org>\nAcked-by: Yosry Ahmed <yosryahmed@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/zswap: stop lru list shrinking when encounter warm region", "commit_message": "When the shrinker encounter an existing folio in swap cache, it means we\nare shrinking into the warmer region.  We should terminate shrinking if\nwe're in the dynamic shrinker context.\n\nThis patch add LRU_STOP to support this, to avoid overshrinking.\n\nLink: https://lkml.kernel.org/r/20240201-b4-zswap-invalidate-entry-v2-3-99d4084260a0@bytedance.com\nSigned-off-by: Chengming Zhou <zhouchengming@bytedance.com>\nAcked-by: Johannes Weiner <hannes@cmpxchg.org>\nAcked-by: Nhat Pham <nphamcs@gmail.com>\nReviewed-by: Yosry Ahmed <yosryahmed@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/zswap: add more comments in shrink_memcg_cb()", "commit_message": "Patch series \"mm/zswap: optimize zswap lru list\", v2.\n\nThis series is motivated when observe the zswap lru list shrinking, noted\nthere are some unexpected cases in zswap_writeback_entry().\n\nbpftrace -e 'kr:zswap_writeback_entry {@[(int32)retval]=count()}'\n\nThere are some -ENOMEM because when the swap entry is freed to per-cpu\nswap pool, it doesn't invalidate/drop zswap entry.  Then the shrinker\nencounter these trashy zswap entries, it can't be reclaimed and return\n-ENOMEM.\n\nSo move the invalidation ahead to when swap entry freed to the per-cpu\nswap pool, since there is no any benefit to leave trashy zswap entries on\nthe zswap tree and lru list.\n\nAnother case is -EEXIST, which is seen more in the case of\n!zswap_exclusive_loads_enabled, in which case the swapin folio will leave\ncompressed copy on the tree and lru list.  And it can't be reclaimed until\nthe folio is removed from swapcache.\n\nChanging to zswap_exclusive_loads_enabled mode will invalidate when folio\nswapin, which has its own drawback if that folio is still clean in\nswapcache and swapout again, we need to compress it again.  Please see the\ncommit for details on why we choose exclusive load as the default for\nzswap.\n\nAnother optimization for -EEXIST is that we add LRU_STOP to support\nterminating the shrinking process to avoid evicting warmer region.\n\nTesting using kernel build in tmpfs, one 50GB swapfile and\nzswap shrinker_enabled, with memory.max set to 2GB.\n\n                mm-unstable   zswap-optimize\nreal               63.90s       63.25s\nuser             1064.05s     1063.40s\nsys               292.32s      270.94s\n\nThe main optimization is in sys cpu, about 7% improvement.\n\n\nThis patch (of 6):\n\nAdd more comments in shrink_memcg_cb() to describe the deref dance which\nis implemented to fix race problem between lru writeback and swapoff, and\nthe reason why we rotate the entry at the beginning.\n\nAlso fix the stale comments in zswap_writeback_entry(), and add more\ncomments to state that we only deref the tree after we get the swapcache\nreference.\n\nLink: https://lkml.kernel.org/r/20240201-b4-zswap-invalidate-entry-v2-0-99d4084260a0@bytedance.com\nLink: https://lkml.kernel.org/r/20240201-b4-zswap-invalidate-entry-v2-1-99d4084260a0@bytedance.com\nSigned-off-by: Johannes Weiner <hannes@cmpxchg.org>\nSigned-off-by: Chengming Zhou <zhouchengming@bytedance.com>\nSuggested-by: Yosry Ahmed <yosryahmed@google.com>\nSuggested-by: Johannes Weiner <hannes@cmpxchg.org>\nAcked-by: Yosry Ahmed <yosryahmed@google.com>\nReviewed-by: Nhat Pham <nphamcs@gmail.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/zswap: only support zswap_exclusive_loads_enabled", "commit_message": "The !zswap_exclusive_loads_enabled mode will leave compressed copy in\nthe zswap tree and lru list after the folio swapin.\n\nThere are some disadvantages in this mode:\n1. It's a waste of memory since there are two copies of data, one is\n   folio, the other one is compressed data in zswap. And it's unlikely\n   the compressed data is useful in the near future.\n\n2. If that folio is dirtied, the compressed data must be not useful,\n   but we don't know and don't invalidate the trashy memory in zswap.\n\n3. It's not reclaimable from zswap shrinker since zswap_writeback_entry()\n   will always return -EEXIST and terminate the shrinking process.\n\nOn the other hand, the only downside of zswap_exclusive_loads_enabled\nis a little more cpu usage/latency when compression, and the same if\nthe folio is removed from swapcache or dirtied.\n\nMore explanation by Johannes on why we should consider exclusive load\nas the default for zswap:\n\n  Caching \"swapout work\" is helpful when the system is thrashing. Then\n  recently swapped in pages might get swapped out again very soon. It\n  certainly makes sense with conventional swap, because keeping a clean\n  copy on the disk saves IO work and doesn't cost any additional memory.\n\n  But with zswap, it's different. It saves some compression work on a\n  thrashing page. But the act of keeping compressed memory contributes\n  to a higher rate of thrashing. And that can cause IO in other places\n  like zswap writeback and file memory.\n\nAnd the A/B test results of the kernel build in tmpfs with limited memory\ncan support this theory:\n\n\t\t\t!exclusive\texclusive\nreal                       63.80         63.01\nuser                       1063.83       1061.32\nsys                        290.31        266.15\n\nworkingset_refault_anon    2383084.40    1976397.40\nworkingset_refault_file    44134.00      45689.40\nworkingset_activate_anon   837878.00     728441.20\nworkingset_activate_file   4710.00       4085.20\nworkingset_restore_anon    732622.60     639428.40\nworkingset_restore_file    1007.00       926.80\nworkingset_nodereclaim     0.00          0.00\npgscan                     14343003.40   12409570.20\npgscan_kswapd              0.00          0.00\npgscan_direct              14343003.40   12409570.20\npgscan_khugepaged          0.00          0.00\n\nLink: https://lkml.kernel.org/r/20240201-b4-zswap-invalidate-entry-v2-5-99d4084260a0@bytedance.com\nSigned-off-by: Chengming Zhou <zhouchengming@bytedance.com>\nAcked-by: Yosry Ahmed <yosryahmed@google.com>\nAcked-by: Johannes Weiner <hannes@cmpxchg.org>\nReviewed-by: Nhat Pham <nphamcs@gmail.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/zswap: remove duplicate_entry debug value", "commit_message": "cat /sys/kernel/debug/zswap/duplicate_entry\n2086447\n\nWhen testing, the duplicate_entry value is very high, but no warning\nmessage in the kernel log.  From the comment of duplicate_entry \"Duplicate\nstore was encountered (rare)\", it seems something goes wrong.\n\nActually it's incremented in the beginning of zswap_store(), which found\nits zswap entry has already on the tree.  And this is a normal case, since\nthe folio could leave zswap entry on the tree after swapin, later it's\ndirtied and swapout/zswap_store again, found its original zswap entry.\n\nSo duplicate_entry should be only incremented in the real bug case, which\nalready have \"WARN_ON(1)\", it looks redundant to count bug case, so this\npatch just remove it.\n\nLink: https://lkml.kernel.org/r/20240201-b4-zswap-invalidate-entry-v2-4-99d4084260a0@bytedance.com\nSigned-off-by: Chengming Zhou <zhouchengming@bytedance.com>\nAcked-by: Johannes Weiner <hannes@cmpxchg.org>\nReviewed-by: Nhat Pham <nphamcs@gmail.com>\nAcked-by: Yosry Ahmed <yosryahmed@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}]
    }
}

2024-12-03 22:38:50,510 - extractor - DEBUG - deepseek response:
ChatCompletion(id='4230a1c4-982c-4150-a861-deda5df44b69', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "proper_nouns": [\n        {\n            "zswap": "zswap is a lightweight in-kernel swap compression mechanism in the Linux kernel. It aims to reduce the I/O overhead associated with swapping by compressing data before it is written to swap space. This can significantly improve performance in memory-constrained environments by reducing the amount of data that needs to be written to and read from disk."\n        },\n        {\n            "LRU list": "The Least Recently Used (LRU) list is a data structure used in the Linux kernel to manage memory pages. It keeps track of the pages that have been accessed least recently, allowing the kernel to prioritize which pages to evict when memory pressure increases. The LRU list is crucial for efficient memory management, especially in systems with limited memory resources."\n        },\n        {\n            "shrinker": "The shrinker is a component in the Linux kernel responsible for reclaiming memory pages when the system is under memory pressure. It works by scanning the LRU lists and deciding which pages to free. The shrinker is essential for maintaining system performance by ensuring that memory is used efficiently."\n        },\n        {\n            "swap cache": "The swap cache is a cache in the Linux kernel that stores pages that have been swapped out but might be needed again soon. When a swapped-out page is accessed, it is brought back into memory from the swap cache, reducing the need for disk I/O. The swap cache is an important part of the kernel\'s memory management strategy."\n        }\n    ],\n    "background_knowledge": [\n        "zswap operates by compressing pages before they are written to swap space, which reduces the amount of data that needs to be written to disk. This compression is managed through a tree structure where each entry represents a compressed page.",\n        "The LRU list in zswap is used to manage the compressed pages, allowing the system to prioritize which pages to evict when memory pressure increases.",\n        "The shrinker in zswap is responsible for reclaiming memory by freeing compressed pages when the system is under memory pressure. It scans the LRU list and decides which pages to free.",\n        "The swap cache is used to store pages that have been swapped out but might be needed again soon. When a swapped-out page is accessed, it is brought back into memory from the swap cache, reducing the need for disk I/O."\n    ],\n    "kernel_changes": [\n        "Before the optimization, zswap entries were not invalidated when they were freed to the per-cpu pool, leading to inefficiencies in memory management. This caused the shrinker to encounter trashy zswap entries, which could not be reclaimed and resulted in -ENOMEM errors.",\n        "After the optimization, zswap entries are invalidated when they are freed to the per-cpu pool, ensuring that they are removed from the zswap tree and LRU list. This reduces memory waste and improves memory availability.",\n        "The optimization also introduced a mechanism to stop the LRU list shrinking when encountering a warm region, avoiding overshrinking and improving memory management efficiency.",\n        "Additionally, the optimization removed the need for a refcount in zswap entries, simplifying the memory management process and reducing the overhead associated with maintaining the refcount."\n    ],\n    "comprehensive_understanding": "The zswap optimization focuses on improving the efficiency of the LRU list management in zswap. By invalidating zswap entries when they are freed to the per-cpu pool, the optimization reduces memory waste and improves memory availability. The introduction of a mechanism to stop LRU list shrinking when encountering a warm region further enhances memory management efficiency. Additionally, the removal of the refcount in zswap entries simplifies the memory management process and reduces overhead. Overall, these changes contribute to better system performance and resource utilization in memory-constrained environments."\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733236687, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=823, prompt_tokens=4872, total_tokens=5695, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=1472, prompt_cache_miss_tokens=3400))
2024-12-03 22:38:50,510 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个命名实体抽取 (NER) 任务的专家，同时也是 linux 内核的专家。给你一个 linux 内核特性的描述 (feature_description)、对应的若干 commit 的标题和提交信息 (commits)、该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)、为了让开发者理解该特性所需的与这个特性直接相关的背景知识 (background_knowledge)、该特性被实现前后，linux 内核发生的变化 (kernel_changes)、对该特性的综合理解 (comprehensive_understanding)。你的任务是：根据这些信息提取出在 linux 内核中涉及到的和这个特性相关的概念实体 (concepts)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}],
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": [
                "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
            ]
        },
        "output": {
            "concepts": ["Copy-on-Write (CoW)", "mTHP", "folio", "memory reclamation", "memory availability", "anti-fragmentation"]
        }
    ],
    "input": {
        "feature_description": "zswap: optimize zswap lru list",
        "commits": [{"commit_subject": "mm/zswap: zswap entry doesn't need refcount anymore", "commit_message": "Since we don't need to leave zswap entry on the zswap tree anymore,\nwe should remove it from tree once we find it from the tree.\n\nThen after using it, we can directly free it, no concurrent path\ncan find it from tree. Only the shrinker can see it from lru list,\nwhich will also double check under tree lock, so no race problem.\n\nSo we don't need refcount in zswap entry anymore and don't need to\ntake the spinlock for the second time to invalidate it.\n\nThe side effect is that zswap_entry_free() maybe not happen in tree\nspinlock, but it's ok since nothing need to be protected by the lock.\n\nLink: https://lkml.kernel.org/r/20240201-b4-zswap-invalidate-entry-v2-6-99d4084260a0@bytedance.com\nSigned-off-by: Chengming Zhou <zhouchengming@bytedance.com>\nReviewed-by: Nhat Pham <nphamcs@gmail.com>\nAcked-by: Johannes Weiner <hannes@cmpxchg.org>\nCc: Yosry Ahmed <yosryahmed@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/zswap: invalidate zswap entry when swap entry free", "commit_message": "During testing I found there are some times the zswap_writeback_entry()\nreturn -ENOMEM, which is not we expected:\n\nbpftrace -e 'kr:zswap_writeback_entry {@[(int32)retval]=count()}'\n@[-12]: 1563\n@[0]: 277221\n\nThe reason is that __read_swap_cache_async() return NULL because\nswapcache_prepare() failed.  The reason is that we won't invalidate zswap\nentry when swap entry freed to the per-cpu pool, these zswap entries are\nstill on the zswap tree and lru list.\n\nThis patch moves the invalidation ahead to when swap entry freed to the\nper-cpu pool, since there is no any benefit to leave trashy zswap entry on\nthe tree and lru list.\n\nWith this patch:\nbpftrace -e 'kr:zswap_writeback_entry {@[(int32)retval]=count()}'\n@[0]: 259744\n\nNote: large folio can't have zswap entry for now, so don't bother\nto add zswap entry invalidation in the large folio swap free path.\n\nLink: https://lkml.kernel.org/r/20240201-b4-zswap-invalidate-entry-v2-2-99d4084260a0@bytedance.com\nSigned-off-by: Chengming Zhou <zhouchengming@bytedance.com>\nReviewed-by: Nhat Pham <nphamcs@gmail.com>\nAcked-by: Johannes Weiner <hannes@cmpxchg.org>\nAcked-by: Yosry Ahmed <yosryahmed@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/zswap: stop lru list shrinking when encounter warm region", "commit_message": "When the shrinker encounter an existing folio in swap cache, it means we\nare shrinking into the warmer region.  We should terminate shrinking if\nwe're in the dynamic shrinker context.\n\nThis patch add LRU_STOP to support this, to avoid overshrinking.\n\nLink: https://lkml.kernel.org/r/20240201-b4-zswap-invalidate-entry-v2-3-99d4084260a0@bytedance.com\nSigned-off-by: Chengming Zhou <zhouchengming@bytedance.com>\nAcked-by: Johannes Weiner <hannes@cmpxchg.org>\nAcked-by: Nhat Pham <nphamcs@gmail.com>\nReviewed-by: Yosry Ahmed <yosryahmed@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/zswap: add more comments in shrink_memcg_cb()", "commit_message": "Patch series \"mm/zswap: optimize zswap lru list\", v2.\n\nThis series is motivated when observe the zswap lru list shrinking, noted\nthere are some unexpected cases in zswap_writeback_entry().\n\nbpftrace -e 'kr:zswap_writeback_entry {@[(int32)retval]=count()}'\n\nThere are some -ENOMEM because when the swap entry is freed to per-cpu\nswap pool, it doesn't invalidate/drop zswap entry.  Then the shrinker\nencounter these trashy zswap entries, it can't be reclaimed and return\n-ENOMEM.\n\nSo move the invalidation ahead to when swap entry freed to the per-cpu\nswap pool, since there is no any benefit to leave trashy zswap entries on\nthe zswap tree and lru list.\n\nAnother case is -EEXIST, which is seen more in the case of\n!zswap_exclusive_loads_enabled, in which case the swapin folio will leave\ncompressed copy on the tree and lru list.  And it can't be reclaimed until\nthe folio is removed from swapcache.\n\nChanging to zswap_exclusive_loads_enabled mode will invalidate when folio\nswapin, which has its own drawback if that folio is still clean in\nswapcache and swapout again, we need to compress it again.  Please see the\ncommit for details on why we choose exclusive load as the default for\nzswap.\n\nAnother optimization for -EEXIST is that we add LRU_STOP to support\nterminating the shrinking process to avoid evicting warmer region.\n\nTesting using kernel build in tmpfs, one 50GB swapfile and\nzswap shrinker_enabled, with memory.max set to 2GB.\n\n                mm-unstable   zswap-optimize\nreal               63.90s       63.25s\nuser             1064.05s     1063.40s\nsys               292.32s      270.94s\n\nThe main optimization is in sys cpu, about 7% improvement.\n\n\nThis patch (of 6):\n\nAdd more comments in shrink_memcg_cb() to describe the deref dance which\nis implemented to fix race problem between lru writeback and swapoff, and\nthe reason why we rotate the entry at the beginning.\n\nAlso fix the stale comments in zswap_writeback_entry(), and add more\ncomments to state that we only deref the tree after we get the swapcache\nreference.\n\nLink: https://lkml.kernel.org/r/20240201-b4-zswap-invalidate-entry-v2-0-99d4084260a0@bytedance.com\nLink: https://lkml.kernel.org/r/20240201-b4-zswap-invalidate-entry-v2-1-99d4084260a0@bytedance.com\nSigned-off-by: Johannes Weiner <hannes@cmpxchg.org>\nSigned-off-by: Chengming Zhou <zhouchengming@bytedance.com>\nSuggested-by: Yosry Ahmed <yosryahmed@google.com>\nSuggested-by: Johannes Weiner <hannes@cmpxchg.org>\nAcked-by: Yosry Ahmed <yosryahmed@google.com>\nReviewed-by: Nhat Pham <nphamcs@gmail.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/zswap: only support zswap_exclusive_loads_enabled", "commit_message": "The !zswap_exclusive_loads_enabled mode will leave compressed copy in\nthe zswap tree and lru list after the folio swapin.\n\nThere are some disadvantages in this mode:\n1. It's a waste of memory since there are two copies of data, one is\n   folio, the other one is compressed data in zswap. And it's unlikely\n   the compressed data is useful in the near future.\n\n2. If that folio is dirtied, the compressed data must be not useful,\n   but we don't know and don't invalidate the trashy memory in zswap.\n\n3. It's not reclaimable from zswap shrinker since zswap_writeback_entry()\n   will always return -EEXIST and terminate the shrinking process.\n\nOn the other hand, the only downside of zswap_exclusive_loads_enabled\nis a little more cpu usage/latency when compression, and the same if\nthe folio is removed from swapcache or dirtied.\n\nMore explanation by Johannes on why we should consider exclusive load\nas the default for zswap:\n\n  Caching \"swapout work\" is helpful when the system is thrashing. Then\n  recently swapped in pages might get swapped out again very soon. It\n  certainly makes sense with conventional swap, because keeping a clean\n  copy on the disk saves IO work and doesn't cost any additional memory.\n\n  But with zswap, it's different. It saves some compression work on a\n  thrashing page. But the act of keeping compressed memory contributes\n  to a higher rate of thrashing. And that can cause IO in other places\n  like zswap writeback and file memory.\n\nAnd the A/B test results of the kernel build in tmpfs with limited memory\ncan support this theory:\n\n\t\t\t!exclusive\texclusive\nreal                       63.80         63.01\nuser                       1063.83       1061.32\nsys                        290.31        266.15\n\nworkingset_refault_anon    2383084.40    1976397.40\nworkingset_refault_file    44134.00      45689.40\nworkingset_activate_anon   837878.00     728441.20\nworkingset_activate_file   4710.00       4085.20\nworkingset_restore_anon    732622.60     639428.40\nworkingset_restore_file    1007.00       926.80\nworkingset_nodereclaim     0.00          0.00\npgscan                     14343003.40   12409570.20\npgscan_kswapd              0.00          0.00\npgscan_direct              14343003.40   12409570.20\npgscan_khugepaged          0.00          0.00\n\nLink: https://lkml.kernel.org/r/20240201-b4-zswap-invalidate-entry-v2-5-99d4084260a0@bytedance.com\nSigned-off-by: Chengming Zhou <zhouchengming@bytedance.com>\nAcked-by: Yosry Ahmed <yosryahmed@google.com>\nAcked-by: Johannes Weiner <hannes@cmpxchg.org>\nReviewed-by: Nhat Pham <nphamcs@gmail.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/zswap: remove duplicate_entry debug value", "commit_message": "cat /sys/kernel/debug/zswap/duplicate_entry\n2086447\n\nWhen testing, the duplicate_entry value is very high, but no warning\nmessage in the kernel log.  From the comment of duplicate_entry \"Duplicate\nstore was encountered (rare)\", it seems something goes wrong.\n\nActually it's incremented in the beginning of zswap_store(), which found\nits zswap entry has already on the tree.  And this is a normal case, since\nthe folio could leave zswap entry on the tree after swapin, later it's\ndirtied and swapout/zswap_store again, found its original zswap entry.\n\nSo duplicate_entry should be only incremented in the real bug case, which\nalready have \"WARN_ON(1)\", it looks redundant to count bug case, so this\npatch just remove it.\n\nLink: https://lkml.kernel.org/r/20240201-b4-zswap-invalidate-entry-v2-4-99d4084260a0@bytedance.com\nSigned-off-by: Chengming Zhou <zhouchengming@bytedance.com>\nAcked-by: Johannes Weiner <hannes@cmpxchg.org>\nReviewed-by: Nhat Pham <nphamcs@gmail.com>\nAcked-by: Yosry Ahmed <yosryahmed@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}],
        "proper_nouns": [{"zswap": "zswap is a lightweight in-kernel swap compression mechanism in the Linux kernel. It aims to reduce the I/O overhead associated with swapping by compressing data before it is written to swap space. This can significantly improve performance in memory-constrained environments by reducing the amount of data that needs to be written to and read from disk."}, {"LRU list": "The Least Recently Used (LRU) list is a data structure used in the Linux kernel to manage memory pages. It keeps track of the pages that have been accessed least recently, allowing the kernel to prioritize which pages to evict when memory pressure increases. The LRU list is crucial for efficient memory management, especially in systems with limited memory resources."}, {"shrinker": "The shrinker is a component in the Linux kernel responsible for reclaiming memory pages when the system is under memory pressure. It works by scanning the LRU lists and deciding which pages to free. The shrinker is essential for maintaining system performance by ensuring that memory is used efficiently."}, {"swap cache": "The swap cache is a cache in the Linux kernel that stores pages that have been swapped out but might be needed again soon. When a swapped-out page is accessed, it is brought back into memory from the swap cache, reducing the need for disk I/O. The swap cache is an important part of the kernel's memory management strategy."}],
        "background_knowledge": ["zswap operates by compressing pages before they are written to swap space, which reduces the amount of data that needs to be written to disk. This compression is managed through a tree structure where each entry represents a compressed page.", "The LRU list in zswap is used to manage the compressed pages, allowing the system to prioritize which pages to evict when memory pressure increases.", "The shrinker in zswap is responsible for reclaiming memory by freeing compressed pages when the system is under memory pressure. It scans the LRU list and decides which pages to free.", "The swap cache is used to store pages that have been swapped out but might be needed again soon. When a swapped-out page is accessed, it is brought back into memory from the swap cache, reducing the need for disk I/O."],
        "kernel_changes": ["Before the optimization, zswap entries were not invalidated when they were freed to the per-cpu pool, leading to inefficiencies in memory management. This caused the shrinker to encounter trashy zswap entries, which could not be reclaimed and resulted in -ENOMEM errors.", "After the optimization, zswap entries are invalidated when they are freed to the per-cpu pool, ensuring that they are removed from the zswap tree and LRU list. This reduces memory waste and improves memory availability.", "The optimization also introduced a mechanism to stop the LRU list shrinking when encountering a warm region, avoiding overshrinking and improving memory management efficiency.", "Additionally, the optimization removed the need for a refcount in zswap entries, simplifying the memory management process and reducing the overhead associated with maintaining the refcount."],
        "comprehensive_understanding": "The zswap optimization focuses on improving the efficiency of the LRU list management in zswap. By invalidating zswap entries when they are freed to the per-cpu pool, the optimization reduces memory waste and improves memory availability. The introduction of a mechanism to stop LRU list shrinking when encountering a warm region further enhances memory management efficiency. Additionally, the removal of the refcount in zswap entries simplifies the memory management process and reduces overhead. Overall, these changes contribute to better system performance and resource utilization in memory-constrained environments."
    }
}

2024-12-03 22:38:55,626 - extractor - DEBUG - deepseek response:
ChatCompletion(id='993e4ae2-4c69-4286-865a-5bff6f05f857', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "concepts": ["zswap", "LRU list", "shrinker", "swap cache", "memory reclamation", "memory availability", "per-cpu pool", "zswap tree", "compressed pages", "memory management", "overshrinking", "refcount", "memory pressure", "disk I/O", "system performance", "resource utilization"]\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733236730, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=86, prompt_tokens=5690, total_tokens=5776, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=1536, prompt_cache_miss_tokens=4154))
2024-12-03 22:38:55,627 - extractor - DEBUG - feature_extracted of feature_id=33221:
{
    "feature_id": 33221,
    "feature_description": "zswap: optimize zswap lru list",
    "version": "6.9",
    "commits": [
        {
            "commit_subject": "mm/zswap: zswap entry doesn't need refcount anymore",
            "commit_message": "Since we don't need to leave zswap entry on the zswap tree anymore,\nwe should remove it from tree once we find it from the tree.\n\nThen after using it, we can directly free it, no concurrent path\ncan find it from tree. Only the shrinker can see it from lru list,\nwhich will also double check under tree lock, so no race problem.\n\nSo we don't need refcount in zswap entry anymore and don't need to\ntake the spinlock for the second time to invalidate it.\n\nThe side effect is that zswap_entry_free() maybe not happen in tree\nspinlock, but it's ok since nothing need to be protected by the lock.\n\nLink: https://lkml.kernel.org/r/20240201-b4-zswap-invalidate-entry-v2-6-99d4084260a0@bytedance.com\nSigned-off-by: Chengming Zhou <zhouchengming@bytedance.com>\nReviewed-by: Nhat Pham <nphamcs@gmail.com>\nAcked-by: Johannes Weiner <hannes@cmpxchg.org>\nCc: Yosry Ahmed <yosryahmed@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm/zswap: invalidate zswap entry when swap entry free",
            "commit_message": "During testing I found there are some times the zswap_writeback_entry()\nreturn -ENOMEM, which is not we expected:\n\nbpftrace -e 'kr:zswap_writeback_entry {@[(int32)retval]=count()}'\n@[-12]: 1563\n@[0]: 277221\n\nThe reason is that __read_swap_cache_async() return NULL because\nswapcache_prepare() failed.  The reason is that we won't invalidate zswap\nentry when swap entry freed to the per-cpu pool, these zswap entries are\nstill on the zswap tree and lru list.\n\nThis patch moves the invalidation ahead to when swap entry freed to the\nper-cpu pool, since there is no any benefit to leave trashy zswap entry on\nthe tree and lru list.\n\nWith this patch:\nbpftrace -e 'kr:zswap_writeback_entry {@[(int32)retval]=count()}'\n@[0]: 259744\n\nNote: large folio can't have zswap entry for now, so don't bother\nto add zswap entry invalidation in the large folio swap free path.\n\nLink: https://lkml.kernel.org/r/20240201-b4-zswap-invalidate-entry-v2-2-99d4084260a0@bytedance.com\nSigned-off-by: Chengming Zhou <zhouchengming@bytedance.com>\nReviewed-by: Nhat Pham <nphamcs@gmail.com>\nAcked-by: Johannes Weiner <hannes@cmpxchg.org>\nAcked-by: Yosry Ahmed <yosryahmed@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm/zswap: stop lru list shrinking when encounter warm region",
            "commit_message": "When the shrinker encounter an existing folio in swap cache, it means we\nare shrinking into the warmer region.  We should terminate shrinking if\nwe're in the dynamic shrinker context.\n\nThis patch add LRU_STOP to support this, to avoid overshrinking.\n\nLink: https://lkml.kernel.org/r/20240201-b4-zswap-invalidate-entry-v2-3-99d4084260a0@bytedance.com\nSigned-off-by: Chengming Zhou <zhouchengming@bytedance.com>\nAcked-by: Johannes Weiner <hannes@cmpxchg.org>\nAcked-by: Nhat Pham <nphamcs@gmail.com>\nReviewed-by: Yosry Ahmed <yosryahmed@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm/zswap: add more comments in shrink_memcg_cb()",
            "commit_message": "Patch series \"mm/zswap: optimize zswap lru list\", v2.\n\nThis series is motivated when observe the zswap lru list shrinking, noted\nthere are some unexpected cases in zswap_writeback_entry().\n\nbpftrace -e 'kr:zswap_writeback_entry {@[(int32)retval]=count()}'\n\nThere are some -ENOMEM because when the swap entry is freed to per-cpu\nswap pool, it doesn't invalidate/drop zswap entry.  Then the shrinker\nencounter these trashy zswap entries, it can't be reclaimed and return\n-ENOMEM.\n\nSo move the invalidation ahead to when swap entry freed to the per-cpu\nswap pool, since there is no any benefit to leave trashy zswap entries on\nthe zswap tree and lru list.\n\nAnother case is -EEXIST, which is seen more in the case of\n!zswap_exclusive_loads_enabled, in which case the swapin folio will leave\ncompressed copy on the tree and lru list.  And it can't be reclaimed until\nthe folio is removed from swapcache.\n\nChanging to zswap_exclusive_loads_enabled mode will invalidate when folio\nswapin, which has its own drawback if that folio is still clean in\nswapcache and swapout again, we need to compress it again.  Please see the\ncommit for details on why we choose exclusive load as the default for\nzswap.\n\nAnother optimization for -EEXIST is that we add LRU_STOP to support\nterminating the shrinking process to avoid evicting warmer region.\n\nTesting using kernel build in tmpfs, one 50GB swapfile and\nzswap shrinker_enabled, with memory.max set to 2GB.\n\n                mm-unstable   zswap-optimize\nreal               63.90s       63.25s\nuser             1064.05s     1063.40s\nsys               292.32s      270.94s\n\nThe main optimization is in sys cpu, about 7% improvement.\n\n\nThis patch (of 6):\n\nAdd more comments in shrink_memcg_cb() to describe the deref dance which\nis implemented to fix race problem between lru writeback and swapoff, and\nthe reason why we rotate the entry at the beginning.\n\nAlso fix the stale comments in zswap_writeback_entry(), and add more\ncomments to state that we only deref the tree after we get the swapcache\nreference.\n\nLink: https://lkml.kernel.org/r/20240201-b4-zswap-invalidate-entry-v2-0-99d4084260a0@bytedance.com\nLink: https://lkml.kernel.org/r/20240201-b4-zswap-invalidate-entry-v2-1-99d4084260a0@bytedance.com\nSigned-off-by: Johannes Weiner <hannes@cmpxchg.org>\nSigned-off-by: Chengming Zhou <zhouchengming@bytedance.com>\nSuggested-by: Yosry Ahmed <yosryahmed@google.com>\nSuggested-by: Johannes Weiner <hannes@cmpxchg.org>\nAcked-by: Yosry Ahmed <yosryahmed@google.com>\nReviewed-by: Nhat Pham <nphamcs@gmail.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm/zswap: only support zswap_exclusive_loads_enabled",
            "commit_message": "The !zswap_exclusive_loads_enabled mode will leave compressed copy in\nthe zswap tree and lru list after the folio swapin.\n\nThere are some disadvantages in this mode:\n1. It's a waste of memory since there are two copies of data, one is\n   folio, the other one is compressed data in zswap. And it's unlikely\n   the compressed data is useful in the near future.\n\n2. If that folio is dirtied, the compressed data must be not useful,\n   but we don't know and don't invalidate the trashy memory in zswap.\n\n3. It's not reclaimable from zswap shrinker since zswap_writeback_entry()\n   will always return -EEXIST and terminate the shrinking process.\n\nOn the other hand, the only downside of zswap_exclusive_loads_enabled\nis a little more cpu usage/latency when compression, and the same if\nthe folio is removed from swapcache or dirtied.\n\nMore explanation by Johannes on why we should consider exclusive load\nas the default for zswap:\n\n  Caching \"swapout work\" is helpful when the system is thrashing. Then\n  recently swapped in pages might get swapped out again very soon. It\n  certainly makes sense with conventional swap, because keeping a clean\n  copy on the disk saves IO work and doesn't cost any additional memory.\n\n  But with zswap, it's different. It saves some compression work on a\n  thrashing page. But the act of keeping compressed memory contributes\n  to a higher rate of thrashing. And that can cause IO in other places\n  like zswap writeback and file memory.\n\nAnd the A/B test results of the kernel build in tmpfs with limited memory\ncan support this theory:\n\n\t\t\t!exclusive\texclusive\nreal                       63.80         63.01\nuser                       1063.83       1061.32\nsys                        290.31        266.15\n\nworkingset_refault_anon    2383084.40    1976397.40\nworkingset_refault_file    44134.00      45689.40\nworkingset_activate_anon   837878.00     728441.20\nworkingset_activate_file   4710.00       4085.20\nworkingset_restore_anon    732622.60     639428.40\nworkingset_restore_file    1007.00       926.80\nworkingset_nodereclaim     0.00          0.00\npgscan                     14343003.40   12409570.20\npgscan_kswapd              0.00          0.00\npgscan_direct              14343003.40   12409570.20\npgscan_khugepaged          0.00          0.00\n\nLink: https://lkml.kernel.org/r/20240201-b4-zswap-invalidate-entry-v2-5-99d4084260a0@bytedance.com\nSigned-off-by: Chengming Zhou <zhouchengming@bytedance.com>\nAcked-by: Yosry Ahmed <yosryahmed@google.com>\nAcked-by: Johannes Weiner <hannes@cmpxchg.org>\nReviewed-by: Nhat Pham <nphamcs@gmail.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm/zswap: remove duplicate_entry debug value",
            "commit_message": "cat /sys/kernel/debug/zswap/duplicate_entry\n2086447\n\nWhen testing, the duplicate_entry value is very high, but no warning\nmessage in the kernel log.  From the comment of duplicate_entry \"Duplicate\nstore was encountered (rare)\", it seems something goes wrong.\n\nActually it's incremented in the beginning of zswap_store(), which found\nits zswap entry has already on the tree.  And this is a normal case, since\nthe folio could leave zswap entry on the tree after swapin, later it's\ndirtied and swapout/zswap_store again, found its original zswap entry.\n\nSo duplicate_entry should be only incremented in the real bug case, which\nalready have \"WARN_ON(1)\", it looks redundant to count bug case, so this\npatch just remove it.\n\nLink: https://lkml.kernel.org/r/20240201-b4-zswap-invalidate-entry-v2-4-99d4084260a0@bytedance.com\nSigned-off-by: Chengming Zhou <zhouchengming@bytedance.com>\nAcked-by: Johannes Weiner <hannes@cmpxchg.org>\nReviewed-by: Nhat Pham <nphamcs@gmail.com>\nAcked-by: Yosry Ahmed <yosryahmed@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        }
    ],
    "proper_nouns": [
        {
            "zswap": "zswap is a lightweight in-kernel swap compression mechanism in the Linux kernel. It aims to reduce the I/O overhead associated with swapping by compressing data before it is written to swap space. This can significantly improve performance in memory-constrained environments by reducing the amount of data that needs to be written to and read from disk."
        },
        {
            "LRU list": "The Least Recently Used (LRU) list is a data structure used in the Linux kernel to manage memory pages. It keeps track of the pages that have been accessed least recently, allowing the kernel to prioritize which pages to evict when memory pressure increases. The LRU list is crucial for efficient memory management, especially in systems with limited memory resources."
        },
        {
            "shrinker": "The shrinker is a component in the Linux kernel responsible for reclaiming memory pages when the system is under memory pressure. It works by scanning the LRU lists and deciding which pages to free. The shrinker is essential for maintaining system performance by ensuring that memory is used efficiently."
        },
        {
            "swap cache": "The swap cache is a cache in the Linux kernel that stores pages that have been swapped out but might be needed again soon. When a swapped-out page is accessed, it is brought back into memory from the swap cache, reducing the need for disk I/O. The swap cache is an important part of the kernel's memory management strategy."
        }
    ],
    "background_knowledge": [
        "zswap operates by compressing pages before they are written to swap space, which reduces the amount of data that needs to be written to disk. This compression is managed through a tree structure where each entry represents a compressed page.",
        "The LRU list in zswap is used to manage the compressed pages, allowing the system to prioritize which pages to evict when memory pressure increases.",
        "The shrinker in zswap is responsible for reclaiming memory by freeing compressed pages when the system is under memory pressure. It scans the LRU list and decides which pages to free.",
        "The swap cache is used to store pages that have been swapped out but might be needed again soon. When a swapped-out page is accessed, it is brought back into memory from the swap cache, reducing the need for disk I/O."
    ],
    "kernel_changes": [
        "Before the optimization, zswap entries were not invalidated when they were freed to the per-cpu pool, leading to inefficiencies in memory management. This caused the shrinker to encounter trashy zswap entries, which could not be reclaimed and resulted in -ENOMEM errors.",
        "After the optimization, zswap entries are invalidated when they are freed to the per-cpu pool, ensuring that they are removed from the zswap tree and LRU list. This reduces memory waste and improves memory availability.",
        "The optimization also introduced a mechanism to stop the LRU list shrinking when encountering a warm region, avoiding overshrinking and improving memory management efficiency.",
        "Additionally, the optimization removed the need for a refcount in zswap entries, simplifying the memory management process and reducing the overhead associated with maintaining the refcount."
    ],
    "comprehensive_understanding": "The zswap optimization focuses on improving the efficiency of the LRU list management in zswap. By invalidating zswap entries when they are freed to the per-cpu pool, the optimization reduces memory waste and improves memory availability. The introduction of a mechanism to stop LRU list shrinking when encountering a warm region further enhances memory management efficiency. Additionally, the removal of the refcount in zswap entries simplifies the memory management process and reduces overhead. Overall, these changes contribute to better system performance and resource utilization in memory-constrained environments.",
    "concepts": [
        "zswap",
        "LRU list",
        "shrinker",
        "swap cache",
        "memory reclamation",
        "memory availability",
        "per-cpu pool",
        "zswap tree",
        "compressed pages",
        "memory management",
        "overshrinking",
        "refcount",
        "memory pressure",
        "disk I/O",
        "system performance",
        "resource utilization"
    ]
}
2024-12-03 22:38:55,627 - extractor - INFO - ---------Processing feature 20/815---------
2024-12-03 22:39:00,793 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个 linux 内核的专家。给你一个 linux 内核特性的描述以及其对应的若干 commit 的标题和提交信息，你的任务是为这个特性写一个多方面的扩写版描述。你的扩写需包含以下几个方面：1. 该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)；2. 为了让开发者理解该特性，需要哪些具体的，与这个特性直接相关的背景知识 (background_knowledge)；3. 该特性被实现前后，linux 内核发生了怎样的变化 (kernel_changes)；4. 你对该特性的综合理解 (comprehensive_understanding)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}]
        },
        "output": {
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
        }
    ],
    "input": {
        "feature_description": "cma: add sysfs file 'release_pages_success'",
        "commits": [{"commit_subject": "mm/cma: add sysfs file 'release_pages_success'", "commit_message": "This adds the following new sysfs file tracking the number of successfully\nreleased pages from a given CMA heap area.  This file will be available\nvia CONFIG_CMA_SYSFS and help in determining active CMA pages available on\nthe CMA heap area.  This adds a new 'nr_pages_released' (CONFIG_CMA_SYSFS)\ninto 'struct cma' which gets updated during cma_release().\n\n/sys/kernel/mm/cma/<cma-heap-area>/release_pages_success\n\nAfter this change, an user will be able to find active CMA pages available\nin a given CMA heap area via the following method.\n\nActive pages = alloc_pages_success - release_pages_success\n\nThat's valuable information for both software designers, and system admins\nas it allows them to tune the number of CMA pages available in the system.\nThis increases user visibility for allocated CMA area and its\nutilization.\n\nLink: https://lkml.kernel.org/r/20240206045731.472759-1-anshuman.khandual@arm.com\nSigned-off-by: Anshuman Khandual <anshuman.khandual@arm.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}]
    }
}

2024-12-03 22:39:37,344 - extractor - DEBUG - deepseek response:
ChatCompletion(id='02e18c29-7a08-4f55-9e7e-28d2ec6444fa', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "proper_nouns": [\n        {\n            "CMA (Contiguous Memory Allocator)": "CMA is a feature in the Linux kernel designed to allocate large contiguous blocks of memory, which is particularly useful for devices that require large, contiguous memory regions, such as video processing hardware or GPUs. CMA reserves a portion of memory during system boot and manages it to ensure that contiguous memory is available when needed."\n        },\n        {\n            "sysfs": "sysfs is a virtual filesystem in the Linux kernel that provides a way to export kernel data structures, their attributes, and the linkages between them to userspace. It is typically mounted at /sys and is used for viewing and manipulating kernel parameters and device information."\n        },\n        {\n            "CONFIG_CMA_SYSFS": "This is a kernel configuration option that enables the sysfs interface for CMA. When enabled, it allows users to interact with CMA-related parameters and statistics through sysfs, providing visibility and control over CMA operations."\n        }\n    ],\n    "background_knowledge": [\n        "The Contiguous Memory Allocator (CMA) is used to manage large, contiguous memory regions in the Linux kernel. These regions are often required by devices that need contiguous memory, such as video processing hardware. CMA reserves a portion of memory during system boot and manages it to ensure that contiguous memory is available when needed.",\n        "sysfs is a virtual filesystem that provides a way to export kernel data structures and their attributes to userspace. It is used for viewing and manipulating kernel parameters and device information.",\n        "CONFIG_CMA_SYSFS is a kernel configuration option that enables the sysfs interface for CMA, allowing users to interact with CMA-related parameters and statistics through sysfs."\n    ],\n    "kernel_changes": [\n        "Before this feature, there was no direct way to track the number of successfully released pages from a given CMA heap area. This made it difficult to determine the active CMA pages available in the system and to tune the number of CMA pages accordingly.",\n        "With the addition of the \'release_pages_success\' sysfs file, users can now track the number of successfully released pages from a given CMA heap area. This file is available via CONFIG_CMA_SYSFS and provides valuable information for software designers and system administrators.",\n        "The new \'nr_pages_released\' field in the \'struct cma\' is updated during the cma_release() function, allowing for accurate tracking of released pages. This change enhances user visibility for allocated CMA areas and their utilization, enabling better tuning of CMA pages in the system."\n    ],\n    "comprehensive_understanding": "This feature enhances the Contiguous Memory Allocator (CMA) by adding a new sysfs file, \'release_pages_success\', which tracks the number of successfully released pages from a given CMA heap area. This file is accessible via CONFIG_CMA_SYSFS and provides valuable information for software designers and system administrators, allowing them to determine the active CMA pages available in the system. By enhancing visibility and control over CMA operations, this feature enables better tuning of CMA pages, ultimately improving system performance and resource utilization."\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733236740, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=697, prompt_tokens=1908, total_tokens=2605, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=1472, prompt_cache_miss_tokens=436))
2024-12-03 22:39:37,344 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个命名实体抽取 (NER) 任务的专家，同时也是 linux 内核的专家。给你一个 linux 内核特性的描述 (feature_description)、对应的若干 commit 的标题和提交信息 (commits)、该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)、为了让开发者理解该特性所需的与这个特性直接相关的背景知识 (background_knowledge)、该特性被实现前后，linux 内核发生的变化 (kernel_changes)、对该特性的综合理解 (comprehensive_understanding)。你的任务是：根据这些信息提取出在 linux 内核中涉及到的和这个特性相关的概念实体 (concepts)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}],
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": [
                "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
            ]
        },
        "output": {
            "concepts": ["Copy-on-Write (CoW)", "mTHP", "folio", "memory reclamation", "memory availability", "anti-fragmentation"]
        }
    ],
    "input": {
        "feature_description": "cma: add sysfs file 'release_pages_success'",
        "commits": [{"commit_subject": "mm/cma: add sysfs file 'release_pages_success'", "commit_message": "This adds the following new sysfs file tracking the number of successfully\nreleased pages from a given CMA heap area.  This file will be available\nvia CONFIG_CMA_SYSFS and help in determining active CMA pages available on\nthe CMA heap area.  This adds a new 'nr_pages_released' (CONFIG_CMA_SYSFS)\ninto 'struct cma' which gets updated during cma_release().\n\n/sys/kernel/mm/cma/<cma-heap-area>/release_pages_success\n\nAfter this change, an user will be able to find active CMA pages available\nin a given CMA heap area via the following method.\n\nActive pages = alloc_pages_success - release_pages_success\n\nThat's valuable information for both software designers, and system admins\nas it allows them to tune the number of CMA pages available in the system.\nThis increases user visibility for allocated CMA area and its\nutilization.\n\nLink: https://lkml.kernel.org/r/20240206045731.472759-1-anshuman.khandual@arm.com\nSigned-off-by: Anshuman Khandual <anshuman.khandual@arm.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}],
        "proper_nouns": [{"CMA (Contiguous Memory Allocator)": "CMA is a feature in the Linux kernel designed to allocate large contiguous blocks of memory, which is particularly useful for devices that require large, contiguous memory regions, such as video processing hardware or GPUs. CMA reserves a portion of memory during system boot and manages it to ensure that contiguous memory is available when needed."}, {"sysfs": "sysfs is a virtual filesystem in the Linux kernel that provides a way to export kernel data structures, their attributes, and the linkages between them to userspace. It is typically mounted at /sys and is used for viewing and manipulating kernel parameters and device information."}, {"CONFIG_CMA_SYSFS": "This is a kernel configuration option that enables the sysfs interface for CMA. When enabled, it allows users to interact with CMA-related parameters and statistics through sysfs, providing visibility and control over CMA operations."}],
        "background_knowledge": ["The Contiguous Memory Allocator (CMA) is used to manage large, contiguous memory regions in the Linux kernel. These regions are often required by devices that need contiguous memory, such as video processing hardware. CMA reserves a portion of memory during system boot and manages it to ensure that contiguous memory is available when needed.", "sysfs is a virtual filesystem that provides a way to export kernel data structures and their attributes to userspace. It is used for viewing and manipulating kernel parameters and device information.", "CONFIG_CMA_SYSFS is a kernel configuration option that enables the sysfs interface for CMA, allowing users to interact with CMA-related parameters and statistics through sysfs."],
        "kernel_changes": ["Before this feature, there was no direct way to track the number of successfully released pages from a given CMA heap area. This made it difficult to determine the active CMA pages available in the system and to tune the number of CMA pages accordingly.", "With the addition of the 'release_pages_success' sysfs file, users can now track the number of successfully released pages from a given CMA heap area. This file is available via CONFIG_CMA_SYSFS and provides valuable information for software designers and system administrators.", "The new 'nr_pages_released' field in the 'struct cma' is updated during the cma_release() function, allowing for accurate tracking of released pages. This change enhances user visibility for allocated CMA areas and their utilization, enabling better tuning of CMA pages in the system."],
        "comprehensive_understanding": "This feature enhances the Contiguous Memory Allocator (CMA) by adding a new sysfs file, 'release_pages_success', which tracks the number of successfully released pages from a given CMA heap area. This file is accessible via CONFIG_CMA_SYSFS and provides valuable information for software designers and system administrators, allowing them to determine the active CMA pages available in the system. By enhancing visibility and control over CMA operations, this feature enables better tuning of CMA pages, ultimately improving system performance and resource utilization."
    }
}

2024-12-03 22:39:41,405 - extractor - DEBUG - deepseek response:
ChatCompletion(id='c8004067-202d-4dc1-8ae2-f3050bfc0759', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "concepts": ["CMA (Contiguous Memory Allocator)", "sysfs", "CONFIG_CMA_SYSFS", "release_pages_success", "nr_pages_released", "cma_release()", "active CMA pages"]\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733236777, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=64, prompt_tokens=2611, total_tokens=2675, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=1536, prompt_cache_miss_tokens=1075))
2024-12-03 22:39:41,407 - extractor - DEBUG - feature_extracted of feature_id=33220:
{
    "feature_id": 33220,
    "feature_description": "cma: add sysfs file 'release_pages_success'",
    "version": "6.9",
    "commits": [
        {
            "commit_subject": "mm/cma: add sysfs file 'release_pages_success'",
            "commit_message": "This adds the following new sysfs file tracking the number of successfully\nreleased pages from a given CMA heap area.  This file will be available\nvia CONFIG_CMA_SYSFS and help in determining active CMA pages available on\nthe CMA heap area.  This adds a new 'nr_pages_released' (CONFIG_CMA_SYSFS)\ninto 'struct cma' which gets updated during cma_release().\n\n/sys/kernel/mm/cma/<cma-heap-area>/release_pages_success\n\nAfter this change, an user will be able to find active CMA pages available\nin a given CMA heap area via the following method.\n\nActive pages = alloc_pages_success - release_pages_success\n\nThat's valuable information for both software designers, and system admins\nas it allows them to tune the number of CMA pages available in the system.\nThis increases user visibility for allocated CMA area and its\nutilization.\n\nLink: https://lkml.kernel.org/r/20240206045731.472759-1-anshuman.khandual@arm.com\nSigned-off-by: Anshuman Khandual <anshuman.khandual@arm.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        }
    ],
    "proper_nouns": [
        {
            "CMA (Contiguous Memory Allocator)": "CMA is a feature in the Linux kernel designed to allocate large contiguous blocks of memory, which is particularly useful for devices that require large, contiguous memory regions, such as video processing hardware or GPUs. CMA reserves a portion of memory during system boot and manages it to ensure that contiguous memory is available when needed."
        },
        {
            "sysfs": "sysfs is a virtual filesystem in the Linux kernel that provides a way to export kernel data structures, their attributes, and the linkages between them to userspace. It is typically mounted at /sys and is used for viewing and manipulating kernel parameters and device information."
        },
        {
            "CONFIG_CMA_SYSFS": "This is a kernel configuration option that enables the sysfs interface for CMA. When enabled, it allows users to interact with CMA-related parameters and statistics through sysfs, providing visibility and control over CMA operations."
        }
    ],
    "background_knowledge": [
        "The Contiguous Memory Allocator (CMA) is used to manage large, contiguous memory regions in the Linux kernel. These regions are often required by devices that need contiguous memory, such as video processing hardware. CMA reserves a portion of memory during system boot and manages it to ensure that contiguous memory is available when needed.",
        "sysfs is a virtual filesystem that provides a way to export kernel data structures and their attributes to userspace. It is used for viewing and manipulating kernel parameters and device information.",
        "CONFIG_CMA_SYSFS is a kernel configuration option that enables the sysfs interface for CMA, allowing users to interact with CMA-related parameters and statistics through sysfs."
    ],
    "kernel_changes": [
        "Before this feature, there was no direct way to track the number of successfully released pages from a given CMA heap area. This made it difficult to determine the active CMA pages available in the system and to tune the number of CMA pages accordingly.",
        "With the addition of the 'release_pages_success' sysfs file, users can now track the number of successfully released pages from a given CMA heap area. This file is available via CONFIG_CMA_SYSFS and provides valuable information for software designers and system administrators.",
        "The new 'nr_pages_released' field in the 'struct cma' is updated during the cma_release() function, allowing for accurate tracking of released pages. This change enhances user visibility for allocated CMA areas and their utilization, enabling better tuning of CMA pages in the system."
    ],
    "comprehensive_understanding": "This feature enhances the Contiguous Memory Allocator (CMA) by adding a new sysfs file, 'release_pages_success', which tracks the number of successfully released pages from a given CMA heap area. This file is accessible via CONFIG_CMA_SYSFS and provides valuable information for software designers and system administrators, allowing them to determine the active CMA pages available in the system. By enhancing visibility and control over CMA operations, this feature enables better tuning of CMA pages, ultimately improving system performance and resource utilization.",
    "concepts": [
        "CMA (Contiguous Memory Allocator)",
        "sysfs",
        "CONFIG_CMA_SYSFS",
        "release_pages_success",
        "nr_pages_released",
        "cma_release()",
        "active CMA pages"
    ]
}
2024-12-03 22:39:41,407 - extractor - INFO - ---------Processing feature 21/815---------
2024-12-03 22:40:23,272 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个 linux 内核的专家。给你一个 linux 内核特性的描述以及其对应的若干 commit 的标题和提交信息，你的任务是为这个特性写一个多方面的扩写版描述。你的扩写需包含以下几个方面：1. 该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)；2. 为了让开发者理解该特性，需要哪些具体的，与这个特性直接相关的背景知识 (background_knowledge)；3. 该特性被实现前后，linux 内核发生了怎样的变化 (kernel_changes)；4. 你对该特性的综合理解 (comprehensive_understanding)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}]
        },
        "output": {
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
        }
    ],
    "input": {
        "feature_description": "Optimize fork() with PTE-mapped THP",
        "commits": [{"commit_subject": "mm/memory: optimize fork() with PTE-mapped THP", "commit_message": "Let's implement PTE batching when consecutive (present) PTEs map\nconsecutive pages of the same large folio, and all other PTE bits besides\nthe PFNs are equal.\n\nWe will optimize folio_pte_batch() separately, to ignore selected PTE\nbits.  This patch is based on work by Ryan Roberts.\n\nUse __always_inline for __copy_present_ptes() and keep the handling for\nsingle PTEs completely separate from the multi-PTE case: we really want\nthe compiler to optimize for the single-PTE case with small folios, to not\ndegrade performance.\n\nNote that PTE batching will never exceed a single page table and will\nalways stay within VMA boundaries.\n\nFurther, processing PTE-mapped THP that maybe pinned and have\nPageAnonExclusive set on at least one subpage should work as expected, but\nthere is room for improvement: We will repeatedly (1) detect a PTE batch\n(2) detect that we have to copy a page (3) fall back and allocate a single\npage to copy a single page.  For now we won't care as pinned pages are a\ncorner case, and we should rather look into maintaining only a single\nPageAnonExclusive bit for large folios.\n\nLink: https://lkml.kernel.org/r/20240129124649.189745-14-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nReviewed-by: Mike Rapoport (IBM) <rppt@kernel.org>\nCc: Albert Ou <aou@eecs.berkeley.edu>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Alexandre Ghiti <alexghiti@rivosinc.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@kernel.org>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: David S. Miller <davem@davemloft.net>\nCc: Dinh Nguyen <dinguyen@kernel.org>\nCc: Gerald Schaefer <gerald.schaefer@linux.ibm.com>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Naveen N. Rao <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Palmer Dabbelt <palmer@dabbelt.com>\nCc: Paul Walmsley <paul.walmsley@sifive.com>\nCc: Russell King (Oracle) <linux@armlinux.org.uk>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/memory: ignore writable bit in folio_pte_batch()", "commit_message": "...  and conditionally return to the caller if any PTE except the first\none is writable.  fork() has to make sure to properly write-protect in\ncase any PTE is writable.  Other users (e.g., page unmaping) are expected\nto not care.\n\nLink: https://lkml.kernel.org/r/20240129124649.189745-16-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: Albert Ou <aou@eecs.berkeley.edu>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Alexandre Ghiti <alexghiti@rivosinc.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@kernel.org>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: David S. Miller <davem@davemloft.net>\nCc: Dinh Nguyen <dinguyen@kernel.org>\nCc: Gerald Schaefer <gerald.schaefer@linux.ibm.com>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Naveen N. Rao <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Palmer Dabbelt <palmer@dabbelt.com>\nCc: Paul Walmsley <paul.walmsley@sifive.com>\nCc: Russell King (Oracle) <linux@armlinux.org.uk>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nCc: Mike Rapoport (IBM) <rppt@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/memory: factor out copying the actual PTE in copy_present_pte()", "commit_message": "Let's prepare for further changes.\n\nLink: https://lkml.kernel.org/r/20240129124649.189745-12-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nReviewed-by: Mike Rapoport (IBM) <rppt@kernel.org>\nCc: Albert Ou <aou@eecs.berkeley.edu>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Alexandre Ghiti <alexghiti@rivosinc.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@kernel.org>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: David S. Miller <davem@davemloft.net>\nCc: Dinh Nguyen <dinguyen@kernel.org>\nCc: Gerald Schaefer <gerald.schaefer@linux.ibm.com>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Naveen N. Rao <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Palmer Dabbelt <palmer@dabbelt.com>\nCc: Paul Walmsley <paul.walmsley@sifive.com>\nCc: Russell King (Oracle) <linux@armlinux.org.uk>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "arm/pgtable: define PFN_PTE_SHIFT", "commit_message": "We want to make use of pte_next_pfn() outside of set_ptes().  Let's simply\ndefine PFN_PTE_SHIFT, required by pte_next_pfn().\n\nLink: https://lkml.kernel.org/r/20240129124649.189745-3-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nTested-by: Ryan Roberts <ryan.roberts@arm.com>\nReviewed-by: Mike Rapoport (IBM) <rppt@kernel.org>\nCc: Albert Ou <aou@eecs.berkeley.edu>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Alexandre Ghiti <alexghiti@rivosinc.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@kernel.org>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: David S. Miller <davem@davemloft.net>\nCc: Dinh Nguyen <dinguyen@kernel.org>\nCc: Gerald Schaefer <gerald.schaefer@linux.ibm.com>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Naveen N. Rao <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Palmer Dabbelt <palmer@dabbelt.com>\nCc: Paul Walmsley <paul.walmsley@sifive.com>\nCc: Russell King (Oracle) <linux@armlinux.org.uk>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/memory: pass PTE to copy_present_pte()", "commit_message": "We already read it, let's just forward it.\n\nThis patch is based on work by Ryan Roberts.\n\n[david@redhat.com: fix the hmm \"exclusive_cow\" selftest]\n  Link: https://lkml.kernel.org/r/13f296b8-e882-47fd-b939-c2141dc28717@redhat.com\nLink: https://lkml.kernel.org/r/20240129124649.189745-13-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nReviewed-by: Mike Rapoport (IBM) <rppt@kernel.org>\nCc: Albert Ou <aou@eecs.berkeley.edu>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Alexandre Ghiti <alexghiti@rivosinc.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@kernel.org>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: David S. Miller <davem@davemloft.net>\nCc: Dinh Nguyen <dinguyen@kernel.org>\nCc: Gerald Schaefer <gerald.schaefer@linux.ibm.com>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Naveen N. Rao <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Palmer Dabbelt <palmer@dabbelt.com>\nCc: Paul Walmsley <paul.walmsley@sifive.com>\nCc: Russell King (Oracle) <linux@armlinux.org.uk>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/memory: ignore dirty/accessed/soft-dirty bits in folio_pte_batch()", "commit_message": "Let's always ignore the accessed/young bit: we'll always mark the PTE as\nold in our child process during fork, and upcoming users will similarly\nnot care.\n\nIgnore the dirty bit only if we don't want to duplicate the dirty bit into\nthe child process during fork.  Maybe, we could just set all PTEs in the\nchild dirty if any PTE is dirty.  For now, let's keep the behavior\nunchanged, this can be optimized later if required.\n\nIgnore the soft-dirty bit only if the bit doesn't have any meaning in the\nsrc vma, and similarly won't have any in the copied dst vma.\n\nFor now, we won't bother with the uffd-wp bit.\n\nLink: https://lkml.kernel.org/r/20240129124649.189745-15-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: Albert Ou <aou@eecs.berkeley.edu>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Alexandre Ghiti <alexghiti@rivosinc.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@kernel.org>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: David S. Miller <davem@davemloft.net>\nCc: Dinh Nguyen <dinguyen@kernel.org>\nCc: Gerald Schaefer <gerald.schaefer@linux.ibm.com>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Naveen N. Rao <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Palmer Dabbelt <palmer@dabbelt.com>\nCc: Paul Walmsley <paul.walmsley@sifive.com>\nCc: Russell King (Oracle) <linux@armlinux.org.uk>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nCc: Mike Rapoport (IBM) <rppt@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "arm64/mm: make set_ptes() robust when OAs cross 48-bit boundary", "commit_message": "Patch series \"mm/memory: optimize fork() with PTE-mapped THP\", v3.\n\nNow that the rmap overhaul[1] is upstream that provides a clean interface\nfor rmap batching, let's implement PTE batching during fork when\nprocessing PTE-mapped THPs.\n\nThis series is partially based on Ryan's previous work[2] to implement\ncont-pte support on arm64, but its a complete rewrite based on [1] to\noptimize all architectures independent of any such PTE bits, and to use\nthe new rmap batching functions that simplify the code and prepare for\nfurther rmap accounting changes.\n\nWe collect consecutive PTEs that map consecutive pages of the same large\nfolio, making sure that the other PTE bits are compatible, and (a) adjust\nthe refcount only once per batch, (b) call rmap handling functions only\nonce per batch and (c) perform batch PTE setting/updates.\n\nWhile this series should be beneficial for adding cont-pte support on\nARM64[2], it's one of the requirements for maintaining a total mapcount[3]\nfor large folios with minimal added overhead and further changes[4] that\nbuild up on top of the total mapcount.\n\nIndependent of all that, this series results in a speedup during fork with\nPTE-mapped THP, which is the default with THPs that are smaller than a PMD\n(for example, 16KiB to 1024KiB mTHPs for anonymous memory[5]).\n\nOn an Intel Xeon Silver 4210R CPU, fork'ing with 1GiB of PTE-mapped folios\nof the same size (stddev < 1%) results in the following runtimes for\nfork() (shorter is better):\n\nFolio Size | v6.8-rc1 |      New | Change\n------------------------------------------\n      4KiB | 0.014328 | 0.014035 |   - 2%\n     16KiB | 0.014263 | 0.01196  |   -16%\n     32KiB | 0.014334 | 0.01094  |   -24%\n     64KiB | 0.014046 | 0.010444 |   -26%\n    128KiB | 0.014011 | 0.010063 |   -28%\n    256KiB | 0.013993 | 0.009938 |   -29%\n    512KiB | 0.013983 | 0.00985  |   -30%\n   1024KiB | 0.013986 | 0.00982  |   -30%\n   2048KiB | 0.014305 | 0.010076 |   -30%\n\nNote that these numbers are even better than the ones from v1 (verified\nover multiple reboots), even though there were only minimal code changes. \nWell, I removed a pte_mkclean() call for anon folios, maybe that also\nplays a role.\n\nBut my experience is that fork() is extremely sensitive to code size,\ninlining, ...  so I suspect we'll see on other architectures rather a\nchange of -20% instead of -30%, and it will be easy to \"lose\" some of that\nspeedup in the future by subtle code changes.\n\nNext up is PTE batching when unmapping.  Only tested on x86-64. \nCompile-tested on most other architectures.\n\n[1] https://lkml.kernel.org/r/20231220224504.646757-1-david@redhat.com\n[2] https://lkml.kernel.org/r/20231218105100.172635-1-ryan.roberts@arm.com\n[3] https://lkml.kernel.org/r/20230809083256.699513-1-david@redhat.com\n[4] https://lkml.kernel.org/r/20231124132626.235350-1-david@redhat.com\n[5] https://lkml.kernel.org/r/20231207161211.2374093-1-ryan.roberts@arm.com\n\n\nThis patch (of 15):\n\nSince the high bits [51:48] of an OA are not stored contiguously in the\nPTE, there is a theoretical bug in set_ptes(), which just adds PAGE_SIZE\nto the pte to get the pte with the next pfn.  This works until the pfn\ncrosses the 48-bit boundary, at which point we overflow into the upper\nattributes.\n\nOf course one could argue (and Matthew Wilcox has :) that we will never\nsee a folio cross this boundary because we only allow naturally aligned\npower-of-2 allocation, so this would require a half-petabyte folio.  So\nits only a theoretical bug.  But its better that the code is robust\nregardless.\n\nI've implemented pte_next_pfn() as part of the fix, which is an opt-in\ncore-mm interface.  So that is now available to the core-mm, which will be\nneeded shortly to support forthcoming fork()-batching optimizations.\n\nLink: https://lkml.kernel.org/r/20240129124649.189745-1-david@redhat.com\nLink: https://lkml.kernel.org/r/20240125173534.1659317-1-ryan.roberts@arm.com\nLink: https://lkml.kernel.org/r/20240129124649.189745-2-david@redhat.com\nFixes: 4a169d61c2ed (\"arm64: implement the new page table range API\")\nCloses: https://lore.kernel.org/linux-mm/fdaeb9a5-d890-499a-92c8-d171df43ad01@arm.com/\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Catalin Marinas <catalin.marinas@arm.com>\nReviewed-by: David Hildenbrand <david@redhat.com>\nTested-by: Ryan Roberts <ryan.roberts@arm.com>\nReviewed-by: Mike Rapoport (IBM) <rppt@kernel.org>\nCc: Albert Ou <aou@eecs.berkeley.edu>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@kernel.org>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: David S. Miller <davem@davemloft.net>\nCc: Dinh Nguyen <dinguyen@kernel.org>\nCc: Gerald Schaefer <gerald.schaefer@linux.ibm.com>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Naveen N. Rao <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Palmer Dabbelt <palmer@dabbelt.com>\nCc: Paul Walmsley <paul.walmsley@sifive.com>\nCc: Russell King (Oracle) <linux@armlinux.org.uk>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nCc: Alexandre Ghiti <alexghiti@rivosinc.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/pgtable: make pte_next_pfn() independent of set_ptes()", "commit_message": "Let's provide pte_next_pfn(), independently of set_ptes().  This allows\nfor using the generic pte_next_pfn() version in some arch-specific\nset_ptes() implementations, and prepares for reusing pte_next_pfn() in\nother context.\n\nLink: https://lkml.kernel.org/r/20240129124649.189745-9-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Christophe Leroy <christophe.leroy@csgroup.eu>\nTested-by: Ryan Roberts <ryan.roberts@arm.com>\nReviewed-by: Mike Rapoport (IBM) <rppt@kernel.org>\nCc: Albert Ou <aou@eecs.berkeley.edu>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Alexandre Ghiti <alexghiti@rivosinc.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@kernel.org>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: David S. Miller <davem@davemloft.net>\nCc: Dinh Nguyen <dinguyen@kernel.org>\nCc: Gerald Schaefer <gerald.schaefer@linux.ibm.com>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Naveen N. Rao <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Palmer Dabbelt <palmer@dabbelt.com>\nCc: Paul Walmsley <paul.walmsley@sifive.com>\nCc: Russell King (Oracle) <linux@armlinux.org.uk>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "arm/mm: use pte_next_pfn() in set_ptes()", "commit_message": "Let's use our handy helper now that it's available on all archs.\n\nLink: https://lkml.kernel.org/r/20240129124649.189745-10-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nTested-by: Ryan Roberts <ryan.roberts@arm.com>\nReviewed-by: Mike Rapoport (IBM) <rppt@kernel.org>\nCc: Albert Ou <aou@eecs.berkeley.edu>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Alexandre Ghiti <alexghiti@rivosinc.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@kernel.org>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: David S. Miller <davem@davemloft.net>\nCc: Dinh Nguyen <dinguyen@kernel.org>\nCc: Gerald Schaefer <gerald.schaefer@linux.ibm.com>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Naveen N. Rao <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Palmer Dabbelt <palmer@dabbelt.com>\nCc: Paul Walmsley <paul.walmsley@sifive.com>\nCc: Russell King (Oracle) <linux@armlinux.org.uk>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}]
    }
}

2024-12-03 22:41:02,153 - extractor - DEBUG - deepseek response:
ChatCompletion(id='eeccf682-6857-4f84-a470-223e94b0a398', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "proper_nouns": [\n        {\n            "PTE": "Page Table Entry (PTE) is a data structure used in the Linux kernel to map virtual memory addresses to physical memory addresses. Each PTE contains information about a specific page of memory, including its physical address, access permissions, and other attributes."\n        },\n        {\n            "THP": "Transparent Huge Pages (THP) is a feature in the Linux kernel that allows the use of larger memory pages (typically 2MB or 1GB) to reduce the overhead of managing smaller pages (usually 4KB). THP can improve performance by reducing the number of PTEs that need to be managed and by reducing the frequency of page faults."\n        },\n        {\n            "fork()": "The fork() system call is a fundamental operation in Unix-like operating systems, including Linux. It creates a new process by duplicating the calling process, known as the parent process, to create a child process. The child process is an exact copy of the parent process, except for certain attributes like process ID."\n        },\n        {\n            "folio": "In the context of the Linux kernel, a \'folio\' is a term used to describe a large contiguous block of memory pages. Folios are used to manage memory more efficiently, especially when dealing with large files or when performing operations that span multiple pages."\n        }\n    ],\n    "background_knowledge": [\n        "The optimization of the fork() system call with PTE-mapped THP involves improving the efficiency of memory management during the creation of new processes. Specifically, this optimization focuses on batching PTEs (Page Table Entries) when they map consecutive pages of the same large folio, thereby reducing the overhead associated with managing individual PTEs.",\n        "Understanding this feature requires familiarity with the concept of PTE batching, which involves grouping multiple PTEs together to perform operations more efficiently. Additionally, knowledge of how THP (Transparent Huge Pages) are managed and how they interact with the fork() system call is essential."\n    ],\n    "kernel_changes": [\n        "Before the implementation of this feature, the fork() system call would handle each PTE individually, which could be inefficient, especially when dealing with large folios. The new optimization introduces PTE batching, where consecutive PTEs mapping consecutive pages of the same large folio are processed together. This reduces the number of operations required during the fork() process, leading to faster execution times and improved memory management.",\n        "The changes also include modifications to the folio_pte_batch() function to ignore certain PTE bits, such as the writable bit, during batch processing. This ensures that the fork() operation can be performed more efficiently without compromising the integrity of the memory management system."\n    ],\n    "comprehensive_understanding": "The optimization of the fork() system call with PTE-mapped THP is a significant improvement in the Linux kernel\'s memory management. By implementing PTE batching, the kernel can process multiple PTEs simultaneously when they map consecutive pages of the same large folio. This reduces the overhead associated with managing individual PTEs and improves the efficiency of the fork() operation. The changes also ensure that certain PTE bits, such as the writable bit, are ignored during batch processing, further enhancing performance. Overall, this optimization leads to faster process creation times and better memory utilization, making the Linux kernel more efficient and responsive."\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733236823, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=736, prompt_tokens=8714, total_tokens=9450, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=1472, prompt_cache_miss_tokens=7242))
2024-12-03 22:41:02,153 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个命名实体抽取 (NER) 任务的专家，同时也是 linux 内核的专家。给你一个 linux 内核特性的描述 (feature_description)、对应的若干 commit 的标题和提交信息 (commits)、该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)、为了让开发者理解该特性所需的与这个特性直接相关的背景知识 (background_knowledge)、该特性被实现前后，linux 内核发生的变化 (kernel_changes)、对该特性的综合理解 (comprehensive_understanding)。你的任务是：根据这些信息提取出在 linux 内核中涉及到的和这个特性相关的概念实体 (concepts)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}],
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": [
                "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
            ]
        },
        "output": {
            "concepts": ["Copy-on-Write (CoW)", "mTHP", "folio", "memory reclamation", "memory availability", "anti-fragmentation"]
        }
    ],
    "input": {
        "feature_description": "Optimize fork() with PTE-mapped THP",
        "commits": [{"commit_subject": "mm/memory: optimize fork() with PTE-mapped THP", "commit_message": "Let's implement PTE batching when consecutive (present) PTEs map\nconsecutive pages of the same large folio, and all other PTE bits besides\nthe PFNs are equal.\n\nWe will optimize folio_pte_batch() separately, to ignore selected PTE\nbits.  This patch is based on work by Ryan Roberts.\n\nUse __always_inline for __copy_present_ptes() and keep the handling for\nsingle PTEs completely separate from the multi-PTE case: we really want\nthe compiler to optimize for the single-PTE case with small folios, to not\ndegrade performance.\n\nNote that PTE batching will never exceed a single page table and will\nalways stay within VMA boundaries.\n\nFurther, processing PTE-mapped THP that maybe pinned and have\nPageAnonExclusive set on at least one subpage should work as expected, but\nthere is room for improvement: We will repeatedly (1) detect a PTE batch\n(2) detect that we have to copy a page (3) fall back and allocate a single\npage to copy a single page.  For now we won't care as pinned pages are a\ncorner case, and we should rather look into maintaining only a single\nPageAnonExclusive bit for large folios.\n\nLink: https://lkml.kernel.org/r/20240129124649.189745-14-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nReviewed-by: Mike Rapoport (IBM) <rppt@kernel.org>\nCc: Albert Ou <aou@eecs.berkeley.edu>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Alexandre Ghiti <alexghiti@rivosinc.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@kernel.org>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: David S. Miller <davem@davemloft.net>\nCc: Dinh Nguyen <dinguyen@kernel.org>\nCc: Gerald Schaefer <gerald.schaefer@linux.ibm.com>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Naveen N. Rao <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Palmer Dabbelt <palmer@dabbelt.com>\nCc: Paul Walmsley <paul.walmsley@sifive.com>\nCc: Russell King (Oracle) <linux@armlinux.org.uk>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/memory: ignore writable bit in folio_pte_batch()", "commit_message": "...  and conditionally return to the caller if any PTE except the first\none is writable.  fork() has to make sure to properly write-protect in\ncase any PTE is writable.  Other users (e.g., page unmaping) are expected\nto not care.\n\nLink: https://lkml.kernel.org/r/20240129124649.189745-16-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: Albert Ou <aou@eecs.berkeley.edu>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Alexandre Ghiti <alexghiti@rivosinc.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@kernel.org>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: David S. Miller <davem@davemloft.net>\nCc: Dinh Nguyen <dinguyen@kernel.org>\nCc: Gerald Schaefer <gerald.schaefer@linux.ibm.com>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Naveen N. Rao <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Palmer Dabbelt <palmer@dabbelt.com>\nCc: Paul Walmsley <paul.walmsley@sifive.com>\nCc: Russell King (Oracle) <linux@armlinux.org.uk>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nCc: Mike Rapoport (IBM) <rppt@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/memory: factor out copying the actual PTE in copy_present_pte()", "commit_message": "Let's prepare for further changes.\n\nLink: https://lkml.kernel.org/r/20240129124649.189745-12-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nReviewed-by: Mike Rapoport (IBM) <rppt@kernel.org>\nCc: Albert Ou <aou@eecs.berkeley.edu>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Alexandre Ghiti <alexghiti@rivosinc.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@kernel.org>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: David S. Miller <davem@davemloft.net>\nCc: Dinh Nguyen <dinguyen@kernel.org>\nCc: Gerald Schaefer <gerald.schaefer@linux.ibm.com>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Naveen N. Rao <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Palmer Dabbelt <palmer@dabbelt.com>\nCc: Paul Walmsley <paul.walmsley@sifive.com>\nCc: Russell King (Oracle) <linux@armlinux.org.uk>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "arm/pgtable: define PFN_PTE_SHIFT", "commit_message": "We want to make use of pte_next_pfn() outside of set_ptes().  Let's simply\ndefine PFN_PTE_SHIFT, required by pte_next_pfn().\n\nLink: https://lkml.kernel.org/r/20240129124649.189745-3-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nTested-by: Ryan Roberts <ryan.roberts@arm.com>\nReviewed-by: Mike Rapoport (IBM) <rppt@kernel.org>\nCc: Albert Ou <aou@eecs.berkeley.edu>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Alexandre Ghiti <alexghiti@rivosinc.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@kernel.org>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: David S. Miller <davem@davemloft.net>\nCc: Dinh Nguyen <dinguyen@kernel.org>\nCc: Gerald Schaefer <gerald.schaefer@linux.ibm.com>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Naveen N. Rao <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Palmer Dabbelt <palmer@dabbelt.com>\nCc: Paul Walmsley <paul.walmsley@sifive.com>\nCc: Russell King (Oracle) <linux@armlinux.org.uk>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/memory: pass PTE to copy_present_pte()", "commit_message": "We already read it, let's just forward it.\n\nThis patch is based on work by Ryan Roberts.\n\n[david@redhat.com: fix the hmm \"exclusive_cow\" selftest]\n  Link: https://lkml.kernel.org/r/13f296b8-e882-47fd-b939-c2141dc28717@redhat.com\nLink: https://lkml.kernel.org/r/20240129124649.189745-13-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nReviewed-by: Mike Rapoport (IBM) <rppt@kernel.org>\nCc: Albert Ou <aou@eecs.berkeley.edu>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Alexandre Ghiti <alexghiti@rivosinc.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@kernel.org>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: David S. Miller <davem@davemloft.net>\nCc: Dinh Nguyen <dinguyen@kernel.org>\nCc: Gerald Schaefer <gerald.schaefer@linux.ibm.com>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Naveen N. Rao <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Palmer Dabbelt <palmer@dabbelt.com>\nCc: Paul Walmsley <paul.walmsley@sifive.com>\nCc: Russell King (Oracle) <linux@armlinux.org.uk>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/memory: ignore dirty/accessed/soft-dirty bits in folio_pte_batch()", "commit_message": "Let's always ignore the accessed/young bit: we'll always mark the PTE as\nold in our child process during fork, and upcoming users will similarly\nnot care.\n\nIgnore the dirty bit only if we don't want to duplicate the dirty bit into\nthe child process during fork.  Maybe, we could just set all PTEs in the\nchild dirty if any PTE is dirty.  For now, let's keep the behavior\nunchanged, this can be optimized later if required.\n\nIgnore the soft-dirty bit only if the bit doesn't have any meaning in the\nsrc vma, and similarly won't have any in the copied dst vma.\n\nFor now, we won't bother with the uffd-wp bit.\n\nLink: https://lkml.kernel.org/r/20240129124649.189745-15-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: Albert Ou <aou@eecs.berkeley.edu>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Alexandre Ghiti <alexghiti@rivosinc.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@kernel.org>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: David S. Miller <davem@davemloft.net>\nCc: Dinh Nguyen <dinguyen@kernel.org>\nCc: Gerald Schaefer <gerald.schaefer@linux.ibm.com>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Naveen N. Rao <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Palmer Dabbelt <palmer@dabbelt.com>\nCc: Paul Walmsley <paul.walmsley@sifive.com>\nCc: Russell King (Oracle) <linux@armlinux.org.uk>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nCc: Mike Rapoport (IBM) <rppt@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "arm64/mm: make set_ptes() robust when OAs cross 48-bit boundary", "commit_message": "Patch series \"mm/memory: optimize fork() with PTE-mapped THP\", v3.\n\nNow that the rmap overhaul[1] is upstream that provides a clean interface\nfor rmap batching, let's implement PTE batching during fork when\nprocessing PTE-mapped THPs.\n\nThis series is partially based on Ryan's previous work[2] to implement\ncont-pte support on arm64, but its a complete rewrite based on [1] to\noptimize all architectures independent of any such PTE bits, and to use\nthe new rmap batching functions that simplify the code and prepare for\nfurther rmap accounting changes.\n\nWe collect consecutive PTEs that map consecutive pages of the same large\nfolio, making sure that the other PTE bits are compatible, and (a) adjust\nthe refcount only once per batch, (b) call rmap handling functions only\nonce per batch and (c) perform batch PTE setting/updates.\n\nWhile this series should be beneficial for adding cont-pte support on\nARM64[2], it's one of the requirements for maintaining a total mapcount[3]\nfor large folios with minimal added overhead and further changes[4] that\nbuild up on top of the total mapcount.\n\nIndependent of all that, this series results in a speedup during fork with\nPTE-mapped THP, which is the default with THPs that are smaller than a PMD\n(for example, 16KiB to 1024KiB mTHPs for anonymous memory[5]).\n\nOn an Intel Xeon Silver 4210R CPU, fork'ing with 1GiB of PTE-mapped folios\nof the same size (stddev < 1%) results in the following runtimes for\nfork() (shorter is better):\n\nFolio Size | v6.8-rc1 |      New | Change\n------------------------------------------\n      4KiB | 0.014328 | 0.014035 |   - 2%\n     16KiB | 0.014263 | 0.01196  |   -16%\n     32KiB | 0.014334 | 0.01094  |   -24%\n     64KiB | 0.014046 | 0.010444 |   -26%\n    128KiB | 0.014011 | 0.010063 |   -28%\n    256KiB | 0.013993 | 0.009938 |   -29%\n    512KiB | 0.013983 | 0.00985  |   -30%\n   1024KiB | 0.013986 | 0.00982  |   -30%\n   2048KiB | 0.014305 | 0.010076 |   -30%\n\nNote that these numbers are even better than the ones from v1 (verified\nover multiple reboots), even though there were only minimal code changes. \nWell, I removed a pte_mkclean() call for anon folios, maybe that also\nplays a role.\n\nBut my experience is that fork() is extremely sensitive to code size,\ninlining, ...  so I suspect we'll see on other architectures rather a\nchange of -20% instead of -30%, and it will be easy to \"lose\" some of that\nspeedup in the future by subtle code changes.\n\nNext up is PTE batching when unmapping.  Only tested on x86-64. \nCompile-tested on most other architectures.\n\n[1] https://lkml.kernel.org/r/20231220224504.646757-1-david@redhat.com\n[2] https://lkml.kernel.org/r/20231218105100.172635-1-ryan.roberts@arm.com\n[3] https://lkml.kernel.org/r/20230809083256.699513-1-david@redhat.com\n[4] https://lkml.kernel.org/r/20231124132626.235350-1-david@redhat.com\n[5] https://lkml.kernel.org/r/20231207161211.2374093-1-ryan.roberts@arm.com\n\n\nThis patch (of 15):\n\nSince the high bits [51:48] of an OA are not stored contiguously in the\nPTE, there is a theoretical bug in set_ptes(), which just adds PAGE_SIZE\nto the pte to get the pte with the next pfn.  This works until the pfn\ncrosses the 48-bit boundary, at which point we overflow into the upper\nattributes.\n\nOf course one could argue (and Matthew Wilcox has :) that we will never\nsee a folio cross this boundary because we only allow naturally aligned\npower-of-2 allocation, so this would require a half-petabyte folio.  So\nits only a theoretical bug.  But its better that the code is robust\nregardless.\n\nI've implemented pte_next_pfn() as part of the fix, which is an opt-in\ncore-mm interface.  So that is now available to the core-mm, which will be\nneeded shortly to support forthcoming fork()-batching optimizations.\n\nLink: https://lkml.kernel.org/r/20240129124649.189745-1-david@redhat.com\nLink: https://lkml.kernel.org/r/20240125173534.1659317-1-ryan.roberts@arm.com\nLink: https://lkml.kernel.org/r/20240129124649.189745-2-david@redhat.com\nFixes: 4a169d61c2ed (\"arm64: implement the new page table range API\")\nCloses: https://lore.kernel.org/linux-mm/fdaeb9a5-d890-499a-92c8-d171df43ad01@arm.com/\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Catalin Marinas <catalin.marinas@arm.com>\nReviewed-by: David Hildenbrand <david@redhat.com>\nTested-by: Ryan Roberts <ryan.roberts@arm.com>\nReviewed-by: Mike Rapoport (IBM) <rppt@kernel.org>\nCc: Albert Ou <aou@eecs.berkeley.edu>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@kernel.org>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: David S. Miller <davem@davemloft.net>\nCc: Dinh Nguyen <dinguyen@kernel.org>\nCc: Gerald Schaefer <gerald.schaefer@linux.ibm.com>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Naveen N. Rao <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Palmer Dabbelt <palmer@dabbelt.com>\nCc: Paul Walmsley <paul.walmsley@sifive.com>\nCc: Russell King (Oracle) <linux@armlinux.org.uk>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nCc: Alexandre Ghiti <alexghiti@rivosinc.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/pgtable: make pte_next_pfn() independent of set_ptes()", "commit_message": "Let's provide pte_next_pfn(), independently of set_ptes().  This allows\nfor using the generic pte_next_pfn() version in some arch-specific\nset_ptes() implementations, and prepares for reusing pte_next_pfn() in\nother context.\n\nLink: https://lkml.kernel.org/r/20240129124649.189745-9-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Christophe Leroy <christophe.leroy@csgroup.eu>\nTested-by: Ryan Roberts <ryan.roberts@arm.com>\nReviewed-by: Mike Rapoport (IBM) <rppt@kernel.org>\nCc: Albert Ou <aou@eecs.berkeley.edu>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Alexandre Ghiti <alexghiti@rivosinc.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@kernel.org>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: David S. Miller <davem@davemloft.net>\nCc: Dinh Nguyen <dinguyen@kernel.org>\nCc: Gerald Schaefer <gerald.schaefer@linux.ibm.com>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Naveen N. Rao <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Palmer Dabbelt <palmer@dabbelt.com>\nCc: Paul Walmsley <paul.walmsley@sifive.com>\nCc: Russell King (Oracle) <linux@armlinux.org.uk>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "arm/mm: use pte_next_pfn() in set_ptes()", "commit_message": "Let's use our handy helper now that it's available on all archs.\n\nLink: https://lkml.kernel.org/r/20240129124649.189745-10-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nTested-by: Ryan Roberts <ryan.roberts@arm.com>\nReviewed-by: Mike Rapoport (IBM) <rppt@kernel.org>\nCc: Albert Ou <aou@eecs.berkeley.edu>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Alexandre Ghiti <alexghiti@rivosinc.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@kernel.org>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: David S. Miller <davem@davemloft.net>\nCc: Dinh Nguyen <dinguyen@kernel.org>\nCc: Gerald Schaefer <gerald.schaefer@linux.ibm.com>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Naveen N. Rao <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Palmer Dabbelt <palmer@dabbelt.com>\nCc: Paul Walmsley <paul.walmsley@sifive.com>\nCc: Russell King (Oracle) <linux@armlinux.org.uk>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}],
        "proper_nouns": [{"PTE": "Page Table Entry (PTE) is a data structure used in the Linux kernel to map virtual memory addresses to physical memory addresses. Each PTE contains information about a specific page of memory, including its physical address, access permissions, and other attributes."}, {"THP": "Transparent Huge Pages (THP) is a feature in the Linux kernel that allows the use of larger memory pages (typically 2MB or 1GB) to reduce the overhead of managing smaller pages (usually 4KB). THP can improve performance by reducing the number of PTEs that need to be managed and by reducing the frequency of page faults."}, {"fork()": "The fork() system call is a fundamental operation in Unix-like operating systems, including Linux. It creates a new process by duplicating the calling process, known as the parent process, to create a child process. The child process is an exact copy of the parent process, except for certain attributes like process ID."}, {"folio": "In the context of the Linux kernel, a 'folio' is a term used to describe a large contiguous block of memory pages. Folios are used to manage memory more efficiently, especially when dealing with large files or when performing operations that span multiple pages."}],
        "background_knowledge": ["The optimization of the fork() system call with PTE-mapped THP involves improving the efficiency of memory management during the creation of new processes. Specifically, this optimization focuses on batching PTEs (Page Table Entries) when they map consecutive pages of the same large folio, thereby reducing the overhead associated with managing individual PTEs.", "Understanding this feature requires familiarity with the concept of PTE batching, which involves grouping multiple PTEs together to perform operations more efficiently. Additionally, knowledge of how THP (Transparent Huge Pages) are managed and how they interact with the fork() system call is essential."],
        "kernel_changes": ["Before the implementation of this feature, the fork() system call would handle each PTE individually, which could be inefficient, especially when dealing with large folios. The new optimization introduces PTE batching, where consecutive PTEs mapping consecutive pages of the same large folio are processed together. This reduces the number of operations required during the fork() process, leading to faster execution times and improved memory management.", "The changes also include modifications to the folio_pte_batch() function to ignore certain PTE bits, such as the writable bit, during batch processing. This ensures that the fork() operation can be performed more efficiently without compromising the integrity of the memory management system."],
        "comprehensive_understanding": "The optimization of the fork() system call with PTE-mapped THP is a significant improvement in the Linux kernel's memory management. By implementing PTE batching, the kernel can process multiple PTEs simultaneously when they map consecutive pages of the same large folio. This reduces the overhead associated with managing individual PTEs and improves the efficiency of the fork() operation. The changes also ensure that certain PTE bits, such as the writable bit, are ignored during batch processing, further enhancing performance. Overall, this optimization leads to faster process creation times and better memory utilization, making the Linux kernel more efficient and responsive."
    }
}

2024-12-03 22:41:05,538 - extractor - DEBUG - deepseek response:
ChatCompletion(id='33e20c7c-ae51-4554-b4a2-eb5bf0bd708c', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "concepts": ["PTE", "THP", "fork()", "folio", "PTE batching", "memory management", "process creation", "memory utilization"]\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733236862, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=47, prompt_tokens=9453, total_tokens=9500, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=1536, prompt_cache_miss_tokens=7917))
2024-12-03 22:41:05,539 - extractor - DEBUG - feature_extracted of feature_id=33219:
{
    "feature_id": 33219,
    "feature_description": "Optimize fork() with PTE-mapped THP",
    "version": "6.9",
    "commits": [
        {
            "commit_subject": "mm/memory: optimize fork() with PTE-mapped THP",
            "commit_message": "Let's implement PTE batching when consecutive (present) PTEs map\nconsecutive pages of the same large folio, and all other PTE bits besides\nthe PFNs are equal.\n\nWe will optimize folio_pte_batch() separately, to ignore selected PTE\nbits.  This patch is based on work by Ryan Roberts.\n\nUse __always_inline for __copy_present_ptes() and keep the handling for\nsingle PTEs completely separate from the multi-PTE case: we really want\nthe compiler to optimize for the single-PTE case with small folios, to not\ndegrade performance.\n\nNote that PTE batching will never exceed a single page table and will\nalways stay within VMA boundaries.\n\nFurther, processing PTE-mapped THP that maybe pinned and have\nPageAnonExclusive set on at least one subpage should work as expected, but\nthere is room for improvement: We will repeatedly (1) detect a PTE batch\n(2) detect that we have to copy a page (3) fall back and allocate a single\npage to copy a single page.  For now we won't care as pinned pages are a\ncorner case, and we should rather look into maintaining only a single\nPageAnonExclusive bit for large folios.\n\nLink: https://lkml.kernel.org/r/20240129124649.189745-14-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nReviewed-by: Mike Rapoport (IBM) <rppt@kernel.org>\nCc: Albert Ou <aou@eecs.berkeley.edu>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Alexandre Ghiti <alexghiti@rivosinc.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@kernel.org>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: David S. Miller <davem@davemloft.net>\nCc: Dinh Nguyen <dinguyen@kernel.org>\nCc: Gerald Schaefer <gerald.schaefer@linux.ibm.com>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Naveen N. Rao <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Palmer Dabbelt <palmer@dabbelt.com>\nCc: Paul Walmsley <paul.walmsley@sifive.com>\nCc: Russell King (Oracle) <linux@armlinux.org.uk>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm/memory: ignore writable bit in folio_pte_batch()",
            "commit_message": "...  and conditionally return to the caller if any PTE except the first\none is writable.  fork() has to make sure to properly write-protect in\ncase any PTE is writable.  Other users (e.g., page unmaping) are expected\nto not care.\n\nLink: https://lkml.kernel.org/r/20240129124649.189745-16-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: Albert Ou <aou@eecs.berkeley.edu>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Alexandre Ghiti <alexghiti@rivosinc.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@kernel.org>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: David S. Miller <davem@davemloft.net>\nCc: Dinh Nguyen <dinguyen@kernel.org>\nCc: Gerald Schaefer <gerald.schaefer@linux.ibm.com>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Naveen N. Rao <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Palmer Dabbelt <palmer@dabbelt.com>\nCc: Paul Walmsley <paul.walmsley@sifive.com>\nCc: Russell King (Oracle) <linux@armlinux.org.uk>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nCc: Mike Rapoport (IBM) <rppt@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm/memory: factor out copying the actual PTE in copy_present_pte()",
            "commit_message": "Let's prepare for further changes.\n\nLink: https://lkml.kernel.org/r/20240129124649.189745-12-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nReviewed-by: Mike Rapoport (IBM) <rppt@kernel.org>\nCc: Albert Ou <aou@eecs.berkeley.edu>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Alexandre Ghiti <alexghiti@rivosinc.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@kernel.org>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: David S. Miller <davem@davemloft.net>\nCc: Dinh Nguyen <dinguyen@kernel.org>\nCc: Gerald Schaefer <gerald.schaefer@linux.ibm.com>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Naveen N. Rao <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Palmer Dabbelt <palmer@dabbelt.com>\nCc: Paul Walmsley <paul.walmsley@sifive.com>\nCc: Russell King (Oracle) <linux@armlinux.org.uk>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "arm/pgtable: define PFN_PTE_SHIFT",
            "commit_message": "We want to make use of pte_next_pfn() outside of set_ptes().  Let's simply\ndefine PFN_PTE_SHIFT, required by pte_next_pfn().\n\nLink: https://lkml.kernel.org/r/20240129124649.189745-3-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nTested-by: Ryan Roberts <ryan.roberts@arm.com>\nReviewed-by: Mike Rapoport (IBM) <rppt@kernel.org>\nCc: Albert Ou <aou@eecs.berkeley.edu>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Alexandre Ghiti <alexghiti@rivosinc.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@kernel.org>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: David S. Miller <davem@davemloft.net>\nCc: Dinh Nguyen <dinguyen@kernel.org>\nCc: Gerald Schaefer <gerald.schaefer@linux.ibm.com>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Naveen N. Rao <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Palmer Dabbelt <palmer@dabbelt.com>\nCc: Paul Walmsley <paul.walmsley@sifive.com>\nCc: Russell King (Oracle) <linux@armlinux.org.uk>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm/memory: pass PTE to copy_present_pte()",
            "commit_message": "We already read it, let's just forward it.\n\nThis patch is based on work by Ryan Roberts.\n\n[david@redhat.com: fix the hmm \"exclusive_cow\" selftest]\n  Link: https://lkml.kernel.org/r/13f296b8-e882-47fd-b939-c2141dc28717@redhat.com\nLink: https://lkml.kernel.org/r/20240129124649.189745-13-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nReviewed-by: Mike Rapoport (IBM) <rppt@kernel.org>\nCc: Albert Ou <aou@eecs.berkeley.edu>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Alexandre Ghiti <alexghiti@rivosinc.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@kernel.org>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: David S. Miller <davem@davemloft.net>\nCc: Dinh Nguyen <dinguyen@kernel.org>\nCc: Gerald Schaefer <gerald.schaefer@linux.ibm.com>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Naveen N. Rao <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Palmer Dabbelt <palmer@dabbelt.com>\nCc: Paul Walmsley <paul.walmsley@sifive.com>\nCc: Russell King (Oracle) <linux@armlinux.org.uk>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm/memory: ignore dirty/accessed/soft-dirty bits in folio_pte_batch()",
            "commit_message": "Let's always ignore the accessed/young bit: we'll always mark the PTE as\nold in our child process during fork, and upcoming users will similarly\nnot care.\n\nIgnore the dirty bit only if we don't want to duplicate the dirty bit into\nthe child process during fork.  Maybe, we could just set all PTEs in the\nchild dirty if any PTE is dirty.  For now, let's keep the behavior\nunchanged, this can be optimized later if required.\n\nIgnore the soft-dirty bit only if the bit doesn't have any meaning in the\nsrc vma, and similarly won't have any in the copied dst vma.\n\nFor now, we won't bother with the uffd-wp bit.\n\nLink: https://lkml.kernel.org/r/20240129124649.189745-15-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Ryan Roberts <ryan.roberts@arm.com>\nCc: Albert Ou <aou@eecs.berkeley.edu>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Alexandre Ghiti <alexghiti@rivosinc.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@kernel.org>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: David S. Miller <davem@davemloft.net>\nCc: Dinh Nguyen <dinguyen@kernel.org>\nCc: Gerald Schaefer <gerald.schaefer@linux.ibm.com>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Naveen N. Rao <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Palmer Dabbelt <palmer@dabbelt.com>\nCc: Paul Walmsley <paul.walmsley@sifive.com>\nCc: Russell King (Oracle) <linux@armlinux.org.uk>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nCc: Mike Rapoport (IBM) <rppt@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "arm64/mm: make set_ptes() robust when OAs cross 48-bit boundary",
            "commit_message": "Patch series \"mm/memory: optimize fork() with PTE-mapped THP\", v3.\n\nNow that the rmap overhaul[1] is upstream that provides a clean interface\nfor rmap batching, let's implement PTE batching during fork when\nprocessing PTE-mapped THPs.\n\nThis series is partially based on Ryan's previous work[2] to implement\ncont-pte support on arm64, but its a complete rewrite based on [1] to\noptimize all architectures independent of any such PTE bits, and to use\nthe new rmap batching functions that simplify the code and prepare for\nfurther rmap accounting changes.\n\nWe collect consecutive PTEs that map consecutive pages of the same large\nfolio, making sure that the other PTE bits are compatible, and (a) adjust\nthe refcount only once per batch, (b) call rmap handling functions only\nonce per batch and (c) perform batch PTE setting/updates.\n\nWhile this series should be beneficial for adding cont-pte support on\nARM64[2], it's one of the requirements for maintaining a total mapcount[3]\nfor large folios with minimal added overhead and further changes[4] that\nbuild up on top of the total mapcount.\n\nIndependent of all that, this series results in a speedup during fork with\nPTE-mapped THP, which is the default with THPs that are smaller than a PMD\n(for example, 16KiB to 1024KiB mTHPs for anonymous memory[5]).\n\nOn an Intel Xeon Silver 4210R CPU, fork'ing with 1GiB of PTE-mapped folios\nof the same size (stddev < 1%) results in the following runtimes for\nfork() (shorter is better):\n\nFolio Size | v6.8-rc1 |      New | Change\n------------------------------------------\n      4KiB | 0.014328 | 0.014035 |   - 2%\n     16KiB | 0.014263 | 0.01196  |   -16%\n     32KiB | 0.014334 | 0.01094  |   -24%\n     64KiB | 0.014046 | 0.010444 |   -26%\n    128KiB | 0.014011 | 0.010063 |   -28%\n    256KiB | 0.013993 | 0.009938 |   -29%\n    512KiB | 0.013983 | 0.00985  |   -30%\n   1024KiB | 0.013986 | 0.00982  |   -30%\n   2048KiB | 0.014305 | 0.010076 |   -30%\n\nNote that these numbers are even better than the ones from v1 (verified\nover multiple reboots), even though there were only minimal code changes. \nWell, I removed a pte_mkclean() call for anon folios, maybe that also\nplays a role.\n\nBut my experience is that fork() is extremely sensitive to code size,\ninlining, ...  so I suspect we'll see on other architectures rather a\nchange of -20% instead of -30%, and it will be easy to \"lose\" some of that\nspeedup in the future by subtle code changes.\n\nNext up is PTE batching when unmapping.  Only tested on x86-64. \nCompile-tested on most other architectures.\n\n[1] https://lkml.kernel.org/r/20231220224504.646757-1-david@redhat.com\n[2] https://lkml.kernel.org/r/20231218105100.172635-1-ryan.roberts@arm.com\n[3] https://lkml.kernel.org/r/20230809083256.699513-1-david@redhat.com\n[4] https://lkml.kernel.org/r/20231124132626.235350-1-david@redhat.com\n[5] https://lkml.kernel.org/r/20231207161211.2374093-1-ryan.roberts@arm.com\n\n\nThis patch (of 15):\n\nSince the high bits [51:48] of an OA are not stored contiguously in the\nPTE, there is a theoretical bug in set_ptes(), which just adds PAGE_SIZE\nto the pte to get the pte with the next pfn.  This works until the pfn\ncrosses the 48-bit boundary, at which point we overflow into the upper\nattributes.\n\nOf course one could argue (and Matthew Wilcox has :) that we will never\nsee a folio cross this boundary because we only allow naturally aligned\npower-of-2 allocation, so this would require a half-petabyte folio.  So\nits only a theoretical bug.  But its better that the code is robust\nregardless.\n\nI've implemented pte_next_pfn() as part of the fix, which is an opt-in\ncore-mm interface.  So that is now available to the core-mm, which will be\nneeded shortly to support forthcoming fork()-batching optimizations.\n\nLink: https://lkml.kernel.org/r/20240129124649.189745-1-david@redhat.com\nLink: https://lkml.kernel.org/r/20240125173534.1659317-1-ryan.roberts@arm.com\nLink: https://lkml.kernel.org/r/20240129124649.189745-2-david@redhat.com\nFixes: 4a169d61c2ed (\"arm64: implement the new page table range API\")\nCloses: https://lore.kernel.org/linux-mm/fdaeb9a5-d890-499a-92c8-d171df43ad01@arm.com/\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Catalin Marinas <catalin.marinas@arm.com>\nReviewed-by: David Hildenbrand <david@redhat.com>\nTested-by: Ryan Roberts <ryan.roberts@arm.com>\nReviewed-by: Mike Rapoport (IBM) <rppt@kernel.org>\nCc: Albert Ou <aou@eecs.berkeley.edu>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@kernel.org>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: David S. Miller <davem@davemloft.net>\nCc: Dinh Nguyen <dinguyen@kernel.org>\nCc: Gerald Schaefer <gerald.schaefer@linux.ibm.com>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Naveen N. Rao <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Palmer Dabbelt <palmer@dabbelt.com>\nCc: Paul Walmsley <paul.walmsley@sifive.com>\nCc: Russell King (Oracle) <linux@armlinux.org.uk>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nCc: Alexandre Ghiti <alexghiti@rivosinc.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm/pgtable: make pte_next_pfn() independent of set_ptes()",
            "commit_message": "Let's provide pte_next_pfn(), independently of set_ptes().  This allows\nfor using the generic pte_next_pfn() version in some arch-specific\nset_ptes() implementations, and prepares for reusing pte_next_pfn() in\nother context.\n\nLink: https://lkml.kernel.org/r/20240129124649.189745-9-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nReviewed-by: Christophe Leroy <christophe.leroy@csgroup.eu>\nTested-by: Ryan Roberts <ryan.roberts@arm.com>\nReviewed-by: Mike Rapoport (IBM) <rppt@kernel.org>\nCc: Albert Ou <aou@eecs.berkeley.edu>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Alexandre Ghiti <alexghiti@rivosinc.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@kernel.org>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: David S. Miller <davem@davemloft.net>\nCc: Dinh Nguyen <dinguyen@kernel.org>\nCc: Gerald Schaefer <gerald.schaefer@linux.ibm.com>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Naveen N. Rao <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Palmer Dabbelt <palmer@dabbelt.com>\nCc: Paul Walmsley <paul.walmsley@sifive.com>\nCc: Russell King (Oracle) <linux@armlinux.org.uk>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "arm/mm: use pte_next_pfn() in set_ptes()",
            "commit_message": "Let's use our handy helper now that it's available on all archs.\n\nLink: https://lkml.kernel.org/r/20240129124649.189745-10-david@redhat.com\nSigned-off-by: David Hildenbrand <david@redhat.com>\nTested-by: Ryan Roberts <ryan.roberts@arm.com>\nReviewed-by: Mike Rapoport (IBM) <rppt@kernel.org>\nCc: Albert Ou <aou@eecs.berkeley.edu>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Alexandre Ghiti <alexghiti@rivosinc.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@kernel.org>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: David S. Miller <davem@davemloft.net>\nCc: Dinh Nguyen <dinguyen@kernel.org>\nCc: Gerald Schaefer <gerald.schaefer@linux.ibm.com>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: Matthew Wilcox <willy@infradead.org>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: Naveen N. Rao <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Palmer Dabbelt <palmer@dabbelt.com>\nCc: Paul Walmsley <paul.walmsley@sifive.com>\nCc: Russell King (Oracle) <linux@armlinux.org.uk>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        }
    ],
    "proper_nouns": [
        {
            "PTE": "Page Table Entry (PTE) is a data structure used in the Linux kernel to map virtual memory addresses to physical memory addresses. Each PTE contains information about a specific page of memory, including its physical address, access permissions, and other attributes."
        },
        {
            "THP": "Transparent Huge Pages (THP) is a feature in the Linux kernel that allows the use of larger memory pages (typically 2MB or 1GB) to reduce the overhead of managing smaller pages (usually 4KB). THP can improve performance by reducing the number of PTEs that need to be managed and by reducing the frequency of page faults."
        },
        {
            "fork()": "The fork() system call is a fundamental operation in Unix-like operating systems, including Linux. It creates a new process by duplicating the calling process, known as the parent process, to create a child process. The child process is an exact copy of the parent process, except for certain attributes like process ID."
        },
        {
            "folio": "In the context of the Linux kernel, a 'folio' is a term used to describe a large contiguous block of memory pages. Folios are used to manage memory more efficiently, especially when dealing with large files or when performing operations that span multiple pages."
        }
    ],
    "background_knowledge": [
        "The optimization of the fork() system call with PTE-mapped THP involves improving the efficiency of memory management during the creation of new processes. Specifically, this optimization focuses on batching PTEs (Page Table Entries) when they map consecutive pages of the same large folio, thereby reducing the overhead associated with managing individual PTEs.",
        "Understanding this feature requires familiarity with the concept of PTE batching, which involves grouping multiple PTEs together to perform operations more efficiently. Additionally, knowledge of how THP (Transparent Huge Pages) are managed and how they interact with the fork() system call is essential."
    ],
    "kernel_changes": [
        "Before the implementation of this feature, the fork() system call would handle each PTE individually, which could be inefficient, especially when dealing with large folios. The new optimization introduces PTE batching, where consecutive PTEs mapping consecutive pages of the same large folio are processed together. This reduces the number of operations required during the fork() process, leading to faster execution times and improved memory management.",
        "The changes also include modifications to the folio_pte_batch() function to ignore certain PTE bits, such as the writable bit, during batch processing. This ensures that the fork() operation can be performed more efficiently without compromising the integrity of the memory management system."
    ],
    "comprehensive_understanding": "The optimization of the fork() system call with PTE-mapped THP is a significant improvement in the Linux kernel's memory management. By implementing PTE batching, the kernel can process multiple PTEs simultaneously when they map consecutive pages of the same large folio. This reduces the overhead associated with managing individual PTEs and improves the efficiency of the fork() operation. The changes also ensure that certain PTE bits, such as the writable bit, are ignored during batch processing, further enhancing performance. Overall, this optimization leads to faster process creation times and better memory utilization, making the Linux kernel more efficient and responsive.",
    "concepts": [
        "PTE",
        "THP",
        "fork()",
        "folio",
        "PTE batching",
        "memory management",
        "process creation",
        "memory utilization"
    ]
}
2024-12-03 22:41:05,539 - extractor - INFO - ---------Processing feature 22/815---------
2024-12-03 22:41:09,191 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个 linux 内核的专家。给你一个 linux 内核特性的描述以及其对应的若干 commit 的标题和提交信息，你的任务是为这个特性写一个多方面的扩写版描述。你的扩写需包含以下几个方面：1. 该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)；2. 为了让开发者理解该特性，需要哪些具体的，与这个特性直接相关的背景知识 (background_knowledge)；3. 该特性被实现前后，linux 内核发生了怎样的变化 (kernel_changes)；4. 你对该特性的综合理解 (comprehensive_understanding)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}]
        },
        "output": {
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
        }
    ],
    "input": {
        "feature_description": "userfaultfd: handle zeropage moves by UFFDIO_MOVE",
        "commits": [{"commit_subject": "userfaultfd: handle zeropage moves by UFFDIO_MOVE", "commit_message": "Current implementation of UFFDIO_MOVE fails to move zeropages and returns\nEBUSY when it encounters one.  We can handle them by mapping a zeropage at\nthe destination and clearing the mapping at the source.  This is done both\nfor ordinary and for huge zeropages.\n\nLink: https://lkml.kernel.org/r/20240131175618.2417291-1-surenb@google.com\nSigned-off-by: Suren Baghdasaryan <surenb@google.com>\nReported-by: kernel test robot <lkp@intel.com>\nReported-by: Dan Carpenter <dan.carpenter@linaro.org>\nCloses: https://lore.kernel.org/r/202401300107.U8iMAkTl-lkp@intel.com/\nCc: Alexander Viro <viro@zeniv.linux.org.uk>\nCc: Andrea Arcangeli <aarcange@redhat.com>\nCc: Axel Rasmussen <axelrasmussen@google.com>\nCc: Brian Geffon <bgeffon@google.com>\nCc: Christian Brauner <brauner@kernel.org>\nCc: David Hildenbrand <david@redhat.com>\nCc: Hugh Dickins <hughd@google.com>\nCc: Jann Horn <jannh@google.com>\nCc: Kalesh Singh <kaleshsingh@google.com>\nCc: Liam R. Howlett <Liam.Howlett@oracle.com>\nCc: Lokesh Gidra <lokeshgidra@google.com>\nCc: Matthew Wilcox <willy@infradead.org>\nCc: Michal Hocko <mhocko@suse.com>\nCc: Mike Rapoport (IBM) <rppt@kernel.org>\nCc: Nicolas Geoffray <ngeoffray@google.com>\nCc: Peter Xu <peterx@redhat.com>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Shuah Khan <shuah@kernel.org>\nCc: ZhangPeng <zhangpeng362@huawei.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}]
    }
}

2024-12-03 22:41:37,970 - extractor - DEBUG - deepseek response:
ChatCompletion(id='321172e8-c856-4f62-a55a-fb2fb11f2ffd', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "proper_nouns": [\n        {\n            "userfaultfd": "userfaultfd is a Linux kernel feature that provides a mechanism for user-space applications to handle page faults in a controlled manner. It allows applications to register memory ranges and receive notifications when a page fault occurs in those ranges, enabling custom fault handling logic."\n        },\n        {\n            "UFFDIO_MOVE": "UFFDIO_MOVE is an ioctl command used with the userfaultfd system call to move a memory range from one address to another. This operation is particularly useful for memory management tasks such as migrating memory pages between different regions."\n        },\n        {\n            "zeropage": "A zeropage, also known as a zero page, is a special memory page filled with zeros. It is often used as a source for initializing new memory pages, as it avoids the need to physically allocate and zero out memory, thereby saving time and resources."\n        }\n    ],\n    "background_knowledge": [\n        "The userfaultfd system call provides a way for user-space applications to handle page faults. The UFFDIO_MOVE ioctl command allows moving memory ranges, which is useful for tasks like memory migration. However, the current implementation of UFFDIO_MOVE fails to handle zeropages, returning EBUSY when it encounters one."\n    ],\n    "kernel_changes": [\n        "Before this feature was implemented, the UFFDIO_MOVE operation would fail when encountering a zeropage, returning an EBUSY error. This limitation prevented the efficient handling of memory moves involving zeropages. After the implementation, the kernel was modified to handle zeropages during UFFDIO_MOVE operations by mapping a zeropage at the destination and clearing the mapping at the source. This change allows for the successful movement of both ordinary and huge zeropages, enhancing the functionality and reliability of the UFFDIO_MOVE operation."\n    ],\n    "comprehensive_understanding": "This feature enhances the userfaultfd system by enabling the UFFDIO_MOVE operation to handle zeropages effectively. Previously, encountering a zeropage during a memory move would result in an EBUSY error. With this update, the kernel now maps a zeropage at the destination and clears the mapping at the source, allowing for the successful movement of zeropages. This improvement not only expands the capabilities of the UFFDIO_MOVE operation but also ensures smoother and more efficient memory management in scenarios involving zeropages."\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733236869, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=548, prompt_tokens=2142, total_tokens=2690, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=1472, prompt_cache_miss_tokens=670))
2024-12-03 22:41:37,970 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个命名实体抽取 (NER) 任务的专家，同时也是 linux 内核的专家。给你一个 linux 内核特性的描述 (feature_description)、对应的若干 commit 的标题和提交信息 (commits)、该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)、为了让开发者理解该特性所需的与这个特性直接相关的背景知识 (background_knowledge)、该特性被实现前后，linux 内核发生的变化 (kernel_changes)、对该特性的综合理解 (comprehensive_understanding)。你的任务是：根据这些信息提取出在 linux 内核中涉及到的和这个特性相关的概念实体 (concepts)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}],
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": [
                "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
            ]
        },
        "output": {
            "concepts": ["Copy-on-Write (CoW)", "mTHP", "folio", "memory reclamation", "memory availability", "anti-fragmentation"]
        }
    ],
    "input": {
        "feature_description": "userfaultfd: handle zeropage moves by UFFDIO_MOVE",
        "commits": [{"commit_subject": "userfaultfd: handle zeropage moves by UFFDIO_MOVE", "commit_message": "Current implementation of UFFDIO_MOVE fails to move zeropages and returns\nEBUSY when it encounters one.  We can handle them by mapping a zeropage at\nthe destination and clearing the mapping at the source.  This is done both\nfor ordinary and for huge zeropages.\n\nLink: https://lkml.kernel.org/r/20240131175618.2417291-1-surenb@google.com\nSigned-off-by: Suren Baghdasaryan <surenb@google.com>\nReported-by: kernel test robot <lkp@intel.com>\nReported-by: Dan Carpenter <dan.carpenter@linaro.org>\nCloses: https://lore.kernel.org/r/202401300107.U8iMAkTl-lkp@intel.com/\nCc: Alexander Viro <viro@zeniv.linux.org.uk>\nCc: Andrea Arcangeli <aarcange@redhat.com>\nCc: Axel Rasmussen <axelrasmussen@google.com>\nCc: Brian Geffon <bgeffon@google.com>\nCc: Christian Brauner <brauner@kernel.org>\nCc: David Hildenbrand <david@redhat.com>\nCc: Hugh Dickins <hughd@google.com>\nCc: Jann Horn <jannh@google.com>\nCc: Kalesh Singh <kaleshsingh@google.com>\nCc: Liam R. Howlett <Liam.Howlett@oracle.com>\nCc: Lokesh Gidra <lokeshgidra@google.com>\nCc: Matthew Wilcox <willy@infradead.org>\nCc: Michal Hocko <mhocko@suse.com>\nCc: Mike Rapoport (IBM) <rppt@kernel.org>\nCc: Nicolas Geoffray <ngeoffray@google.com>\nCc: Peter Xu <peterx@redhat.com>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Shuah Khan <shuah@kernel.org>\nCc: ZhangPeng <zhangpeng362@huawei.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}],
        "proper_nouns": [{"userfaultfd": "userfaultfd is a Linux kernel feature that provides a mechanism for user-space applications to handle page faults in a controlled manner. It allows applications to register memory ranges and receive notifications when a page fault occurs in those ranges, enabling custom fault handling logic."}, {"UFFDIO_MOVE": "UFFDIO_MOVE is an ioctl command used with the userfaultfd system call to move a memory range from one address to another. This operation is particularly useful for memory management tasks such as migrating memory pages between different regions."}, {"zeropage": "A zeropage, also known as a zero page, is a special memory page filled with zeros. It is often used as a source for initializing new memory pages, as it avoids the need to physically allocate and zero out memory, thereby saving time and resources."}],
        "background_knowledge": ["The userfaultfd system call provides a way for user-space applications to handle page faults. The UFFDIO_MOVE ioctl command allows moving memory ranges, which is useful for tasks like memory migration. However, the current implementation of UFFDIO_MOVE fails to handle zeropages, returning EBUSY when it encounters one."],
        "kernel_changes": ["Before this feature was implemented, the UFFDIO_MOVE operation would fail when encountering a zeropage, returning an EBUSY error. This limitation prevented the efficient handling of memory moves involving zeropages. After the implementation, the kernel was modified to handle zeropages during UFFDIO_MOVE operations by mapping a zeropage at the destination and clearing the mapping at the source. This change allows for the successful movement of both ordinary and huge zeropages, enhancing the functionality and reliability of the UFFDIO_MOVE operation."],
        "comprehensive_understanding": "This feature enhances the userfaultfd system by enabling the UFFDIO_MOVE operation to handle zeropages effectively. Previously, encountering a zeropage during a memory move would result in an EBUSY error. With this update, the kernel now maps a zeropage at the destination and clears the mapping at the source, allowing for the successful movement of zeropages. This improvement not only expands the capabilities of the UFFDIO_MOVE operation but also ensures smoother and more efficient memory management in scenarios involving zeropages."
    }
}

2024-12-03 22:41:40,743 - extractor - DEBUG - deepseek response:
ChatCompletion(id='faba5ca0-78ac-48c0-9c65-8b0480f5422f', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "concepts": ["userfaultfd", "UFFDIO_MOVE", "zeropage", "memory management", "page faults", "memory migration"]\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733236897, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=43, prompt_tokens=2704, total_tokens=2747, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=1536, prompt_cache_miss_tokens=1168))
2024-12-03 22:41:40,744 - extractor - DEBUG - feature_extracted of feature_id=33218:
{
    "feature_id": 33218,
    "feature_description": "userfaultfd: handle zeropage moves by UFFDIO_MOVE",
    "version": "6.9",
    "commits": [
        {
            "commit_subject": "userfaultfd: handle zeropage moves by UFFDIO_MOVE",
            "commit_message": "Current implementation of UFFDIO_MOVE fails to move zeropages and returns\nEBUSY when it encounters one.  We can handle them by mapping a zeropage at\nthe destination and clearing the mapping at the source.  This is done both\nfor ordinary and for huge zeropages.\n\nLink: https://lkml.kernel.org/r/20240131175618.2417291-1-surenb@google.com\nSigned-off-by: Suren Baghdasaryan <surenb@google.com>\nReported-by: kernel test robot <lkp@intel.com>\nReported-by: Dan Carpenter <dan.carpenter@linaro.org>\nCloses: https://lore.kernel.org/r/202401300107.U8iMAkTl-lkp@intel.com/\nCc: Alexander Viro <viro@zeniv.linux.org.uk>\nCc: Andrea Arcangeli <aarcange@redhat.com>\nCc: Axel Rasmussen <axelrasmussen@google.com>\nCc: Brian Geffon <bgeffon@google.com>\nCc: Christian Brauner <brauner@kernel.org>\nCc: David Hildenbrand <david@redhat.com>\nCc: Hugh Dickins <hughd@google.com>\nCc: Jann Horn <jannh@google.com>\nCc: Kalesh Singh <kaleshsingh@google.com>\nCc: Liam R. Howlett <Liam.Howlett@oracle.com>\nCc: Lokesh Gidra <lokeshgidra@google.com>\nCc: Matthew Wilcox <willy@infradead.org>\nCc: Michal Hocko <mhocko@suse.com>\nCc: Mike Rapoport (IBM) <rppt@kernel.org>\nCc: Nicolas Geoffray <ngeoffray@google.com>\nCc: Peter Xu <peterx@redhat.com>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Shuah Khan <shuah@kernel.org>\nCc: ZhangPeng <zhangpeng362@huawei.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        }
    ],
    "proper_nouns": [
        {
            "userfaultfd": "userfaultfd is a Linux kernel feature that provides a mechanism for user-space applications to handle page faults in a controlled manner. It allows applications to register memory ranges and receive notifications when a page fault occurs in those ranges, enabling custom fault handling logic."
        },
        {
            "UFFDIO_MOVE": "UFFDIO_MOVE is an ioctl command used with the userfaultfd system call to move a memory range from one address to another. This operation is particularly useful for memory management tasks such as migrating memory pages between different regions."
        },
        {
            "zeropage": "A zeropage, also known as a zero page, is a special memory page filled with zeros. It is often used as a source for initializing new memory pages, as it avoids the need to physically allocate and zero out memory, thereby saving time and resources."
        }
    ],
    "background_knowledge": [
        "The userfaultfd system call provides a way for user-space applications to handle page faults. The UFFDIO_MOVE ioctl command allows moving memory ranges, which is useful for tasks like memory migration. However, the current implementation of UFFDIO_MOVE fails to handle zeropages, returning EBUSY when it encounters one."
    ],
    "kernel_changes": [
        "Before this feature was implemented, the UFFDIO_MOVE operation would fail when encountering a zeropage, returning an EBUSY error. This limitation prevented the efficient handling of memory moves involving zeropages. After the implementation, the kernel was modified to handle zeropages during UFFDIO_MOVE operations by mapping a zeropage at the destination and clearing the mapping at the source. This change allows for the successful movement of both ordinary and huge zeropages, enhancing the functionality and reliability of the UFFDIO_MOVE operation."
    ],
    "comprehensive_understanding": "This feature enhances the userfaultfd system by enabling the UFFDIO_MOVE operation to handle zeropages effectively. Previously, encountering a zeropage during a memory move would result in an EBUSY error. With this update, the kernel now maps a zeropage at the destination and clears the mapping at the source, allowing for the successful movement of zeropages. This improvement not only expands the capabilities of the UFFDIO_MOVE operation but also ensures smoother and more efficient memory management in scenarios involving zeropages.",
    "concepts": [
        "userfaultfd",
        "UFFDIO_MOVE",
        "zeropage",
        "memory management",
        "page faults",
        "memory migration"
    ]
}
2024-12-03 22:41:40,746 - extractor - INFO - ---------Processing feature 23/815---------
2024-12-03 22:42:00,109 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个 linux 内核的专家。给你一个 linux 内核特性的描述以及其对应的若干 commit 的标题和提交信息，你的任务是为这个特性写一个多方面的扩写版描述。你的扩写需包含以下几个方面：1. 该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)；2. 为了让开发者理解该特性，需要哪些具体的，与这个特性直接相关的背景知识 (background_knowledge)；3. 该特性被实现前后，linux 内核发生了怎样的变化 (kernel_changes)；4. 你对该特性的综合理解 (comprehensive_understanding)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}]
        },
        "output": {
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
        }
    ],
    "input": {
        "feature_description": "ptdump: Refactor CONFIG_DEBUG_WX and check_wx_pages debugfs attribute",
        "commits": [{"commit_subject": "mm: ptdump: add check_wx_pages debugfs attribute", "commit_message": "Add a readable attribute in debugfs to trigger a W^X pages check at any\ntime.\n\nTo trigger the test, just read /sys/kernel/debug/check_wx_pages It will\nreport FAILED if the test failed, SUCCESS otherwise.\n\nDetailed result is provided into dmesg.\n\nLink: https://lkml.kernel.org/r/e947fb1a9f3f5466344823e532d343ff194ae03d.1706610398.git.christophe.leroy@csgroup.eu\nSigned-off-by: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: Albert Ou <aou@eecs.berkeley.edu>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Alexandre Ghiti <alexghiti@rivosinc.com>\nCc: Andy Lutomirski <luto@kernel.org>\nCc: \"Aneesh Kumar K.V (IBM)\" <aneesh.kumar@kernel.org>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: Gerald Schaefer <gerald.schaefer@linux.ibm.com>\nCc: Greg KH <greg@kroah.com>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: Kees Cook <keescook@chromium.org>\nCc: Mark Rutland <mark.rutland@arm.com>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: \"Naveen N. Rao\" <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Palmer Dabbelt <palmer@dabbelt.com>\nCc: Paul Walmsley <paul.walmsley@sifive.com>\nCc: Peter Zijlstra <peterz@infradead.org>\nCc: Phong Tran <tranmanphong@gmail.com>\nCc: Russell King <linux@armlinux.org.uk>\nCc: Steven Price <steven.price@arm.com>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: ptdump: have ptdump_check_wx() return bool", "commit_message": "Have ptdump_check_wx() return true when the check is successful or false\notherwise.\n\n[akpm@linux-foundation.org: fix a couple of build issues (x86_64 allmodconfig)]\nLink: https://lkml.kernel.org/r/7943149fe955458cb7b57cd483bf41a3aad94684.1706610398.git.christophe.leroy@csgroup.eu\nSigned-off-by: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: Albert Ou <aou@eecs.berkeley.edu>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Alexandre Ghiti <alexghiti@rivosinc.com>\nCc: Andy Lutomirski <luto@kernel.org>\nCc: \"Aneesh Kumar K.V (IBM)\" <aneesh.kumar@kernel.org>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: Gerald Schaefer <gerald.schaefer@linux.ibm.com>\nCc: Greg KH <greg@kroah.com>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: Kees Cook <keescook@chromium.org>\nCc: Mark Rutland <mark.rutland@arm.com>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: \"Naveen N. Rao\" <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Palmer Dabbelt <palmer@dabbelt.com>\nCc: Paul Walmsley <paul.walmsley@sifive.com>\nCc: Peter Zijlstra <peterz@infradead.org>\nCc: Phong Tran <tranmanphong@gmail.com>\nCc: Russell King <linux@armlinux.org.uk>\nCc: Steven Price <steven.price@arm.com>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "powerpc,s390: ptdump: define ptdump_check_wx() regardless of CONFIG_DEBUG_WX", "commit_message": "Following patch will use ptdump_check_wx() regardless of CONFIG_DEBUG_WX,\nso define it at all times on powerpc and s390 just like other\narchitectures.  Though keep the WARN_ON_ONCE() only when CONFIG_DEBUG_WX\nis set.\n\nLink: https://lkml.kernel.org/r/07bfb04c7fec58e84413e91d2533581be357a696.1706610398.git.christophe.leroy@csgroup.eu\nSigned-off-by: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: Albert Ou <aou@eecs.berkeley.edu>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Alexandre Ghiti <alexghiti@rivosinc.com>\nCc: Andy Lutomirski <luto@kernel.org>\nCc: \"Aneesh Kumar K.V (IBM)\" <aneesh.kumar@kernel.org>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: Gerald Schaefer <gerald.schaefer@linux.ibm.com>\nCc: Greg KH <greg@kroah.com>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: Kees Cook <keescook@chromium.org>\nCc: Mark Rutland <mark.rutland@arm.com>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: \"Naveen N. Rao\" <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Palmer Dabbelt <palmer@dabbelt.com>\nCc: Paul Walmsley <paul.walmsley@sifive.com>\nCc: Peter Zijlstra <peterz@infradead.org>\nCc: Phong Tran <tranmanphong@gmail.com>\nCc: Russell King <linux@armlinux.org.uk>\nCc: Steven Price <steven.price@arm.com>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "arm64, powerpc, riscv, s390, x86: ptdump: refactor CONFIG_DEBUG_WX", "commit_message": "All architectures using the core ptdump functionality also implement\nCONFIG_DEBUG_WX, and they all do it more or less the same way, with a\nfunction called debug_checkwx() that is called by mark_rodata_ro(), which\nis a substitute to ptdump_check_wx() when CONFIG_DEBUG_WX is set and a\nno-op otherwise.\n\nRefactor by centrally defining debug_checkwx() in linux/ptdump.h and call\ndebug_checkwx() immediately after calling mark_rodata_ro() instead of\ncalling it at the end of every mark_rodata_ro().\n\nOn x86_32, mark_rodata_ro() first checks __supported_pte_mask has _PAGE_NX\nbefore calling debug_checkwx().  Now the check is inside the callee\nptdump_walk_pgd_level_checkwx().\n\nOn powerpc_64, mark_rodata_ro() bails out early before calling\nptdump_check_wx() when the MMU doesn't have KERNEL_RO feature.  The check\nis now also done in ptdump_check_wx() as it is called outside\nmark_rodata_ro().\n\nLink: https://lkml.kernel.org/r/a59b102d7964261d31ead0316a9f18628e4e7a8e.1706610398.git.christophe.leroy@csgroup.eu\nSigned-off-by: Christophe Leroy <christophe.leroy@csgroup.eu>\nReviewed-by: Alexandre Ghiti <alexghiti@rivosinc.com>\nCc: Albert Ou <aou@eecs.berkeley.edu>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Andy Lutomirski <luto@kernel.org>\nCc: \"Aneesh Kumar K.V (IBM)\" <aneesh.kumar@kernel.org>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: Gerald Schaefer <gerald.schaefer@linux.ibm.com>\nCc: Greg KH <greg@kroah.com>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: Kees Cook <keescook@chromium.org>\nCc: Mark Rutland <mark.rutland@arm.com>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: \"Naveen N. Rao\" <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Palmer Dabbelt <palmer@dabbelt.com>\nCc: Paul Walmsley <paul.walmsley@sifive.com>\nCc: Peter Zijlstra <peterz@infradead.org>\nCc: Phong Tran <tranmanphong@gmail.com>\nCc: Russell King <linux@armlinux.org.uk>\nCc: Steven Price <steven.price@arm.com>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "arm: ptdump: rename CONFIG_DEBUG_WX to CONFIG_ARM_DEBUG_WX", "commit_message": "Patch series \"mm: ptdump: Refactor CONFIG_DEBUG_WX and check_wx_pages\ndebugfs attribute\", v2.\n\nThis series refactors CONFIG_DEBUG_WX for the 5 architectures implementing\nCONFIG_GENERIC_PTDUMP\n\nFirst rename stuff in ARM which uses similar names while not implementing\nCONFIG_GENERIC_PTDUMP.\n\nThen define a generic version of debug_checkwx() that calls\nptdump_check_wx() when CONFIG_DEBUG_WX is set.  Call it immediately after\ncalling mark_rodata_ro() instead of calling it at the end of every\nmark_rodata_ro().\n\nThen implement a debugfs attribute that can be used to trigger a W^X test\nat anytime and regardless of CONFIG_DEBUG_WX\n\n\nThis patch (of 5):\n\nCONFIG_DEBUG_WX is a core option defined in mm/Kconfig.debug\n\nTo avoid any future conflict, rename ARM version into CONFIG_ARM_DEBUG_WX.\n\nLink: https://lore.kernel.org/lkml/20200422152656.GF676@willie-the-truck/T/#m802eaf33efd6f8d575939d157301b35ac0d4a64f\nLink: https://github.com/KSPP/linux/issues/35\nLink: https://lkml.kernel.org/r/cover.1706610398.git.christophe.leroy@csgroup.eu\nLink: https://lkml.kernel.org/r/fa297aa90caeb61eee2b70c6c5897a2ab58a9562.1706610398.git.christophe.leroy@csgroup.eu\nSigned-off-by: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: Albert Ou <aou@eecs.berkeley.edu>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Andy Lutomirski <luto@kernel.org>\nCc: \"Aneesh Kumar K.V (IBM)\" <aneesh.kumar@kernel.org>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: Gerald Schaefer <gerald.schaefer@linux.ibm.com>\nCc: Greg KH <greg@kroah.com>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: Kees Cook <keescook@chromium.org>\nCc: Mark Rutland <mark.rutland@arm.com>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: \"Naveen N. Rao\" <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Palmer Dabbelt <palmer@dabbelt.com>\nCc: Paul Walmsley <paul.walmsley@sifive.com>\nCc: Peter Zijlstra <peterz@infradead.org>\nCc: Phong Tran <tranmanphong@gmail.com>\nCc: Russell King <linux@armlinux.org.uk>\nCc: Steven Price <steven.price@arm.com>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nCc: Alexandre Ghiti <alexghiti@rivosinc.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}]
    }
}

2024-12-03 22:42:38,724 - extractor - DEBUG - deepseek response:
ChatCompletion(id='8abad236-ec87-4a7d-bfdb-d495c860ae24', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "proper_nouns": [\n        {\n            "CONFIG_DEBUG_WX": "CONFIG_DEBUG_WX is a kernel configuration option that enables debugging for Write-Xor-Execute (W^X) violations. W^X is a security feature that ensures memory pages are either writable or executable, but not both, thereby preventing certain types of exploits."\n        },\n        {\n            "debugfs": "debugfs is a virtual filesystem in the Linux kernel designed for debugging purposes. It provides an interface for kernel developers to expose debugging information and control various debugging features."\n        },\n        {\n            "W^X": "W^X (Write-Xor-Execute) is a security feature in the Linux kernel that ensures memory pages are either writable or executable, but not both. This prevents certain types of exploits that rely on executing code from writable memory regions."\n        },\n        {\n            "ptdump": "ptdump is a tool in the Linux kernel used for dumping page table information. It is primarily used for debugging and analysis of memory management and page table structures."\n        }\n    ],\n    "background_knowledge": [\n        "Understanding the W^X (Write-Xor-Execute) security feature is crucial for comprehending the purpose of this refactoring. W^X ensures that memory pages are either writable or executable, but not both, thereby enhancing security by preventing certain types of exploits.",\n        "Knowledge of the debugfs filesystem is necessary to understand how the new debugfs attribute \'check_wx_pages\' is used to trigger W^X checks at any time. This attribute allows developers to manually verify the W^X compliance of memory pages.",\n        "Familiarity with the ptdump tool is important as it is the primary mechanism used for checking W^X violations and dumping page table information."\n    ],\n    "kernel_changes": [\n        "Before this refactoring, the CONFIG_DEBUG_WX option was implemented differently across various architectures, leading to inconsistencies and potential conflicts. The refactoring unified the implementation of CONFIG_DEBUG_WX across architectures, making the codebase more maintainable and consistent.",\n        "The addition of the \'check_wx_pages\' debugfs attribute allows developers to manually trigger W^X checks at any time, providing more flexibility and control over the debugging process.",\n        "The refactoring also involved renaming CONFIG_DEBUG_WX to CONFIG_ARM_DEBUG_WX in the ARM architecture to avoid conflicts with the core CONFIG_DEBUG_WX option defined in mm/Kconfig.debug.",\n        "The ptdump_check_wx() function was modified to return a boolean value, indicating whether the W^X check was successful or not. This change enhances the clarity and usability of the function."\n    ],\n    "comprehensive_understanding": "This feature refactors the CONFIG_DEBUG_WX option and introduces a new debugfs attribute \'check_wx_pages\' to enhance the debugging and verification of W^X (Write-Xor-Execute) compliance in the Linux kernel. By unifying the implementation of CONFIG_DEBUG_WX across architectures and providing a flexible mechanism to trigger W^X checks, this refactoring improves the consistency, maintainability, and usability of the kernel\'s memory management and security features. The changes also enhance the clarity of the ptdump_check_wx() function by having it return a boolean value, making it easier for developers to understand and use."\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733236920, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=736, prompt_tokens=5756, total_tokens=6492, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=1472, prompt_cache_miss_tokens=4284))
2024-12-03 22:42:38,724 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个命名实体抽取 (NER) 任务的专家，同时也是 linux 内核的专家。给你一个 linux 内核特性的描述 (feature_description)、对应的若干 commit 的标题和提交信息 (commits)、该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)、为了让开发者理解该特性所需的与这个特性直接相关的背景知识 (background_knowledge)、该特性被实现前后，linux 内核发生的变化 (kernel_changes)、对该特性的综合理解 (comprehensive_understanding)。你的任务是：根据这些信息提取出在 linux 内核中涉及到的和这个特性相关的概念实体 (concepts)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}],
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": [
                "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
            ]
        },
        "output": {
            "concepts": ["Copy-on-Write (CoW)", "mTHP", "folio", "memory reclamation", "memory availability", "anti-fragmentation"]
        }
    ],
    "input": {
        "feature_description": "ptdump: Refactor CONFIG_DEBUG_WX and check_wx_pages debugfs attribute",
        "commits": [{"commit_subject": "mm: ptdump: add check_wx_pages debugfs attribute", "commit_message": "Add a readable attribute in debugfs to trigger a W^X pages check at any\ntime.\n\nTo trigger the test, just read /sys/kernel/debug/check_wx_pages It will\nreport FAILED if the test failed, SUCCESS otherwise.\n\nDetailed result is provided into dmesg.\n\nLink: https://lkml.kernel.org/r/e947fb1a9f3f5466344823e532d343ff194ae03d.1706610398.git.christophe.leroy@csgroup.eu\nSigned-off-by: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: Albert Ou <aou@eecs.berkeley.edu>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Alexandre Ghiti <alexghiti@rivosinc.com>\nCc: Andy Lutomirski <luto@kernel.org>\nCc: \"Aneesh Kumar K.V (IBM)\" <aneesh.kumar@kernel.org>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: Gerald Schaefer <gerald.schaefer@linux.ibm.com>\nCc: Greg KH <greg@kroah.com>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: Kees Cook <keescook@chromium.org>\nCc: Mark Rutland <mark.rutland@arm.com>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: \"Naveen N. Rao\" <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Palmer Dabbelt <palmer@dabbelt.com>\nCc: Paul Walmsley <paul.walmsley@sifive.com>\nCc: Peter Zijlstra <peterz@infradead.org>\nCc: Phong Tran <tranmanphong@gmail.com>\nCc: Russell King <linux@armlinux.org.uk>\nCc: Steven Price <steven.price@arm.com>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: ptdump: have ptdump_check_wx() return bool", "commit_message": "Have ptdump_check_wx() return true when the check is successful or false\notherwise.\n\n[akpm@linux-foundation.org: fix a couple of build issues (x86_64 allmodconfig)]\nLink: https://lkml.kernel.org/r/7943149fe955458cb7b57cd483bf41a3aad94684.1706610398.git.christophe.leroy@csgroup.eu\nSigned-off-by: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: Albert Ou <aou@eecs.berkeley.edu>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Alexandre Ghiti <alexghiti@rivosinc.com>\nCc: Andy Lutomirski <luto@kernel.org>\nCc: \"Aneesh Kumar K.V (IBM)\" <aneesh.kumar@kernel.org>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: Gerald Schaefer <gerald.schaefer@linux.ibm.com>\nCc: Greg KH <greg@kroah.com>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: Kees Cook <keescook@chromium.org>\nCc: Mark Rutland <mark.rutland@arm.com>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: \"Naveen N. Rao\" <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Palmer Dabbelt <palmer@dabbelt.com>\nCc: Paul Walmsley <paul.walmsley@sifive.com>\nCc: Peter Zijlstra <peterz@infradead.org>\nCc: Phong Tran <tranmanphong@gmail.com>\nCc: Russell King <linux@armlinux.org.uk>\nCc: Steven Price <steven.price@arm.com>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "powerpc,s390: ptdump: define ptdump_check_wx() regardless of CONFIG_DEBUG_WX", "commit_message": "Following patch will use ptdump_check_wx() regardless of CONFIG_DEBUG_WX,\nso define it at all times on powerpc and s390 just like other\narchitectures.  Though keep the WARN_ON_ONCE() only when CONFIG_DEBUG_WX\nis set.\n\nLink: https://lkml.kernel.org/r/07bfb04c7fec58e84413e91d2533581be357a696.1706610398.git.christophe.leroy@csgroup.eu\nSigned-off-by: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: Albert Ou <aou@eecs.berkeley.edu>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Alexandre Ghiti <alexghiti@rivosinc.com>\nCc: Andy Lutomirski <luto@kernel.org>\nCc: \"Aneesh Kumar K.V (IBM)\" <aneesh.kumar@kernel.org>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: Gerald Schaefer <gerald.schaefer@linux.ibm.com>\nCc: Greg KH <greg@kroah.com>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: Kees Cook <keescook@chromium.org>\nCc: Mark Rutland <mark.rutland@arm.com>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: \"Naveen N. Rao\" <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Palmer Dabbelt <palmer@dabbelt.com>\nCc: Paul Walmsley <paul.walmsley@sifive.com>\nCc: Peter Zijlstra <peterz@infradead.org>\nCc: Phong Tran <tranmanphong@gmail.com>\nCc: Russell King <linux@armlinux.org.uk>\nCc: Steven Price <steven.price@arm.com>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "arm64, powerpc, riscv, s390, x86: ptdump: refactor CONFIG_DEBUG_WX", "commit_message": "All architectures using the core ptdump functionality also implement\nCONFIG_DEBUG_WX, and they all do it more or less the same way, with a\nfunction called debug_checkwx() that is called by mark_rodata_ro(), which\nis a substitute to ptdump_check_wx() when CONFIG_DEBUG_WX is set and a\nno-op otherwise.\n\nRefactor by centrally defining debug_checkwx() in linux/ptdump.h and call\ndebug_checkwx() immediately after calling mark_rodata_ro() instead of\ncalling it at the end of every mark_rodata_ro().\n\nOn x86_32, mark_rodata_ro() first checks __supported_pte_mask has _PAGE_NX\nbefore calling debug_checkwx().  Now the check is inside the callee\nptdump_walk_pgd_level_checkwx().\n\nOn powerpc_64, mark_rodata_ro() bails out early before calling\nptdump_check_wx() when the MMU doesn't have KERNEL_RO feature.  The check\nis now also done in ptdump_check_wx() as it is called outside\nmark_rodata_ro().\n\nLink: https://lkml.kernel.org/r/a59b102d7964261d31ead0316a9f18628e4e7a8e.1706610398.git.christophe.leroy@csgroup.eu\nSigned-off-by: Christophe Leroy <christophe.leroy@csgroup.eu>\nReviewed-by: Alexandre Ghiti <alexghiti@rivosinc.com>\nCc: Albert Ou <aou@eecs.berkeley.edu>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Andy Lutomirski <luto@kernel.org>\nCc: \"Aneesh Kumar K.V (IBM)\" <aneesh.kumar@kernel.org>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: Gerald Schaefer <gerald.schaefer@linux.ibm.com>\nCc: Greg KH <greg@kroah.com>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: Kees Cook <keescook@chromium.org>\nCc: Mark Rutland <mark.rutland@arm.com>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: \"Naveen N. Rao\" <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Palmer Dabbelt <palmer@dabbelt.com>\nCc: Paul Walmsley <paul.walmsley@sifive.com>\nCc: Peter Zijlstra <peterz@infradead.org>\nCc: Phong Tran <tranmanphong@gmail.com>\nCc: Russell King <linux@armlinux.org.uk>\nCc: Steven Price <steven.price@arm.com>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "arm: ptdump: rename CONFIG_DEBUG_WX to CONFIG_ARM_DEBUG_WX", "commit_message": "Patch series \"mm: ptdump: Refactor CONFIG_DEBUG_WX and check_wx_pages\ndebugfs attribute\", v2.\n\nThis series refactors CONFIG_DEBUG_WX for the 5 architectures implementing\nCONFIG_GENERIC_PTDUMP\n\nFirst rename stuff in ARM which uses similar names while not implementing\nCONFIG_GENERIC_PTDUMP.\n\nThen define a generic version of debug_checkwx() that calls\nptdump_check_wx() when CONFIG_DEBUG_WX is set.  Call it immediately after\ncalling mark_rodata_ro() instead of calling it at the end of every\nmark_rodata_ro().\n\nThen implement a debugfs attribute that can be used to trigger a W^X test\nat anytime and regardless of CONFIG_DEBUG_WX\n\n\nThis patch (of 5):\n\nCONFIG_DEBUG_WX is a core option defined in mm/Kconfig.debug\n\nTo avoid any future conflict, rename ARM version into CONFIG_ARM_DEBUG_WX.\n\nLink: https://lore.kernel.org/lkml/20200422152656.GF676@willie-the-truck/T/#m802eaf33efd6f8d575939d157301b35ac0d4a64f\nLink: https://github.com/KSPP/linux/issues/35\nLink: https://lkml.kernel.org/r/cover.1706610398.git.christophe.leroy@csgroup.eu\nLink: https://lkml.kernel.org/r/fa297aa90caeb61eee2b70c6c5897a2ab58a9562.1706610398.git.christophe.leroy@csgroup.eu\nSigned-off-by: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: Albert Ou <aou@eecs.berkeley.edu>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Andy Lutomirski <luto@kernel.org>\nCc: \"Aneesh Kumar K.V (IBM)\" <aneesh.kumar@kernel.org>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: Gerald Schaefer <gerald.schaefer@linux.ibm.com>\nCc: Greg KH <greg@kroah.com>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: Kees Cook <keescook@chromium.org>\nCc: Mark Rutland <mark.rutland@arm.com>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: \"Naveen N. Rao\" <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Palmer Dabbelt <palmer@dabbelt.com>\nCc: Paul Walmsley <paul.walmsley@sifive.com>\nCc: Peter Zijlstra <peterz@infradead.org>\nCc: Phong Tran <tranmanphong@gmail.com>\nCc: Russell King <linux@armlinux.org.uk>\nCc: Steven Price <steven.price@arm.com>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nCc: Alexandre Ghiti <alexghiti@rivosinc.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}],
        "proper_nouns": [{"CONFIG_DEBUG_WX": "CONFIG_DEBUG_WX is a kernel configuration option that enables debugging for Write-Xor-Execute (W^X) violations. W^X is a security feature that ensures memory pages are either writable or executable, but not both, thereby preventing certain types of exploits."}, {"debugfs": "debugfs is a virtual filesystem in the Linux kernel designed for debugging purposes. It provides an interface for kernel developers to expose debugging information and control various debugging features."}, {"W^X": "W^X (Write-Xor-Execute) is a security feature in the Linux kernel that ensures memory pages are either writable or executable, but not both. This prevents certain types of exploits that rely on executing code from writable memory regions."}, {"ptdump": "ptdump is a tool in the Linux kernel used for dumping page table information. It is primarily used for debugging and analysis of memory management and page table structures."}],
        "background_knowledge": ["Understanding the W^X (Write-Xor-Execute) security feature is crucial for comprehending the purpose of this refactoring. W^X ensures that memory pages are either writable or executable, but not both, thereby enhancing security by preventing certain types of exploits.", "Knowledge of the debugfs filesystem is necessary to understand how the new debugfs attribute 'check_wx_pages' is used to trigger W^X checks at any time. This attribute allows developers to manually verify the W^X compliance of memory pages.", "Familiarity with the ptdump tool is important as it is the primary mechanism used for checking W^X violations and dumping page table information."],
        "kernel_changes": ["Before this refactoring, the CONFIG_DEBUG_WX option was implemented differently across various architectures, leading to inconsistencies and potential conflicts. The refactoring unified the implementation of CONFIG_DEBUG_WX across architectures, making the codebase more maintainable and consistent.", "The addition of the 'check_wx_pages' debugfs attribute allows developers to manually trigger W^X checks at any time, providing more flexibility and control over the debugging process.", "The refactoring also involved renaming CONFIG_DEBUG_WX to CONFIG_ARM_DEBUG_WX in the ARM architecture to avoid conflicts with the core CONFIG_DEBUG_WX option defined in mm/Kconfig.debug.", "The ptdump_check_wx() function was modified to return a boolean value, indicating whether the W^X check was successful or not. This change enhances the clarity and usability of the function."],
        "comprehensive_understanding": "This feature refactors the CONFIG_DEBUG_WX option and introduces a new debugfs attribute 'check_wx_pages' to enhance the debugging and verification of W^X (Write-Xor-Execute) compliance in the Linux kernel. By unifying the implementation of CONFIG_DEBUG_WX across architectures and providing a flexible mechanism to trigger W^X checks, this refactoring improves the consistency, maintainability, and usability of the kernel's memory management and security features. The changes also enhance the clarity of the ptdump_check_wx() function by having it return a boolean value, making it easier for developers to understand and use."
    }
}

2024-12-03 22:42:42,731 - extractor - DEBUG - deepseek response:
ChatCompletion(id='f88f2560-887d-4e02-bb05-1812d6e0f06c', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "concepts": ["CONFIG_DEBUG_WX", "debugfs", "W^X", "ptdump", "memory management", "security features", "page table", "Write-Xor-Execute compliance", "debugging", "kernel configuration"]\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733236958, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=63, prompt_tokens=6489, total_tokens=6552, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=1536, prompt_cache_miss_tokens=4953))
2024-12-03 22:42:42,731 - extractor - DEBUG - feature_extracted of feature_id=33217:
{
    "feature_id": 33217,
    "feature_description": "ptdump: Refactor CONFIG_DEBUG_WX and check_wx_pages debugfs attribute",
    "version": "6.9",
    "commits": [
        {
            "commit_subject": "mm: ptdump: add check_wx_pages debugfs attribute",
            "commit_message": "Add a readable attribute in debugfs to trigger a W^X pages check at any\ntime.\n\nTo trigger the test, just read /sys/kernel/debug/check_wx_pages It will\nreport FAILED if the test failed, SUCCESS otherwise.\n\nDetailed result is provided into dmesg.\n\nLink: https://lkml.kernel.org/r/e947fb1a9f3f5466344823e532d343ff194ae03d.1706610398.git.christophe.leroy@csgroup.eu\nSigned-off-by: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: Albert Ou <aou@eecs.berkeley.edu>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Alexandre Ghiti <alexghiti@rivosinc.com>\nCc: Andy Lutomirski <luto@kernel.org>\nCc: \"Aneesh Kumar K.V (IBM)\" <aneesh.kumar@kernel.org>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: Gerald Schaefer <gerald.schaefer@linux.ibm.com>\nCc: Greg KH <greg@kroah.com>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: Kees Cook <keescook@chromium.org>\nCc: Mark Rutland <mark.rutland@arm.com>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: \"Naveen N. Rao\" <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Palmer Dabbelt <palmer@dabbelt.com>\nCc: Paul Walmsley <paul.walmsley@sifive.com>\nCc: Peter Zijlstra <peterz@infradead.org>\nCc: Phong Tran <tranmanphong@gmail.com>\nCc: Russell King <linux@armlinux.org.uk>\nCc: Steven Price <steven.price@arm.com>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm: ptdump: have ptdump_check_wx() return bool",
            "commit_message": "Have ptdump_check_wx() return true when the check is successful or false\notherwise.\n\n[akpm@linux-foundation.org: fix a couple of build issues (x86_64 allmodconfig)]\nLink: https://lkml.kernel.org/r/7943149fe955458cb7b57cd483bf41a3aad94684.1706610398.git.christophe.leroy@csgroup.eu\nSigned-off-by: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: Albert Ou <aou@eecs.berkeley.edu>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Alexandre Ghiti <alexghiti@rivosinc.com>\nCc: Andy Lutomirski <luto@kernel.org>\nCc: \"Aneesh Kumar K.V (IBM)\" <aneesh.kumar@kernel.org>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: Gerald Schaefer <gerald.schaefer@linux.ibm.com>\nCc: Greg KH <greg@kroah.com>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: Kees Cook <keescook@chromium.org>\nCc: Mark Rutland <mark.rutland@arm.com>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: \"Naveen N. Rao\" <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Palmer Dabbelt <palmer@dabbelt.com>\nCc: Paul Walmsley <paul.walmsley@sifive.com>\nCc: Peter Zijlstra <peterz@infradead.org>\nCc: Phong Tran <tranmanphong@gmail.com>\nCc: Russell King <linux@armlinux.org.uk>\nCc: Steven Price <steven.price@arm.com>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "powerpc,s390: ptdump: define ptdump_check_wx() regardless of CONFIG_DEBUG_WX",
            "commit_message": "Following patch will use ptdump_check_wx() regardless of CONFIG_DEBUG_WX,\nso define it at all times on powerpc and s390 just like other\narchitectures.  Though keep the WARN_ON_ONCE() only when CONFIG_DEBUG_WX\nis set.\n\nLink: https://lkml.kernel.org/r/07bfb04c7fec58e84413e91d2533581be357a696.1706610398.git.christophe.leroy@csgroup.eu\nSigned-off-by: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: Albert Ou <aou@eecs.berkeley.edu>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Alexandre Ghiti <alexghiti@rivosinc.com>\nCc: Andy Lutomirski <luto@kernel.org>\nCc: \"Aneesh Kumar K.V (IBM)\" <aneesh.kumar@kernel.org>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: Gerald Schaefer <gerald.schaefer@linux.ibm.com>\nCc: Greg KH <greg@kroah.com>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: Kees Cook <keescook@chromium.org>\nCc: Mark Rutland <mark.rutland@arm.com>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: \"Naveen N. Rao\" <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Palmer Dabbelt <palmer@dabbelt.com>\nCc: Paul Walmsley <paul.walmsley@sifive.com>\nCc: Peter Zijlstra <peterz@infradead.org>\nCc: Phong Tran <tranmanphong@gmail.com>\nCc: Russell King <linux@armlinux.org.uk>\nCc: Steven Price <steven.price@arm.com>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "arm64, powerpc, riscv, s390, x86: ptdump: refactor CONFIG_DEBUG_WX",
            "commit_message": "All architectures using the core ptdump functionality also implement\nCONFIG_DEBUG_WX, and they all do it more or less the same way, with a\nfunction called debug_checkwx() that is called by mark_rodata_ro(), which\nis a substitute to ptdump_check_wx() when CONFIG_DEBUG_WX is set and a\nno-op otherwise.\n\nRefactor by centrally defining debug_checkwx() in linux/ptdump.h and call\ndebug_checkwx() immediately after calling mark_rodata_ro() instead of\ncalling it at the end of every mark_rodata_ro().\n\nOn x86_32, mark_rodata_ro() first checks __supported_pte_mask has _PAGE_NX\nbefore calling debug_checkwx().  Now the check is inside the callee\nptdump_walk_pgd_level_checkwx().\n\nOn powerpc_64, mark_rodata_ro() bails out early before calling\nptdump_check_wx() when the MMU doesn't have KERNEL_RO feature.  The check\nis now also done in ptdump_check_wx() as it is called outside\nmark_rodata_ro().\n\nLink: https://lkml.kernel.org/r/a59b102d7964261d31ead0316a9f18628e4e7a8e.1706610398.git.christophe.leroy@csgroup.eu\nSigned-off-by: Christophe Leroy <christophe.leroy@csgroup.eu>\nReviewed-by: Alexandre Ghiti <alexghiti@rivosinc.com>\nCc: Albert Ou <aou@eecs.berkeley.edu>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Andy Lutomirski <luto@kernel.org>\nCc: \"Aneesh Kumar K.V (IBM)\" <aneesh.kumar@kernel.org>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: Gerald Schaefer <gerald.schaefer@linux.ibm.com>\nCc: Greg KH <greg@kroah.com>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: Kees Cook <keescook@chromium.org>\nCc: Mark Rutland <mark.rutland@arm.com>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: \"Naveen N. Rao\" <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Palmer Dabbelt <palmer@dabbelt.com>\nCc: Paul Walmsley <paul.walmsley@sifive.com>\nCc: Peter Zijlstra <peterz@infradead.org>\nCc: Phong Tran <tranmanphong@gmail.com>\nCc: Russell King <linux@armlinux.org.uk>\nCc: Steven Price <steven.price@arm.com>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "arm: ptdump: rename CONFIG_DEBUG_WX to CONFIG_ARM_DEBUG_WX",
            "commit_message": "Patch series \"mm: ptdump: Refactor CONFIG_DEBUG_WX and check_wx_pages\ndebugfs attribute\", v2.\n\nThis series refactors CONFIG_DEBUG_WX for the 5 architectures implementing\nCONFIG_GENERIC_PTDUMP\n\nFirst rename stuff in ARM which uses similar names while not implementing\nCONFIG_GENERIC_PTDUMP.\n\nThen define a generic version of debug_checkwx() that calls\nptdump_check_wx() when CONFIG_DEBUG_WX is set.  Call it immediately after\ncalling mark_rodata_ro() instead of calling it at the end of every\nmark_rodata_ro().\n\nThen implement a debugfs attribute that can be used to trigger a W^X test\nat anytime and regardless of CONFIG_DEBUG_WX\n\n\nThis patch (of 5):\n\nCONFIG_DEBUG_WX is a core option defined in mm/Kconfig.debug\n\nTo avoid any future conflict, rename ARM version into CONFIG_ARM_DEBUG_WX.\n\nLink: https://lore.kernel.org/lkml/20200422152656.GF676@willie-the-truck/T/#m802eaf33efd6f8d575939d157301b35ac0d4a64f\nLink: https://github.com/KSPP/linux/issues/35\nLink: https://lkml.kernel.org/r/cover.1706610398.git.christophe.leroy@csgroup.eu\nLink: https://lkml.kernel.org/r/fa297aa90caeb61eee2b70c6c5897a2ab58a9562.1706610398.git.christophe.leroy@csgroup.eu\nSigned-off-by: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: Albert Ou <aou@eecs.berkeley.edu>\nCc: Alexander Gordeev <agordeev@linux.ibm.com>\nCc: Andy Lutomirski <luto@kernel.org>\nCc: \"Aneesh Kumar K.V (IBM)\" <aneesh.kumar@kernel.org>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Christian Borntraeger <borntraeger@linux.ibm.com>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: Gerald Schaefer <gerald.schaefer@linux.ibm.com>\nCc: Greg KH <greg@kroah.com>\nCc: Heiko Carstens <hca@linux.ibm.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: Kees Cook <keescook@chromium.org>\nCc: Mark Rutland <mark.rutland@arm.com>\nCc: Michael Ellerman <mpe@ellerman.id.au>\nCc: \"Naveen N. Rao\" <naveen.n.rao@linux.ibm.com>\nCc: Nicholas Piggin <npiggin@gmail.com>\nCc: Palmer Dabbelt <palmer@dabbelt.com>\nCc: Paul Walmsley <paul.walmsley@sifive.com>\nCc: Peter Zijlstra <peterz@infradead.org>\nCc: Phong Tran <tranmanphong@gmail.com>\nCc: Russell King <linux@armlinux.org.uk>\nCc: Steven Price <steven.price@arm.com>\nCc: Sven Schnelle <svens@linux.ibm.com>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Vasily Gorbik <gor@linux.ibm.com>\nCc: Will Deacon <will@kernel.org>\nCc: Alexandre Ghiti <alexghiti@rivosinc.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        }
    ],
    "proper_nouns": [
        {
            "CONFIG_DEBUG_WX": "CONFIG_DEBUG_WX is a kernel configuration option that enables debugging for Write-Xor-Execute (W^X) violations. W^X is a security feature that ensures memory pages are either writable or executable, but not both, thereby preventing certain types of exploits."
        },
        {
            "debugfs": "debugfs is a virtual filesystem in the Linux kernel designed for debugging purposes. It provides an interface for kernel developers to expose debugging information and control various debugging features."
        },
        {
            "W^X": "W^X (Write-Xor-Execute) is a security feature in the Linux kernel that ensures memory pages are either writable or executable, but not both. This prevents certain types of exploits that rely on executing code from writable memory regions."
        },
        {
            "ptdump": "ptdump is a tool in the Linux kernel used for dumping page table information. It is primarily used for debugging and analysis of memory management and page table structures."
        }
    ],
    "background_knowledge": [
        "Understanding the W^X (Write-Xor-Execute) security feature is crucial for comprehending the purpose of this refactoring. W^X ensures that memory pages are either writable or executable, but not both, thereby enhancing security by preventing certain types of exploits.",
        "Knowledge of the debugfs filesystem is necessary to understand how the new debugfs attribute 'check_wx_pages' is used to trigger W^X checks at any time. This attribute allows developers to manually verify the W^X compliance of memory pages.",
        "Familiarity with the ptdump tool is important as it is the primary mechanism used for checking W^X violations and dumping page table information."
    ],
    "kernel_changes": [
        "Before this refactoring, the CONFIG_DEBUG_WX option was implemented differently across various architectures, leading to inconsistencies and potential conflicts. The refactoring unified the implementation of CONFIG_DEBUG_WX across architectures, making the codebase more maintainable and consistent.",
        "The addition of the 'check_wx_pages' debugfs attribute allows developers to manually trigger W^X checks at any time, providing more flexibility and control over the debugging process.",
        "The refactoring also involved renaming CONFIG_DEBUG_WX to CONFIG_ARM_DEBUG_WX in the ARM architecture to avoid conflicts with the core CONFIG_DEBUG_WX option defined in mm/Kconfig.debug.",
        "The ptdump_check_wx() function was modified to return a boolean value, indicating whether the W^X check was successful or not. This change enhances the clarity and usability of the function."
    ],
    "comprehensive_understanding": "This feature refactors the CONFIG_DEBUG_WX option and introduces a new debugfs attribute 'check_wx_pages' to enhance the debugging and verification of W^X (Write-Xor-Execute) compliance in the Linux kernel. By unifying the implementation of CONFIG_DEBUG_WX across architectures and providing a flexible mechanism to trigger W^X checks, this refactoring improves the consistency, maintainability, and usability of the kernel's memory management and security features. The changes also enhance the clarity of the ptdump_check_wx() function by having it return a boolean value, making it easier for developers to understand and use.",
    "concepts": [
        "CONFIG_DEBUG_WX",
        "debugfs",
        "W^X",
        "ptdump",
        "memory management",
        "security features",
        "page table",
        "Write-Xor-Execute compliance",
        "debugging",
        "kernel configuration"
    ]
}
2024-12-03 22:42:42,732 - extractor - INFO - ---------Processing feature 24/815---------
2024-12-03 22:42:50,950 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个 linux 内核的专家。给你一个 linux 内核特性的描述以及其对应的若干 commit 的标题和提交信息，你的任务是为这个特性写一个多方面的扩写版描述。你的扩写需包含以下几个方面：1. 该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)；2. 为了让开发者理解该特性，需要哪些具体的，与这个特性直接相关的背景知识 (background_knowledge)；3. 该特性被实现前后，linux 内核发生了怎样的变化 (kernel_changes)；4. 你对该特性的综合理解 (comprehensive_understanding)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}]
        },
        "output": {
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
        }
    ],
    "input": {
        "feature_description": "zswap: optimize the scalability of zswap rb-tree",
        "commits": [{"commit_subject": "mm/zswap: split zswap rb-tree", "commit_message": "Each swapfile has one rb-tree to search the mapping of swp_entry_t to\nzswap_entry, that use a spinlock to protect, which can cause heavy lock\ncontention if multiple tasks zswap_store/load concurrently.\n\nOptimize the scalability problem by splitting the zswap rb-tree into\nmultiple rb-trees, each corresponds to SWAP_ADDRESS_SPACE_PAGES (64M),\njust like we did in the swap cache address_space splitting.\n\nAlthough this method can't solve the spinlock contention completely, it\ncan mitigate much of that contention.  Below is the results of kernel\nbuild in tmpfs with zswap shrinker enabled:\n\n     linux-next  zswap-lock-optimize\nreal 1m9.181s    1m3.820s\nuser 17m44.036s  17m40.100s\nsys  7m37.297s   4m54.622s\n\nSo there are clearly improvements.\n\nLink: https://lkml.kernel.org/r/20240117-b4-zswap-lock-optimize-v2-2-b5cc55479090@bytedance.com\nSigned-off-by: Chengming Zhou <zhouchengming@bytedance.com>\nAcked-by: Johannes Weiner <hannes@cmpxchg.org>\nAcked-by: Nhat Pham <nphamcs@gmail.com>\nAcked-by: Yosry Ahmed <yosryahmed@google.com>\nCc: Chris Li <chriscli@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/zswap: make sure each swapfile always have zswap rb-tree", "commit_message": "Patch series \"mm/zswap: optimize the scalability of zswap rb-tree\", v2.\n\nWhen testing the zswap performance by using kernel build -j32 in a tmpfs\ndirectory, I found the scalability of zswap rb-tree is not good, which is\nprotected by the only spinlock.  That would cause heavy lock contention if\nmultiple tasks zswap_store/load concurrently.\n\nSo a simple solution is to split the only one zswap rb-tree into multiple\nrb-trees, each corresponds to SWAP_ADDRESS_SPACE_PAGES (64M).  This idea\nis from the commit 4b3ef9daa4fc (\"mm/swap: split swap cache into 64MB\ntrunks\").\n\nAlthough this method can't solve the spinlock contention completely, it\ncan mitigate much of that contention.  Below is the results of kernel\nbuild in tmpfs with zswap shrinker enabled:\n\n     linux-next  zswap-lock-optimize\nreal 1m9.181s    1m3.820s\nuser 17m44.036s  17m40.100s\nsys  7m37.297s   4m54.622s\n\nSo there are clearly improvements.  And it's complementary with the\nongoing zswap xarray conversion by Chris.  Anyway, I think we can also\nmerge this first, it's complementary IMHO.  So I just refresh and resend\nthis for further discussion.\n\n\nThis patch (of 2):\n\nNot all zswap interfaces can handle the absence of the zswap rb-tree,\nactually only zswap_store() has handled it for now.\n\nTo make things simple, we make sure each swapfile always have the zswap\nrb-tree prepared before being enabled and used.  The preparation is\nunlikely to fail in practice, this patch just make it explicit.\n\nLink: https://lkml.kernel.org/r/20240117-b4-zswap-lock-optimize-v2-0-b5cc55479090@bytedance.com\nLink: https://lkml.kernel.org/r/20240117-b4-zswap-lock-optimize-v2-1-b5cc55479090@bytedance.com\nSigned-off-by: Chengming Zhou <zhouchengming@bytedance.com>\nAcked-by: Nhat Pham <nphamcs@gmail.com>\nAcked-by: Johannes Weiner <hannes@cmpxchg.org>\nAcked-by: Yosry Ahmed <yosryahmed@google.com>\nCc: Chris Li <chriscli@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}]
    }
}

2024-12-03 22:43:32,576 - extractor - DEBUG - deepseek response:
ChatCompletion(id='e775a9c3-9161-44a5-84b1-027a896624d9', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "proper_nouns": [\n        {\n            "zswap": "zswap is a lightweight compressed cache in the Linux kernel that sits between memory and swap space. It is designed to reduce the number of times data is written to disk by compressing data in memory before it is swapped out. This can significantly improve performance by reducing disk I/O, especially in systems with limited swap space."\n        },\n        {\n            "rb-tree": "An rb-tree, or Red-Black Tree, is a type of self-balancing binary search tree. In the context of the Linux kernel, rb-trees are often used for efficient data storage and retrieval, especially in scenarios where data needs to be quickly accessed and modified. In zswap, an rb-tree is used to map swap entries (swp_entry_t) to zswap entries, facilitating quick lookups and updates."\n        },\n        {\n            "spinlock": "A spinlock is a synchronization primitive used in the Linux kernel to protect shared resources from concurrent access. It works by causing a thread to repeatedly check (or \'spin\') until the lock becomes available. While spinlocks are efficient in scenarios with short lock hold times, they can lead to significant performance degradation if held for extended periods or if there is high contention for the lock."\n        },\n        {\n            "SWAP_ADDRESS_SPACE_PAGES": "SWAP_ADDRESS_SPACE_PAGES is a constant in the Linux kernel that defines the size of the address space used for swap operations. In the context of zswap, this constant is used to determine the size of each segment when splitting the rb-tree to improve scalability. The value is typically set to 64MB, which corresponds to the size of each segment in the split rb-tree."\n        }\n    ],\n    "background_knowledge": [\n        "zswap operates by compressing data in memory before it is written to swap space, thereby reducing the amount of data that needs to be written to disk. This process involves mapping swap entries (swp_entry_t) to zswap entries, which are stored in an rb-tree for quick access.",\n        "The original implementation of zswap used a single rb-tree protected by a spinlock, which could lead to significant lock contention when multiple tasks attempted to store or load data concurrently. This contention could degrade performance, especially in high-concurrency scenarios."\n    ],\n    "kernel_changes": [\n        "Before the optimization, zswap used a single rb-tree protected by a spinlock, which could cause heavy lock contention when multiple tasks accessed the rb-tree concurrently. This contention limited the scalability of zswap, especially in scenarios with high concurrency.",\n        "After the optimization, the single rb-tree was split into multiple rb-trees, each corresponding to a segment of the swap address space (64MB). This split reduced lock contention by allowing multiple tasks to access different segments of the rb-tree concurrently. As a result, the performance of zswap improved, as evidenced by reduced real, user, and system times in kernel build tests."\n    ],\n    "comprehensive_understanding": "The optimization of zswap\'s rb-tree scalability addresses a critical performance bottleneck by reducing lock contention. By splitting the single rb-tree into multiple rb-trees, each corresponding to a segment of the swap address space, the kernel allows multiple tasks to access different parts of the rb-tree concurrently. This change not only improves the scalability of zswap but also enhances overall system performance by reducing the time spent waiting for locks. While this optimization does not eliminate lock contention entirely, it significantly mitigates it, leading to noticeable improvements in performance metrics such as real, user, and system times."\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733236970, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=796, prompt_tokens=2730, total_tokens=3526, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=1472, prompt_cache_miss_tokens=1258))
2024-12-03 22:43:32,576 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个命名实体抽取 (NER) 任务的专家，同时也是 linux 内核的专家。给你一个 linux 内核特性的描述 (feature_description)、对应的若干 commit 的标题和提交信息 (commits)、该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)、为了让开发者理解该特性所需的与这个特性直接相关的背景知识 (background_knowledge)、该特性被实现前后，linux 内核发生的变化 (kernel_changes)、对该特性的综合理解 (comprehensive_understanding)。你的任务是：根据这些信息提取出在 linux 内核中涉及到的和这个特性相关的概念实体 (concepts)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}],
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": [
                "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
            ]
        },
        "output": {
            "concepts": ["Copy-on-Write (CoW)", "mTHP", "folio", "memory reclamation", "memory availability", "anti-fragmentation"]
        }
    ],
    "input": {
        "feature_description": "zswap: optimize the scalability of zswap rb-tree",
        "commits": [{"commit_subject": "mm/zswap: split zswap rb-tree", "commit_message": "Each swapfile has one rb-tree to search the mapping of swp_entry_t to\nzswap_entry, that use a spinlock to protect, which can cause heavy lock\ncontention if multiple tasks zswap_store/load concurrently.\n\nOptimize the scalability problem by splitting the zswap rb-tree into\nmultiple rb-trees, each corresponds to SWAP_ADDRESS_SPACE_PAGES (64M),\njust like we did in the swap cache address_space splitting.\n\nAlthough this method can't solve the spinlock contention completely, it\ncan mitigate much of that contention.  Below is the results of kernel\nbuild in tmpfs with zswap shrinker enabled:\n\n     linux-next  zswap-lock-optimize\nreal 1m9.181s    1m3.820s\nuser 17m44.036s  17m40.100s\nsys  7m37.297s   4m54.622s\n\nSo there are clearly improvements.\n\nLink: https://lkml.kernel.org/r/20240117-b4-zswap-lock-optimize-v2-2-b5cc55479090@bytedance.com\nSigned-off-by: Chengming Zhou <zhouchengming@bytedance.com>\nAcked-by: Johannes Weiner <hannes@cmpxchg.org>\nAcked-by: Nhat Pham <nphamcs@gmail.com>\nAcked-by: Yosry Ahmed <yosryahmed@google.com>\nCc: Chris Li <chriscli@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/zswap: make sure each swapfile always have zswap rb-tree", "commit_message": "Patch series \"mm/zswap: optimize the scalability of zswap rb-tree\", v2.\n\nWhen testing the zswap performance by using kernel build -j32 in a tmpfs\ndirectory, I found the scalability of zswap rb-tree is not good, which is\nprotected by the only spinlock.  That would cause heavy lock contention if\nmultiple tasks zswap_store/load concurrently.\n\nSo a simple solution is to split the only one zswap rb-tree into multiple\nrb-trees, each corresponds to SWAP_ADDRESS_SPACE_PAGES (64M).  This idea\nis from the commit 4b3ef9daa4fc (\"mm/swap: split swap cache into 64MB\ntrunks\").\n\nAlthough this method can't solve the spinlock contention completely, it\ncan mitigate much of that contention.  Below is the results of kernel\nbuild in tmpfs with zswap shrinker enabled:\n\n     linux-next  zswap-lock-optimize\nreal 1m9.181s    1m3.820s\nuser 17m44.036s  17m40.100s\nsys  7m37.297s   4m54.622s\n\nSo there are clearly improvements.  And it's complementary with the\nongoing zswap xarray conversion by Chris.  Anyway, I think we can also\nmerge this first, it's complementary IMHO.  So I just refresh and resend\nthis for further discussion.\n\n\nThis patch (of 2):\n\nNot all zswap interfaces can handle the absence of the zswap rb-tree,\nactually only zswap_store() has handled it for now.\n\nTo make things simple, we make sure each swapfile always have the zswap\nrb-tree prepared before being enabled and used.  The preparation is\nunlikely to fail in practice, this patch just make it explicit.\n\nLink: https://lkml.kernel.org/r/20240117-b4-zswap-lock-optimize-v2-0-b5cc55479090@bytedance.com\nLink: https://lkml.kernel.org/r/20240117-b4-zswap-lock-optimize-v2-1-b5cc55479090@bytedance.com\nSigned-off-by: Chengming Zhou <zhouchengming@bytedance.com>\nAcked-by: Nhat Pham <nphamcs@gmail.com>\nAcked-by: Johannes Weiner <hannes@cmpxchg.org>\nAcked-by: Yosry Ahmed <yosryahmed@google.com>\nCc: Chris Li <chriscli@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}],
        "proper_nouns": [{"zswap": "zswap is a lightweight compressed cache in the Linux kernel that sits between memory and swap space. It is designed to reduce the number of times data is written to disk by compressing data in memory before it is swapped out. This can significantly improve performance by reducing disk I/O, especially in systems with limited swap space."}, {"rb-tree": "An rb-tree, or Red-Black Tree, is a type of self-balancing binary search tree. In the context of the Linux kernel, rb-trees are often used for efficient data storage and retrieval, especially in scenarios where data needs to be quickly accessed and modified. In zswap, an rb-tree is used to map swap entries (swp_entry_t) to zswap entries, facilitating quick lookups and updates."}, {"spinlock": "A spinlock is a synchronization primitive used in the Linux kernel to protect shared resources from concurrent access. It works by causing a thread to repeatedly check (or 'spin') until the lock becomes available. While spinlocks are efficient in scenarios with short lock hold times, they can lead to significant performance degradation if held for extended periods or if there is high contention for the lock."}, {"SWAP_ADDRESS_SPACE_PAGES": "SWAP_ADDRESS_SPACE_PAGES is a constant in the Linux kernel that defines the size of the address space used for swap operations. In the context of zswap, this constant is used to determine the size of each segment when splitting the rb-tree to improve scalability. The value is typically set to 64MB, which corresponds to the size of each segment in the split rb-tree."}],
        "background_knowledge": ["zswap operates by compressing data in memory before it is written to swap space, thereby reducing the amount of data that needs to be written to disk. This process involves mapping swap entries (swp_entry_t) to zswap entries, which are stored in an rb-tree for quick access.", "The original implementation of zswap used a single rb-tree protected by a spinlock, which could lead to significant lock contention when multiple tasks attempted to store or load data concurrently. This contention could degrade performance, especially in high-concurrency scenarios."],
        "kernel_changes": ["Before the optimization, zswap used a single rb-tree protected by a spinlock, which could cause heavy lock contention when multiple tasks accessed the rb-tree concurrently. This contention limited the scalability of zswap, especially in scenarios with high concurrency.", "After the optimization, the single rb-tree was split into multiple rb-trees, each corresponding to a segment of the swap address space (64MB). This split reduced lock contention by allowing multiple tasks to access different segments of the rb-tree concurrently. As a result, the performance of zswap improved, as evidenced by reduced real, user, and system times in kernel build tests."],
        "comprehensive_understanding": "The optimization of zswap's rb-tree scalability addresses a critical performance bottleneck by reducing lock contention. By splitting the single rb-tree into multiple rb-trees, each corresponding to a segment of the swap address space, the kernel allows multiple tasks to access different parts of the rb-tree concurrently. This change not only improves the scalability of zswap but also enhances overall system performance by reducing the time spent waiting for locks. While this optimization does not eliminate lock contention entirely, it significantly mitigates it, leading to noticeable improvements in performance metrics such as real, user, and system times."
    }
}

2024-12-03 22:43:36,207 - extractor - DEBUG - deepseek response:
ChatCompletion(id='a89e4d14-f000-4d7b-8030-96a9b748517e', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "concepts": ["zswap", "rb-tree", "spinlock", "SWAP_ADDRESS_SPACE_PAGES", "lock contention", "scalability", "performance improvement"]\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733237012, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=50, prompt_tokens=3529, total_tokens=3579, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=1536, prompt_cache_miss_tokens=1993))
2024-12-03 22:43:36,208 - extractor - DEBUG - feature_extracted of feature_id=33216:
{
    "feature_id": 33216,
    "feature_description": "zswap: optimize the scalability of zswap rb-tree",
    "version": "6.9",
    "commits": [
        {
            "commit_subject": "mm/zswap: split zswap rb-tree",
            "commit_message": "Each swapfile has one rb-tree to search the mapping of swp_entry_t to\nzswap_entry, that use a spinlock to protect, which can cause heavy lock\ncontention if multiple tasks zswap_store/load concurrently.\n\nOptimize the scalability problem by splitting the zswap rb-tree into\nmultiple rb-trees, each corresponds to SWAP_ADDRESS_SPACE_PAGES (64M),\njust like we did in the swap cache address_space splitting.\n\nAlthough this method can't solve the spinlock contention completely, it\ncan mitigate much of that contention.  Below is the results of kernel\nbuild in tmpfs with zswap shrinker enabled:\n\n     linux-next  zswap-lock-optimize\nreal 1m9.181s    1m3.820s\nuser 17m44.036s  17m40.100s\nsys  7m37.297s   4m54.622s\n\nSo there are clearly improvements.\n\nLink: https://lkml.kernel.org/r/20240117-b4-zswap-lock-optimize-v2-2-b5cc55479090@bytedance.com\nSigned-off-by: Chengming Zhou <zhouchengming@bytedance.com>\nAcked-by: Johannes Weiner <hannes@cmpxchg.org>\nAcked-by: Nhat Pham <nphamcs@gmail.com>\nAcked-by: Yosry Ahmed <yosryahmed@google.com>\nCc: Chris Li <chriscli@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm/zswap: make sure each swapfile always have zswap rb-tree",
            "commit_message": "Patch series \"mm/zswap: optimize the scalability of zswap rb-tree\", v2.\n\nWhen testing the zswap performance by using kernel build -j32 in a tmpfs\ndirectory, I found the scalability of zswap rb-tree is not good, which is\nprotected by the only spinlock.  That would cause heavy lock contention if\nmultiple tasks zswap_store/load concurrently.\n\nSo a simple solution is to split the only one zswap rb-tree into multiple\nrb-trees, each corresponds to SWAP_ADDRESS_SPACE_PAGES (64M).  This idea\nis from the commit 4b3ef9daa4fc (\"mm/swap: split swap cache into 64MB\ntrunks\").\n\nAlthough this method can't solve the spinlock contention completely, it\ncan mitigate much of that contention.  Below is the results of kernel\nbuild in tmpfs with zswap shrinker enabled:\n\n     linux-next  zswap-lock-optimize\nreal 1m9.181s    1m3.820s\nuser 17m44.036s  17m40.100s\nsys  7m37.297s   4m54.622s\n\nSo there are clearly improvements.  And it's complementary with the\nongoing zswap xarray conversion by Chris.  Anyway, I think we can also\nmerge this first, it's complementary IMHO.  So I just refresh and resend\nthis for further discussion.\n\n\nThis patch (of 2):\n\nNot all zswap interfaces can handle the absence of the zswap rb-tree,\nactually only zswap_store() has handled it for now.\n\nTo make things simple, we make sure each swapfile always have the zswap\nrb-tree prepared before being enabled and used.  The preparation is\nunlikely to fail in practice, this patch just make it explicit.\n\nLink: https://lkml.kernel.org/r/20240117-b4-zswap-lock-optimize-v2-0-b5cc55479090@bytedance.com\nLink: https://lkml.kernel.org/r/20240117-b4-zswap-lock-optimize-v2-1-b5cc55479090@bytedance.com\nSigned-off-by: Chengming Zhou <zhouchengming@bytedance.com>\nAcked-by: Nhat Pham <nphamcs@gmail.com>\nAcked-by: Johannes Weiner <hannes@cmpxchg.org>\nAcked-by: Yosry Ahmed <yosryahmed@google.com>\nCc: Chris Li <chriscli@google.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        }
    ],
    "proper_nouns": [
        {
            "zswap": "zswap is a lightweight compressed cache in the Linux kernel that sits between memory and swap space. It is designed to reduce the number of times data is written to disk by compressing data in memory before it is swapped out. This can significantly improve performance by reducing disk I/O, especially in systems with limited swap space."
        },
        {
            "rb-tree": "An rb-tree, or Red-Black Tree, is a type of self-balancing binary search tree. In the context of the Linux kernel, rb-trees are often used for efficient data storage and retrieval, especially in scenarios where data needs to be quickly accessed and modified. In zswap, an rb-tree is used to map swap entries (swp_entry_t) to zswap entries, facilitating quick lookups and updates."
        },
        {
            "spinlock": "A spinlock is a synchronization primitive used in the Linux kernel to protect shared resources from concurrent access. It works by causing a thread to repeatedly check (or 'spin') until the lock becomes available. While spinlocks are efficient in scenarios with short lock hold times, they can lead to significant performance degradation if held for extended periods or if there is high contention for the lock."
        },
        {
            "SWAP_ADDRESS_SPACE_PAGES": "SWAP_ADDRESS_SPACE_PAGES is a constant in the Linux kernel that defines the size of the address space used for swap operations. In the context of zswap, this constant is used to determine the size of each segment when splitting the rb-tree to improve scalability. The value is typically set to 64MB, which corresponds to the size of each segment in the split rb-tree."
        }
    ],
    "background_knowledge": [
        "zswap operates by compressing data in memory before it is written to swap space, thereby reducing the amount of data that needs to be written to disk. This process involves mapping swap entries (swp_entry_t) to zswap entries, which are stored in an rb-tree for quick access.",
        "The original implementation of zswap used a single rb-tree protected by a spinlock, which could lead to significant lock contention when multiple tasks attempted to store or load data concurrently. This contention could degrade performance, especially in high-concurrency scenarios."
    ],
    "kernel_changes": [
        "Before the optimization, zswap used a single rb-tree protected by a spinlock, which could cause heavy lock contention when multiple tasks accessed the rb-tree concurrently. This contention limited the scalability of zswap, especially in scenarios with high concurrency.",
        "After the optimization, the single rb-tree was split into multiple rb-trees, each corresponding to a segment of the swap address space (64MB). This split reduced lock contention by allowing multiple tasks to access different segments of the rb-tree concurrently. As a result, the performance of zswap improved, as evidenced by reduced real, user, and system times in kernel build tests."
    ],
    "comprehensive_understanding": "The optimization of zswap's rb-tree scalability addresses a critical performance bottleneck by reducing lock contention. By splitting the single rb-tree into multiple rb-trees, each corresponding to a segment of the swap address space, the kernel allows multiple tasks to access different parts of the rb-tree concurrently. This change not only improves the scalability of zswap but also enhances overall system performance by reducing the time spent waiting for locks. While this optimization does not eliminate lock contention entirely, it significantly mitigates it, leading to noticeable improvements in performance metrics such as real, user, and system times.",
    "concepts": [
        "zswap",
        "rb-tree",
        "spinlock",
        "SWAP_ADDRESS_SPACE_PAGES",
        "lock contention",
        "scalability",
        "performance improvement"
    ]
}
2024-12-03 22:43:36,208 - extractor - INFO - ---------Processing feature 25/815---------
2024-12-03 22:43:54,762 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个 linux 内核的专家。给你一个 linux 内核特性的描述以及其对应的若干 commit 的标题和提交信息，你的任务是为这个特性写一个多方面的扩写版描述。你的扩写需包含以下几个方面：1. 该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)；2. 为了让开发者理解该特性，需要哪些具体的，与这个特性直接相关的背景知识 (background_knowledge)；3. 该特性被实现前后，linux 内核发生了怎样的变化 (kernel_changes)；4. 你对该特性的综合理解 (comprehensive_understanding)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}]
        },
        "output": {
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
        }
    ],
    "input": {
        "feature_description": "Freelist loading optimization. When the per-cpu slab is depleted and a new one loaded from the cpu partial list optimize the loading to avoid an irq enable/disable cycle. This results in a 3.5% performance improvement on the "perf bench sched messaging" test",
        "commits": [{"commit_subject": "mm/slub: directly load freelist from cpu partial slab in the likely case", "commit_message": "The likely case is that we get a usable slab from the cpu partial list,\nwe can directly load freelist from it and return back, instead of going\nthe other way that need more work, like reenable interrupt and recheck.\n\nBut we need to remove the \"VM_BUG_ON(!new.frozen)\" in get_freelist()\nfor reusing it, since cpu partial slab is not frozen. It seems\nacceptable since it's only for debug purpose.\n\nAnd get_freelist() also assumes it can return NULL if the freelist is\nempty, which is not possible for the cpu partial slab case, so we\nadd \"VM_BUG_ON(!freelist)\" after get_freelist() to make it explicit.\n\nThere is some small performance improvement too, which shows by:\nperf bench sched messaging -g 5 -t -l 100000\n\n            mm-stable   slub-optimize\nTotal time      7.473    7.209\n\nSigned-off-by: Chengming Zhou <zhouchengming@bytedance.com>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\nSigned-off-by: Vlastimil Babka <vbabka@suse.cz>\n"}, {"commit_subject": "mm/slub: remove full list manipulation for non-debug slab", "commit_message": "Since debug slab is processed by free_to_partial_list(), and only debug\nslab which has SLAB_STORE_USER flag would care about the full list, we\ncan remove these unrelated full list manipulations from __slab_free().\n\nAcked-by: Christoph Lameter (Ampere) <cl@linux.com>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\nSigned-off-by: Chengming Zhou <zhouchengming@bytedance.com>\nSigned-off-by: Vlastimil Babka <vbabka@suse.cz>\n"}, {"commit_subject": "mm/slub: remove unused parameter in next_freelist_entry()", "commit_message": "The parameter \"struct slab *slab\" is unused in next_freelist_entry(),\nso just remove it.\n\nAcked-by: Christoph Lameter (Ampere) <cl@linux.com>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\nSigned-off-by: Chengming Zhou <zhouchengming@bytedance.com>\nSigned-off-by: Vlastimil Babka <vbabka@suse.cz>\n"}]
    }
}

2024-12-03 22:44:25,555 - extractor - DEBUG - deepseek response:
ChatCompletion(id='5e25fa2e-5369-4170-9fea-e3588895253a', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "proper_nouns": [\n        {\n            "per-cpu slab": "A per-cpu slab is a memory allocation mechanism in the Linux kernel\'s SLUB allocator that maintains a separate slab cache for each CPU. This allows for efficient memory allocation and deallocation by minimizing cache misses and contention between CPUs."\n        },\n        {\n            "cpu partial list": "The cpu partial list is a list of partially used slabs that are managed by each CPU. When a per-cpu slab is depleted, the SLUB allocator can load a new slab from this list to continue serving allocation requests without needing to fetch a new slab from the global pool."\n        },\n        {\n            "SLUB allocator": "The SLUB allocator is a memory management subsystem in the Linux kernel responsible for efficiently allocating and deallocating memory blocks (slabs) of various sizes. It is designed to be simpler and more scalable than its predecessors, the SLAB and SLOB allocators."\n        },\n        {\n            "irq enable/disable cycle": "An irq enable/disable cycle refers to the process of temporarily disabling interrupts (irq) to ensure atomicity during critical sections of code, and then re-enabling them afterward. This is necessary to prevent race conditions but can introduce overhead, especially if performed frequently."\n        }\n    ],\n    "background_knowledge": [\n        "The SLUB allocator maintains a per-cpu slab cache to minimize contention and cache misses. When a per-cpu slab is depleted, the allocator typically loads a new slab from the cpu partial list. However, this process often involves an irq enable/disable cycle, which can introduce performance overhead."\n    ],\n    "kernel_changes": [\n        "Before this optimization, when a per-cpu slab was depleted, the SLUB allocator would load a new slab from the cpu partial list and perform an irq enable/disable cycle. This cycle was necessary to ensure atomicity but introduced performance overhead. After the optimization, the allocator was modified to directly load the freelist from the cpu partial slab in the likely case where a usable slab is available. This eliminated the need for the irq enable/disable cycle, resulting in a 3.5% performance improvement on the \'perf bench sched messaging\' test."\n    ],\n    "comprehensive_understanding": "This feature optimizes the SLUB allocator\'s freelist loading process by directly loading the freelist from the cpu partial slab when a usable slab is available. This avoids the overhead of an irq enable/disable cycle, leading to a significant performance improvement in memory allocation operations. The optimization is particularly beneficial in scenarios with high memory allocation rates, such as the \'perf bench sched messaging\' test, where it results in a 3.5% performance boost."\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733237034, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=587, prompt_tokens=2244, total_tokens=2831, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=1472, prompt_cache_miss_tokens=772))
2024-12-03 22:44:25,555 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个命名实体抽取 (NER) 任务的专家，同时也是 linux 内核的专家。给你一个 linux 内核特性的描述 (feature_description)、对应的若干 commit 的标题和提交信息 (commits)、该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)、为了让开发者理解该特性所需的与这个特性直接相关的背景知识 (background_knowledge)、该特性被实现前后，linux 内核发生的变化 (kernel_changes)、对该特性的综合理解 (comprehensive_understanding)。你的任务是：根据这些信息提取出在 linux 内核中涉及到的和这个特性相关的概念实体 (concepts)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}],
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": [
                "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
            ]
        },
        "output": {
            "concepts": ["Copy-on-Write (CoW)", "mTHP", "folio", "memory reclamation", "memory availability", "anti-fragmentation"]
        }
    ],
    "input": {
        "feature_description": "Freelist loading optimization. When the per-cpu slab is depleted and a new one loaded from the cpu partial list optimize the loading to avoid an irq enable/disable cycle. This results in a 3.5% performance improvement on the "perf bench sched messaging" test",
        "commits": [{"commit_subject": "mm/slub: directly load freelist from cpu partial slab in the likely case", "commit_message": "The likely case is that we get a usable slab from the cpu partial list,\nwe can directly load freelist from it and return back, instead of going\nthe other way that need more work, like reenable interrupt and recheck.\n\nBut we need to remove the \"VM_BUG_ON(!new.frozen)\" in get_freelist()\nfor reusing it, since cpu partial slab is not frozen. It seems\nacceptable since it's only for debug purpose.\n\nAnd get_freelist() also assumes it can return NULL if the freelist is\nempty, which is not possible for the cpu partial slab case, so we\nadd \"VM_BUG_ON(!freelist)\" after get_freelist() to make it explicit.\n\nThere is some small performance improvement too, which shows by:\nperf bench sched messaging -g 5 -t -l 100000\n\n            mm-stable   slub-optimize\nTotal time      7.473    7.209\n\nSigned-off-by: Chengming Zhou <zhouchengming@bytedance.com>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\nSigned-off-by: Vlastimil Babka <vbabka@suse.cz>\n"}, {"commit_subject": "mm/slub: remove full list manipulation for non-debug slab", "commit_message": "Since debug slab is processed by free_to_partial_list(), and only debug\nslab which has SLAB_STORE_USER flag would care about the full list, we\ncan remove these unrelated full list manipulations from __slab_free().\n\nAcked-by: Christoph Lameter (Ampere) <cl@linux.com>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\nSigned-off-by: Chengming Zhou <zhouchengming@bytedance.com>\nSigned-off-by: Vlastimil Babka <vbabka@suse.cz>\n"}, {"commit_subject": "mm/slub: remove unused parameter in next_freelist_entry()", "commit_message": "The parameter \"struct slab *slab\" is unused in next_freelist_entry(),\nso just remove it.\n\nAcked-by: Christoph Lameter (Ampere) <cl@linux.com>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\nSigned-off-by: Chengming Zhou <zhouchengming@bytedance.com>\nSigned-off-by: Vlastimil Babka <vbabka@suse.cz>\n"}],
        "proper_nouns": [{"per-cpu slab": "A per-cpu slab is a memory allocation mechanism in the Linux kernel's SLUB allocator that maintains a separate slab cache for each CPU. This allows for efficient memory allocation and deallocation by minimizing cache misses and contention between CPUs."}, {"cpu partial list": "The cpu partial list is a list of partially used slabs that are managed by each CPU. When a per-cpu slab is depleted, the SLUB allocator can load a new slab from this list to continue serving allocation requests without needing to fetch a new slab from the global pool."}, {"SLUB allocator": "The SLUB allocator is a memory management subsystem in the Linux kernel responsible for efficiently allocating and deallocating memory blocks (slabs) of various sizes. It is designed to be simpler and more scalable than its predecessors, the SLAB and SLOB allocators."}, {"irq enable/disable cycle": "An irq enable/disable cycle refers to the process of temporarily disabling interrupts (irq) to ensure atomicity during critical sections of code, and then re-enabling them afterward. This is necessary to prevent race conditions but can introduce overhead, especially if performed frequently."}],
        "background_knowledge": ["The SLUB allocator maintains a per-cpu slab cache to minimize contention and cache misses. When a per-cpu slab is depleted, the allocator typically loads a new slab from the cpu partial list. However, this process often involves an irq enable/disable cycle, which can introduce performance overhead."],
        "kernel_changes": ["Before this optimization, when a per-cpu slab was depleted, the SLUB allocator would load a new slab from the cpu partial list and perform an irq enable/disable cycle. This cycle was necessary to ensure atomicity but introduced performance overhead. After the optimization, the allocator was modified to directly load the freelist from the cpu partial slab in the likely case where a usable slab is available. This eliminated the need for the irq enable/disable cycle, resulting in a 3.5% performance improvement on the 'perf bench sched messaging' test."],
        "comprehensive_understanding": "This feature optimizes the SLUB allocator's freelist loading process by directly loading the freelist from the cpu partial slab when a usable slab is available. This avoids the overhead of an irq enable/disable cycle, leading to a significant performance improvement in memory allocation operations. The optimization is particularly beneficial in scenarios with high memory allocation rates, such as the 'perf bench sched messaging' test, where it results in a 3.5% performance boost."
    }
}

2024-12-03 22:44:28,851 - extractor - DEBUG - deepseek response:
ChatCompletion(id='b77acf40-a439-467c-9f68-327827f6cf60', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "concepts": ["per-cpu slab", "cpu partial list", "SLUB allocator", "irq enable/disable cycle", "freelist loading", "performance improvement"]\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733237065, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=46, prompt_tokens=2838, total_tokens=2884, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=1536, prompt_cache_miss_tokens=1302))
2024-12-03 22:44:28,851 - extractor - DEBUG - feature_extracted of feature_id=33215:
{
    "feature_id": 33215,
    "feature_description": "Freelist loading optimization. When the per-cpu slab is depleted and a new one loaded from the cpu partial list optimize the loading to avoid an irq enable/disable cycle. This results in a 3.5% performance improvement on the \"perf bench sched messaging\" test",
    "version": "6.9",
    "commits": [
        {
            "commit_subject": "mm/slub: directly load freelist from cpu partial slab in the likely case",
            "commit_message": "The likely case is that we get a usable slab from the cpu partial list,\nwe can directly load freelist from it and return back, instead of going\nthe other way that need more work, like reenable interrupt and recheck.\n\nBut we need to remove the \"VM_BUG_ON(!new.frozen)\" in get_freelist()\nfor reusing it, since cpu partial slab is not frozen. It seems\nacceptable since it's only for debug purpose.\n\nAnd get_freelist() also assumes it can return NULL if the freelist is\nempty, which is not possible for the cpu partial slab case, so we\nadd \"VM_BUG_ON(!freelist)\" after get_freelist() to make it explicit.\n\nThere is some small performance improvement too, which shows by:\nperf bench sched messaging -g 5 -t -l 100000\n\n            mm-stable   slub-optimize\nTotal time      7.473    7.209\n\nSigned-off-by: Chengming Zhou <zhouchengming@bytedance.com>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\nSigned-off-by: Vlastimil Babka <vbabka@suse.cz>\n"
        },
        {
            "commit_subject": "mm/slub: remove full list manipulation for non-debug slab",
            "commit_message": "Since debug slab is processed by free_to_partial_list(), and only debug\nslab which has SLAB_STORE_USER flag would care about the full list, we\ncan remove these unrelated full list manipulations from __slab_free().\n\nAcked-by: Christoph Lameter (Ampere) <cl@linux.com>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\nSigned-off-by: Chengming Zhou <zhouchengming@bytedance.com>\nSigned-off-by: Vlastimil Babka <vbabka@suse.cz>\n"
        },
        {
            "commit_subject": "mm/slub: remove unused parameter in next_freelist_entry()",
            "commit_message": "The parameter \"struct slab *slab\" is unused in next_freelist_entry(),\nso just remove it.\n\nAcked-by: Christoph Lameter (Ampere) <cl@linux.com>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\nSigned-off-by: Chengming Zhou <zhouchengming@bytedance.com>\nSigned-off-by: Vlastimil Babka <vbabka@suse.cz>\n"
        }
    ],
    "proper_nouns": [
        {
            "per-cpu slab": "A per-cpu slab is a memory allocation mechanism in the Linux kernel's SLUB allocator that maintains a separate slab cache for each CPU. This allows for efficient memory allocation and deallocation by minimizing cache misses and contention between CPUs."
        },
        {
            "cpu partial list": "The cpu partial list is a list of partially used slabs that are managed by each CPU. When a per-cpu slab is depleted, the SLUB allocator can load a new slab from this list to continue serving allocation requests without needing to fetch a new slab from the global pool."
        },
        {
            "SLUB allocator": "The SLUB allocator is a memory management subsystem in the Linux kernel responsible for efficiently allocating and deallocating memory blocks (slabs) of various sizes. It is designed to be simpler and more scalable than its predecessors, the SLAB and SLOB allocators."
        },
        {
            "irq enable/disable cycle": "An irq enable/disable cycle refers to the process of temporarily disabling interrupts (irq) to ensure atomicity during critical sections of code, and then re-enabling them afterward. This is necessary to prevent race conditions but can introduce overhead, especially if performed frequently."
        }
    ],
    "background_knowledge": [
        "The SLUB allocator maintains a per-cpu slab cache to minimize contention and cache misses. When a per-cpu slab is depleted, the allocator typically loads a new slab from the cpu partial list. However, this process often involves an irq enable/disable cycle, which can introduce performance overhead."
    ],
    "kernel_changes": [
        "Before this optimization, when a per-cpu slab was depleted, the SLUB allocator would load a new slab from the cpu partial list and perform an irq enable/disable cycle. This cycle was necessary to ensure atomicity but introduced performance overhead. After the optimization, the allocator was modified to directly load the freelist from the cpu partial slab in the likely case where a usable slab is available. This eliminated the need for the irq enable/disable cycle, resulting in a 3.5% performance improvement on the 'perf bench sched messaging' test."
    ],
    "comprehensive_understanding": "This feature optimizes the SLUB allocator's freelist loading process by directly loading the freelist from the cpu partial slab when a usable slab is available. This avoids the overhead of an irq enable/disable cycle, leading to a significant performance improvement in memory allocation operations. The optimization is particularly beneficial in scenarios with high memory allocation rates, such as the 'perf bench sched messaging' test, where it results in a 3.5% performance boost.",
    "concepts": [
        "per-cpu slab",
        "cpu partial list",
        "SLUB allocator",
        "irq enable/disable cycle",
        "freelist loading",
        "performance improvement"
    ]
}
2024-12-03 22:44:28,851 - extractor - INFO - ---------Processing feature 26/815---------
2024-12-03 22:45:03,058 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个 linux 内核的专家。给你一个 linux 内核特性的描述以及其对应的若干 commit 的标题和提交信息，你的任务是为这个特性写一个多方面的扩写版描述。你的扩写需包含以下几个方面：1. 该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)；2. 为了让开发者理解该特性，需要哪些具体的，与这个特性直接相关的背景知识 (background_knowledge)；3. 该特性被实现前后，linux 内核发生了怎样的变化 (kernel_changes)；4. 你对该特性的综合理解 (comprehensive_understanding)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}]
        },
        "output": {
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
        }
    ],
    "input": {
        "feature_description": "convert mm counter to take a folio",
        "commits": [{"commit_subject": "mm: use pfn_swap_entry_to_folio() in zap_huge_pmd()", "commit_message": "Call pfn_swap_entry_to_folio() in zap_huge_pmd() as preparation for\nconverting mm counter functions to take a folio.  Saves a call to\ncompound_head() embedded inside PageAnon().\n\nLink: https://lkml.kernel.org/r/20240111152429.3374566-7-willy@infradead.org\nSigned-off-by: Kefeng Wang <wangkefeng.wang@huawei.com>\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: use pfn_swap_entry_folio() in copy_nonpresent_pte()", "commit_message": "Call pfn_swap_entry_folio() as preparation for converting mm counter\nfunctions to take a folio.\n\nLink: https://lkml.kernel.org/r/20240111152429.3374566-8-willy@infradead.org\nSigned-off-by: Kefeng Wang <wangkefeng.wang@huawei.com>\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "proc: use pfn_swap_entry_folio where obvious", "commit_message": "These callers only pass the result to PageAnon(), so we can save the extra\ncall to compound_head() by using pfn_swap_entry_folio().\n\nLink: https://lkml.kernel.org/r/20240111152429.3374566-3-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: convert mm_counter_file() to take a folio", "commit_message": "Now all callers of mm_counter_file() have a folio, convert\nmm_counter_file() to take a folio.  Saves a call to compound_head() hidden\ninside PageSwapBacked().\n\nLink: https://lkml.kernel.org/r/20240111152429.3374566-11-willy@infradead.org\nSigned-off-by: Kefeng Wang <wangkefeng.wang@huawei.com>\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: add pfn_swap_entry_folio()", "commit_message": "Patch series \"mm: convert mm counter to take a folio\", v3.\n\nMake sure all mm_counter() and mm_counter_file() callers have a folio,\nthen convert mm counter functions to take a folio, which saves some\ncompound_head() calls.\n\n\nThis patch (of 10):  \n\nThanks to the compound_head() hidden inside PageLocked(), this saves a\ncall to compound_head() over calling page_folio(pfn_swap_entry_to_page())\n\nLink: https://lkml.kernel.org/r/20240111152429.3374566-1-willy@infradead.org\nLink: https://lkml.kernel.org/r/20240111152429.3374566-2-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: convert to should_zap_page() to should_zap_folio()", "commit_message": "Make should_zap_page() take a folio and rename it to should_zap_folio() as\npreparation for converting mm counter functions to take a folio.  Saves a\ncall to compound_head() hidden inside PageAnon().\n\n[wangkefeng.wang@huawei.com: fix used-uninitialized warning]\n  Link: https://lkml.kernel.org/r/962a7993-fce9-4de8-85cd-25e290f25736@huawei.com\nLink: https://lkml.kernel.org/r/20240111152429.3374566-9-willy@infradead.org\nSigned-off-by: Kefeng Wang <wangkefeng.wang@huawei.com>\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: use pfn_swap_entry_folio() in __split_huge_pmd_locked()", "commit_message": "Call pfn_swap_entry_folio() in __split_huge_pmd_locked() as preparation\nfor converting mm counter functions to take a folio.\n\nLink: https://lkml.kernel.org/r/20240111152429.3374566-6-willy@infradead.org\nSigned-off-by: Kefeng Wang <wangkefeng.wang@huawei.com>\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "s390: use pfn_swap_entry_folio() in ptep_zap_swap_entry()", "commit_message": "Call pfn_swap_entry_folio() in ptep_zap_swap_entry() as preparation for\nconverting mm counter functions to take a folio.\n\nLink: https://lkml.kernel.org/r/20240111152429.3374566-5-willy@infradead.org\nSigned-off-by: Kefeng Wang <wangkefeng.wang@huawei.com>\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: convert mm_counter() to take a folio", "commit_message": "Now all callers of mm_counter() have a folio, convert mm_counter() to take\na folio.  Saves a call to compound_head() hidden inside PageAnon().\n\nLink: https://lkml.kernel.org/r/20240111152429.3374566-10-willy@infradead.org\nSigned-off-by: Kefeng Wang <wangkefeng.wang@huawei.com>\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}]
    }
}

2024-12-03 22:45:35,849 - extractor - DEBUG - deepseek response:
ChatCompletion(id='d71c7355-94be-49f3-b3dd-1555969479da', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "proper_nouns": [\n        {\n            "folio": "In the Linux kernel, a \'folio\' is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages."\n        },\n        {\n            "mm_counter": "The \'mm_counter\' is a mechanism in the Linux kernel used to count the number of memory pages associated with a particular memory management context, such as a process or a file. It helps in tracking and managing memory usage more accurately."\n        },\n        {\n            "pfn_swap_entry_to_folio": "The function \'pfn_swap_entry_to_folio\' is a utility in the Linux kernel that converts a swap entry (which typically points to a memory page) into a folio. This function is part of the effort to transition from traditional page-based memory management to folio-based management, which is more efficient for certain operations."\n        }\n    ],\n    "background_knowledge": [\n        "The Linux kernel traditionally manages memory using pages, which are typically 4KB in size. However, as memory demands and file sizes have grown, the kernel has introduced higher-level abstractions like folios to manage memory more efficiently. Folios allow for better handling of large files and operations that span multiple pages.",\n        "The \'mm_counter\' functions are used to count and manage memory pages associated with a particular context. These functions are crucial for tracking memory usage and ensuring that memory is managed efficiently. The transition to using folios in these functions aims to improve the accuracy and efficiency of memory management."\n    ],\n    "kernel_changes": [\n        "Before this feature was implemented, the \'mm_counter\' functions operated on traditional memory pages, which could lead to inefficiencies, especially when dealing with large files or operations spanning multiple pages. The conversion of these functions to take a folio as an argument represents a significant shift in how memory is managed within the kernel.",\n        "With the introduction of folio-based management, the kernel can now handle memory more efficiently, reducing the overhead associated with traditional page management. This change also simplifies the code by eliminating the need for multiple calls to \'compound_head()\', which was previously required to convert pages to folios."\n    ],\n    "comprehensive_understanding": "The feature \'convert mm counter to take a folio\' represents a significant improvement in the Linux kernel\'s memory management capabilities. By transitioning from traditional page-based management to folio-based management, the kernel can handle large files and operations spanning multiple pages more efficiently. This change not only improves memory management accuracy but also reduces the overhead associated with converting pages to folios, leading to better overall system performance and resource utilization."\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733237103, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=618, prompt_tokens=3599, total_tokens=4217, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=1472, prompt_cache_miss_tokens=2127))
2024-12-03 22:45:35,849 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个命名实体抽取 (NER) 任务的专家，同时也是 linux 内核的专家。给你一个 linux 内核特性的描述 (feature_description)、对应的若干 commit 的标题和提交信息 (commits)、该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)、为了让开发者理解该特性所需的与这个特性直接相关的背景知识 (background_knowledge)、该特性被实现前后，linux 内核发生的变化 (kernel_changes)、对该特性的综合理解 (comprehensive_understanding)。你的任务是：根据这些信息提取出在 linux 内核中涉及到的和这个特性相关的概念实体 (concepts)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}],
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": [
                "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
            ]
        },
        "output": {
            "concepts": ["Copy-on-Write (CoW)", "mTHP", "folio", "memory reclamation", "memory availability", "anti-fragmentation"]
        }
    ],
    "input": {
        "feature_description": "convert mm counter to take a folio",
        "commits": [{"commit_subject": "mm: use pfn_swap_entry_to_folio() in zap_huge_pmd()", "commit_message": "Call pfn_swap_entry_to_folio() in zap_huge_pmd() as preparation for\nconverting mm counter functions to take a folio.  Saves a call to\ncompound_head() embedded inside PageAnon().\n\nLink: https://lkml.kernel.org/r/20240111152429.3374566-7-willy@infradead.org\nSigned-off-by: Kefeng Wang <wangkefeng.wang@huawei.com>\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: use pfn_swap_entry_folio() in copy_nonpresent_pte()", "commit_message": "Call pfn_swap_entry_folio() as preparation for converting mm counter\nfunctions to take a folio.\n\nLink: https://lkml.kernel.org/r/20240111152429.3374566-8-willy@infradead.org\nSigned-off-by: Kefeng Wang <wangkefeng.wang@huawei.com>\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "proc: use pfn_swap_entry_folio where obvious", "commit_message": "These callers only pass the result to PageAnon(), so we can save the extra\ncall to compound_head() by using pfn_swap_entry_folio().\n\nLink: https://lkml.kernel.org/r/20240111152429.3374566-3-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: convert mm_counter_file() to take a folio", "commit_message": "Now all callers of mm_counter_file() have a folio, convert\nmm_counter_file() to take a folio.  Saves a call to compound_head() hidden\ninside PageSwapBacked().\n\nLink: https://lkml.kernel.org/r/20240111152429.3374566-11-willy@infradead.org\nSigned-off-by: Kefeng Wang <wangkefeng.wang@huawei.com>\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: add pfn_swap_entry_folio()", "commit_message": "Patch series \"mm: convert mm counter to take a folio\", v3.\n\nMake sure all mm_counter() and mm_counter_file() callers have a folio,\nthen convert mm counter functions to take a folio, which saves some\ncompound_head() calls.\n\n\nThis patch (of 10):  \n\nThanks to the compound_head() hidden inside PageLocked(), this saves a\ncall to compound_head() over calling page_folio(pfn_swap_entry_to_page())\n\nLink: https://lkml.kernel.org/r/20240111152429.3374566-1-willy@infradead.org\nLink: https://lkml.kernel.org/r/20240111152429.3374566-2-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: convert to should_zap_page() to should_zap_folio()", "commit_message": "Make should_zap_page() take a folio and rename it to should_zap_folio() as\npreparation for converting mm counter functions to take a folio.  Saves a\ncall to compound_head() hidden inside PageAnon().\n\n[wangkefeng.wang@huawei.com: fix used-uninitialized warning]\n  Link: https://lkml.kernel.org/r/962a7993-fce9-4de8-85cd-25e290f25736@huawei.com\nLink: https://lkml.kernel.org/r/20240111152429.3374566-9-willy@infradead.org\nSigned-off-by: Kefeng Wang <wangkefeng.wang@huawei.com>\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: use pfn_swap_entry_folio() in __split_huge_pmd_locked()", "commit_message": "Call pfn_swap_entry_folio() in __split_huge_pmd_locked() as preparation\nfor converting mm counter functions to take a folio.\n\nLink: https://lkml.kernel.org/r/20240111152429.3374566-6-willy@infradead.org\nSigned-off-by: Kefeng Wang <wangkefeng.wang@huawei.com>\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "s390: use pfn_swap_entry_folio() in ptep_zap_swap_entry()", "commit_message": "Call pfn_swap_entry_folio() in ptep_zap_swap_entry() as preparation for\nconverting mm counter functions to take a folio.\n\nLink: https://lkml.kernel.org/r/20240111152429.3374566-5-willy@infradead.org\nSigned-off-by: Kefeng Wang <wangkefeng.wang@huawei.com>\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: convert mm_counter() to take a folio", "commit_message": "Now all callers of mm_counter() have a folio, convert mm_counter() to take\na folio.  Saves a call to compound_head() hidden inside PageAnon().\n\nLink: https://lkml.kernel.org/r/20240111152429.3374566-10-willy@infradead.org\nSigned-off-by: Kefeng Wang <wangkefeng.wang@huawei.com>\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}],
        "proper_nouns": [{"folio": "In the Linux kernel, a 'folio' is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages."}, {"mm_counter": "The 'mm_counter' is a mechanism in the Linux kernel used to count the number of memory pages associated with a particular memory management context, such as a process or a file. It helps in tracking and managing memory usage more accurately."}, {"pfn_swap_entry_to_folio": "The function 'pfn_swap_entry_to_folio' is a utility in the Linux kernel that converts a swap entry (which typically points to a memory page) into a folio. This function is part of the effort to transition from traditional page-based memory management to folio-based management, which is more efficient for certain operations."}],
        "background_knowledge": ["The Linux kernel traditionally manages memory using pages, which are typically 4KB in size. However, as memory demands and file sizes have grown, the kernel has introduced higher-level abstractions like folios to manage memory more efficiently. Folios allow for better handling of large files and operations that span multiple pages.", "The 'mm_counter' functions are used to count and manage memory pages associated with a particular context. These functions are crucial for tracking memory usage and ensuring that memory is managed efficiently. The transition to using folios in these functions aims to improve the accuracy and efficiency of memory management."],
        "kernel_changes": ["Before this feature was implemented, the 'mm_counter' functions operated on traditional memory pages, which could lead to inefficiencies, especially when dealing with large files or operations spanning multiple pages. The conversion of these functions to take a folio as an argument represents a significant shift in how memory is managed within the kernel.", "With the introduction of folio-based management, the kernel can now handle memory more efficiently, reducing the overhead associated with traditional page management. This change also simplifies the code by eliminating the need for multiple calls to 'compound_head()', which was previously required to convert pages to folios."],
        "comprehensive_understanding": "The feature 'convert mm counter to take a folio' represents a significant improvement in the Linux kernel's memory management capabilities. By transitioning from traditional page-based management to folio-based management, the kernel can handle large files and operations spanning multiple pages more efficiently. This change not only improves memory management accuracy but also reduces the overhead associated with converting pages to folios, leading to better overall system performance and resource utilization."
    }
}

2024-12-03 22:45:39,317 - extractor - DEBUG - deepseek response:
ChatCompletion(id='bd79f8c2-1b6a-4ef0-931e-a701bda46a59', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "concepts": ["folio", "mm_counter", "pfn_swap_entry_to_folio", "memory management", "memory pages", "compound_head()", "memory efficiency", "system performance", "resource utilization"]\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733237135, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=60, prompt_tokens=4227, total_tokens=4287, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=1536, prompt_cache_miss_tokens=2691))
2024-12-03 22:45:39,317 - extractor - DEBUG - feature_extracted of feature_id=33214:
{
    "feature_id": 33214,
    "feature_description": "convert mm counter to take a folio",
    "version": "6.9",
    "commits": [
        {
            "commit_subject": "mm: use pfn_swap_entry_to_folio() in zap_huge_pmd()",
            "commit_message": "Call pfn_swap_entry_to_folio() in zap_huge_pmd() as preparation for\nconverting mm counter functions to take a folio.  Saves a call to\ncompound_head() embedded inside PageAnon().\n\nLink: https://lkml.kernel.org/r/20240111152429.3374566-7-willy@infradead.org\nSigned-off-by: Kefeng Wang <wangkefeng.wang@huawei.com>\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm: use pfn_swap_entry_folio() in copy_nonpresent_pte()",
            "commit_message": "Call pfn_swap_entry_folio() as preparation for converting mm counter\nfunctions to take a folio.\n\nLink: https://lkml.kernel.org/r/20240111152429.3374566-8-willy@infradead.org\nSigned-off-by: Kefeng Wang <wangkefeng.wang@huawei.com>\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "proc: use pfn_swap_entry_folio where obvious",
            "commit_message": "These callers only pass the result to PageAnon(), so we can save the extra\ncall to compound_head() by using pfn_swap_entry_folio().\n\nLink: https://lkml.kernel.org/r/20240111152429.3374566-3-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm: convert mm_counter_file() to take a folio",
            "commit_message": "Now all callers of mm_counter_file() have a folio, convert\nmm_counter_file() to take a folio.  Saves a call to compound_head() hidden\ninside PageSwapBacked().\n\nLink: https://lkml.kernel.org/r/20240111152429.3374566-11-willy@infradead.org\nSigned-off-by: Kefeng Wang <wangkefeng.wang@huawei.com>\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm: add pfn_swap_entry_folio()",
            "commit_message": "Patch series \"mm: convert mm counter to take a folio\", v3.\n\nMake sure all mm_counter() and mm_counter_file() callers have a folio,\nthen convert mm counter functions to take a folio, which saves some\ncompound_head() calls.\n\n\nThis patch (of 10):  \n\nThanks to the compound_head() hidden inside PageLocked(), this saves a\ncall to compound_head() over calling page_folio(pfn_swap_entry_to_page())\n\nLink: https://lkml.kernel.org/r/20240111152429.3374566-1-willy@infradead.org\nLink: https://lkml.kernel.org/r/20240111152429.3374566-2-willy@infradead.org\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm: convert to should_zap_page() to should_zap_folio()",
            "commit_message": "Make should_zap_page() take a folio and rename it to should_zap_folio() as\npreparation for converting mm counter functions to take a folio.  Saves a\ncall to compound_head() hidden inside PageAnon().\n\n[wangkefeng.wang@huawei.com: fix used-uninitialized warning]\n  Link: https://lkml.kernel.org/r/962a7993-fce9-4de8-85cd-25e290f25736@huawei.com\nLink: https://lkml.kernel.org/r/20240111152429.3374566-9-willy@infradead.org\nSigned-off-by: Kefeng Wang <wangkefeng.wang@huawei.com>\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm: use pfn_swap_entry_folio() in __split_huge_pmd_locked()",
            "commit_message": "Call pfn_swap_entry_folio() in __split_huge_pmd_locked() as preparation\nfor converting mm counter functions to take a folio.\n\nLink: https://lkml.kernel.org/r/20240111152429.3374566-6-willy@infradead.org\nSigned-off-by: Kefeng Wang <wangkefeng.wang@huawei.com>\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "s390: use pfn_swap_entry_folio() in ptep_zap_swap_entry()",
            "commit_message": "Call pfn_swap_entry_folio() in ptep_zap_swap_entry() as preparation for\nconverting mm counter functions to take a folio.\n\nLink: https://lkml.kernel.org/r/20240111152429.3374566-5-willy@infradead.org\nSigned-off-by: Kefeng Wang <wangkefeng.wang@huawei.com>\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm: convert mm_counter() to take a folio",
            "commit_message": "Now all callers of mm_counter() have a folio, convert mm_counter() to take\na folio.  Saves a call to compound_head() hidden inside PageAnon().\n\nLink: https://lkml.kernel.org/r/20240111152429.3374566-10-willy@infradead.org\nSigned-off-by: Kefeng Wang <wangkefeng.wang@huawei.com>\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: David Hildenbrand <david@redhat.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        }
    ],
    "proper_nouns": [
        {
            "folio": "In the Linux kernel, a 'folio' is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages."
        },
        {
            "mm_counter": "The 'mm_counter' is a mechanism in the Linux kernel used to count the number of memory pages associated with a particular memory management context, such as a process or a file. It helps in tracking and managing memory usage more accurately."
        },
        {
            "pfn_swap_entry_to_folio": "The function 'pfn_swap_entry_to_folio' is a utility in the Linux kernel that converts a swap entry (which typically points to a memory page) into a folio. This function is part of the effort to transition from traditional page-based memory management to folio-based management, which is more efficient for certain operations."
        }
    ],
    "background_knowledge": [
        "The Linux kernel traditionally manages memory using pages, which are typically 4KB in size. However, as memory demands and file sizes have grown, the kernel has introduced higher-level abstractions like folios to manage memory more efficiently. Folios allow for better handling of large files and operations that span multiple pages.",
        "The 'mm_counter' functions are used to count and manage memory pages associated with a particular context. These functions are crucial for tracking memory usage and ensuring that memory is managed efficiently. The transition to using folios in these functions aims to improve the accuracy and efficiency of memory management."
    ],
    "kernel_changes": [
        "Before this feature was implemented, the 'mm_counter' functions operated on traditional memory pages, which could lead to inefficiencies, especially when dealing with large files or operations spanning multiple pages. The conversion of these functions to take a folio as an argument represents a significant shift in how memory is managed within the kernel.",
        "With the introduction of folio-based management, the kernel can now handle memory more efficiently, reducing the overhead associated with traditional page management. This change also simplifies the code by eliminating the need for multiple calls to 'compound_head()', which was previously required to convert pages to folios."
    ],
    "comprehensive_understanding": "The feature 'convert mm counter to take a folio' represents a significant improvement in the Linux kernel's memory management capabilities. By transitioning from traditional page-based management to folio-based management, the kernel can handle large files and operations spanning multiple pages more efficiently. This change not only improves memory management accuracy but also reduces the overhead associated with converting pages to folios, leading to better overall system performance and resource utilization.",
    "concepts": [
        "folio",
        "mm_counter",
        "pfn_swap_entry_to_folio",
        "memory management",
        "memory pages",
        "compound_head()",
        "memory efficiency",
        "system performance",
        "resource utilization"
    ]
}
2024-12-03 22:45:39,318 - extractor - INFO - ---------Processing feature 27/815---------
2024-12-03 22:45:41,678 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个 linux 内核的专家。给你一个 linux 内核特性的描述以及其对应的若干 commit 的标题和提交信息，你的任务是为这个特性写一个多方面的扩写版描述。你的扩写需包含以下几个方面：1. 该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)；2. 为了让开发者理解该特性，需要哪些具体的，与这个特性直接相关的背景知识 (background_knowledge)；3. 该特性被实现前后，linux 内核发生了怎样的变化 (kernel_changes)；4. 你对该特性的综合理解 (comprehensive_understanding)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}]
        },
        "output": {
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
        }
    ],
    "input": {
        "feature_description": "tools/mm: add thpmaps script to dump THP usage info",
        "commits": [{"commit_subject": "tools/mm: add thpmaps script to dump THP usage info", "commit_message": "With the proliferation of large folios for file-backed memory, and more\nrecently the introduction of multi-size THP for anonymous memory, it is\nbecoming useful to be able to see exactly how large folios are mapped into\nprocesses.  For some architectures (e.g.  arm64), if most memory is mapped\nusing contpte-sized and -aligned blocks, TLB usage can be optimized so\nit's useful to see where these requirements are and are not being met.\n\nthpmaps is a Python utility that reads /proc/<pid>/smaps,\n/proc/<pid>/pagemap and /proc/kpageflags to print information about how\ntransparent huge pages (both file and anon) are mapped to a specified\nprocess or cgroup.  It aims to help users debug and optimize their\nworkloads.  In future we may wish to introduce stats directly into the\nkernel (e.g.  smaps or similar), but for now this provides a short term\nsolution without the need to introduce any new ABI.\n\nRun with help option for a full listing of the arguments:\n\n    # ./thpmaps --help\n\n--8<--\nusage: thpmaps [-h] [--pid pid | --cgroup path] [--rollup]\n               [--cont size[KMG]] [--inc-smaps] [--inc-empty]\n               [--periodic sleep_ms]\n\nPrints information about how transparent huge pages are mapped, either\nsystem-wide, or for a specified process or cgroup.\n\nWhen run with --pid, the user explicitly specifies the set of pids to\nscan.  e.g.  \"--pid 10 [--pid 134 ...]\".  When run with --cgroup, the user\npasses either a v1 or v2 cgroup and all pids that belong to the cgroup\nsubtree are scanned.  When run with neither --pid nor --cgroup, the full\nset of pids on the system is gathered from /proc and scanned as if the\nuser had provided \"--pid 1 --pid 2 ...\".\n\nA default set of statistics is always generated for THP mappings. \nHowever, it is also possible to generate additional statistics for\n\"contiguous block mappings\" where the block size is user-defined.\n\nStatistics are maintained independently for anonymous and file-backed\n(pagecache) memory and are shown both in kB and as a percentage of either\ntotal anonymous or total file-backed memory as appropriate.\n\nTHP Statistics\n--------------\n\nStatistics are always generated for fully- and contiguously-mapped THPs\nwhose mapping address is aligned to their size, for each <size> supported\nby the system.  Separate counters describe THPs mapped by PTE vs those\nmapped by PMD.  (Although note a THP can only be mapped by PMD if it is\nPMD-sized):\n\n- anon-thp-pte-aligned-<size>kB\n- file-thp-pte-aligned-<size>kB\n- anon-thp-pmd-aligned-<size>kB\n- file-thp-pmd-aligned-<size>kB\n\nSimilarly, statistics are always generated for fully- and contiguously-\nmapped THPs whose mapping address is *not* aligned to their size, for each\n<size> supported by the system.  Due to the unaligned mapping, it is\nimpossible to map by PMD, so there are only PTE counters for this case:\n\n- anon-thp-pte-unaligned-<size>kB\n- file-thp-pte-unaligned-<size>kB\n\nStatistics are also always generated for mapped pages that belong to a THP\nbut where the is THP is *not* fully- and contiguously- mapped.  These\n\"partial\" mappings are all counted in the same counter regardless of the\nsize of the THP that is partially mapped:\n\n- anon-thp-pte-partial\n- file-thp-pte-partial\n\nContiguous Block Statistics\n---------------------------\n\nAn optional, additional set of statistics is generated for every\ncontiguous block size specified with `--cont <size>`.  These statistics\nshow how much memory is mapped in contiguous blocks of <size> and also\naligned to <size>.  A given contiguous block must all belong to the same\nTHP, but there is no requirement for it to be the *whole* THP.  Separate\ncounters describe contiguous blocks mapped by PTE vs those mapped by PMD:\n\n- anon-cont-pte-aligned-<size>kB\n- file-cont-pte-aligned-<size>kB\n- anon-cont-pmd-aligned-<size>kB\n- file-cont-pmd-aligned-<size>kB\n\nAs an example, if monitoring 64K contiguous blocks (--cont 64K), there are\na number of sources that could provide such blocks: a fully- and\ncontiguously-mapped 64K THP that is aligned to a 64K boundary would\nprovide 1 block.  A fully- and contiguously-mapped 128K THP that is\naligned to at least a 64K boundary would provide 2 blocks.  Or a 128K THP\nthat maps its first 100K, but contiguously and starting at a 64K boundary\nwould provide 1 block.  A fully- and contiguously-mapped 2M THP would\nprovide 32 blocks.  There are many other possible permutations.\n\noptions:\n  -h, --help           show this help message and exit\n  --pid pid            Process id of the target process. Maybe issued\n                       multiple times to scan multiple processes. --pid\n                       and --cgroup are mutually exclusive. If neither\n                       are provided, all processes are scanned to\n                       provide system-wide information.\n  --cgroup path        Path to the target cgroup in sysfs. Iterates\n                       over every pid in the cgroup and its children.\n                       --pid and --cgroup are mutually exclusive. If\n                       neither are provided, all processes are scanned\n                       to provide system-wide information.\n  --rollup             Sum the per-vma statistics to provide a summary\n                       over the whole system, process or cgroup.\n  --cont size[KMG]     Adds stats for memory that is mapped in\n                       contiguous blocks of <size> and also aligned to\n                       <size>. May be issued multiple times to track\n                       multiple sized blocks. Useful to infer e.g.\n                       arm64 contpte and hpa mappings. Size must be a\n                       power-of-2 number of pages.\n  --inc-smaps          Include all numerical, additive\n                       /proc/<pid>/smaps stats in the output.\n  --inc-empty          Show all statistics including those whose value\n                       is 0.\n  --periodic sleep_ms  Run in a loop, polling every sleep_ms\n                       milliseconds.\n\nRequires root privilege to access pagemap and kpageflags.\n--8<--\n\nExample command to summarise fully and partially mapped THPs and 64K\ncontiguous blocks over all VMAs in all processes in the system\n(--inc-empty forces printing stats that are 0):\n\n    # ./thpmaps --cont 64K --rollup --inc-empty\n\n--8<--\nanon-thp-pmd-aligned-2048kB:      139264 kB ( 6%)\nfile-thp-pmd-aligned-2048kB:           0 kB ( 0%)\nanon-thp-pte-aligned-16kB:             0 kB ( 0%)\nanon-thp-pte-aligned-32kB:             0 kB ( 0%)\nanon-thp-pte-aligned-64kB:         72256 kB ( 3%)\nanon-thp-pte-aligned-128kB:            0 kB ( 0%)\nanon-thp-pte-aligned-256kB:            0 kB ( 0%)\nanon-thp-pte-aligned-512kB:            0 kB ( 0%)\nanon-thp-pte-aligned-1024kB:           0 kB ( 0%)\nanon-thp-pte-aligned-2048kB:           0 kB ( 0%)\nanon-thp-pte-unaligned-16kB:           0 kB ( 0%)\nanon-thp-pte-unaligned-32kB:           0 kB ( 0%)\nanon-thp-pte-unaligned-64kB:           0 kB ( 0%)\nanon-thp-pte-unaligned-128kB:          0 kB ( 0%)\nanon-thp-pte-unaligned-256kB:          0 kB ( 0%)\nanon-thp-pte-unaligned-512kB:          0 kB ( 0%)\nanon-thp-pte-unaligned-1024kB:         0 kB ( 0%)\nanon-thp-pte-unaligned-2048kB:         0 kB ( 0%)\nanon-thp-pte-partial:              63232 kB ( 3%)\nfile-thp-pte-aligned-16kB:        809024 kB (47%)\nfile-thp-pte-aligned-32kB:         43168 kB ( 3%)\nfile-thp-pte-aligned-64kB:         98496 kB ( 6%)\nfile-thp-pte-aligned-128kB:        17536 kB ( 1%)\nfile-thp-pte-aligned-256kB:            0 kB ( 0%)\nfile-thp-pte-aligned-512kB:            0 kB ( 0%)\nfile-thp-pte-aligned-1024kB:           0 kB ( 0%)\nfile-thp-pte-aligned-2048kB:           0 kB ( 0%)\nfile-thp-pte-unaligned-16kB:       21712 kB ( 1%)\nfile-thp-pte-unaligned-32kB:         704 kB ( 0%)\nfile-thp-pte-unaligned-64kB:         896 kB ( 0%)\nfile-thp-pte-unaligned-128kB:      44928 kB ( 3%)\nfile-thp-pte-unaligned-256kB:          0 kB ( 0%)\nfile-thp-pte-unaligned-512kB:          0 kB ( 0%)\nfile-thp-pte-unaligned-1024kB:         0 kB ( 0%)\nfile-thp-pte-unaligned-2048kB:         0 kB ( 0%)\nfile-thp-pte-partial:               9252 kB ( 1%)\nanon-cont-pmd-aligned-64kB:       139264 kB ( 6%)\nfile-cont-pmd-aligned-64kB:            0 kB ( 0%)\nanon-cont-pte-aligned-64kB:       100672 kB ( 4%)\nfile-cont-pte-aligned-64kB:       161856 kB ( 9%)\n--8<--\n\nLink: https://lkml.kernel.org/r/20240116141235.960842-1-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nTested-by: Barry Song <v-songbaohua@oppo.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: John Hubbard <jhubbard@nvidia.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: William Kucharski <william.kucharski@oracle.com>\nCc: Zenghui Yu <yuzenghui@huawei.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}]
    }
}

2024-12-03 22:46:22,632 - extractor - DEBUG - deepseek response:
ChatCompletion(id='f40cfa7b-a8b9-4703-8812-0d5b9c63c327', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "proper_nouns": [\n        {\n            "Transparent Huge Pages (THP)": "Transparent Huge Pages (THP) is a feature in the Linux kernel that aims to improve memory performance by using larger memory pages, typically 2MB or 1GB, instead of the standard 4KB pages. THP automatically manages the use of huge pages, making it easier for applications to benefit from them without requiring explicit configuration."\n        },\n        {\n            "Page Table Entries (PTE)": "Page Table Entries (PTE) are the smallest units of memory management in the Linux kernel. Each PTE corresponds to a 4KB memory page and contains information about the physical address, access permissions, and other metadata."\n        },\n        {\n            "Page Middle Directory (PMD)": "The Page Middle Directory (PMD) is a level in the multi-level page table hierarchy used by the Linux kernel. PMD entries point to large pages, such as 2MB or 1GB, and are used to reduce the overhead of managing large memory regions."\n        },\n        {\n            "cgroup": "cgroup (control group) is a Linux kernel feature that allows for the hierarchical grouping of processes and the allocation of resources (such as CPU, memory, and I/O) to these groups. cgroups are used for resource management and isolation."\n        }\n    ],\n    "background_knowledge": [\n        "Understanding how Transparent Huge Pages (THP) are mapped into processes is crucial for optimizing memory usage and performance. THP can be mapped using either Page Table Entries (PTE) or Page Middle Directory (PMD) entries, depending on the size and alignment of the pages.",\n        "The proliferation of large folios for file-backed memory and the introduction of multi-size THP for anonymous memory have made it necessary to have tools that can provide detailed insights into how these large pages are being used and mapped."\n    ],\n    "kernel_changes": [\n        "Before the addition of the thpmaps script, there was no straightforward way to visualize and analyze how Transparent Huge Pages (THP) were being used across processes or cgroups. Developers had to rely on manual inspection of /proc/<pid>/smaps, /proc/<pid>/pagemap, and /proc/kpageflags, which was cumbersome and time-consuming.",\n        "With the introduction of the thpmaps script, developers now have a powerful tool that can dump detailed information about THP usage, including how THPs are mapped (aligned or unaligned) and whether they are fully or partially mapped. This tool helps in identifying inefficiencies and optimizing memory usage, especially in scenarios where large memory pages are being used."\n    ],\n    "comprehensive_understanding": "The thpmaps script is a significant addition to the Linux kernel\'s toolset, providing developers with a comprehensive view of how Transparent Huge Pages (THP) are being used and mapped across processes or cgroups. By reading data from /proc/<pid>/smaps, /proc/<pid>/pagemap, and /proc/kpageflags, thpmaps generates detailed statistics about THP usage, including alignment, mapping type (PTE or PMD), and whether the pages are fully or partially mapped. This tool is particularly useful for optimizing memory usage and performance, especially in architectures like arm64 where TLB usage can be optimized by ensuring memory is mapped using contpte-sized and -aligned blocks. The thpmaps script is a short-term solution that provides valuable insights without requiring any new kernel ABI, making it a practical tool for developers to debug and optimize their workloads."\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733237141, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=782, prompt_tokens=4551, total_tokens=5333, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=1472, prompt_cache_miss_tokens=3079))
2024-12-03 22:46:22,633 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个命名实体抽取 (NER) 任务的专家，同时也是 linux 内核的专家。给你一个 linux 内核特性的描述 (feature_description)、对应的若干 commit 的标题和提交信息 (commits)、该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)、为了让开发者理解该特性所需的与这个特性直接相关的背景知识 (background_knowledge)、该特性被实现前后，linux 内核发生的变化 (kernel_changes)、对该特性的综合理解 (comprehensive_understanding)。你的任务是：根据这些信息提取出在 linux 内核中涉及到的和这个特性相关的概念实体 (concepts)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}],
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": [
                "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
            ]
        },
        "output": {
            "concepts": ["Copy-on-Write (CoW)", "mTHP", "folio", "memory reclamation", "memory availability", "anti-fragmentation"]
        }
    ],
    "input": {
        "feature_description": "tools/mm: add thpmaps script to dump THP usage info",
        "commits": [{"commit_subject": "tools/mm: add thpmaps script to dump THP usage info", "commit_message": "With the proliferation of large folios for file-backed memory, and more\nrecently the introduction of multi-size THP for anonymous memory, it is\nbecoming useful to be able to see exactly how large folios are mapped into\nprocesses.  For some architectures (e.g.  arm64), if most memory is mapped\nusing contpte-sized and -aligned blocks, TLB usage can be optimized so\nit's useful to see where these requirements are and are not being met.\n\nthpmaps is a Python utility that reads /proc/<pid>/smaps,\n/proc/<pid>/pagemap and /proc/kpageflags to print information about how\ntransparent huge pages (both file and anon) are mapped to a specified\nprocess or cgroup.  It aims to help users debug and optimize their\nworkloads.  In future we may wish to introduce stats directly into the\nkernel (e.g.  smaps or similar), but for now this provides a short term\nsolution without the need to introduce any new ABI.\n\nRun with help option for a full listing of the arguments:\n\n    # ./thpmaps --help\n\n--8<--\nusage: thpmaps [-h] [--pid pid | --cgroup path] [--rollup]\n               [--cont size[KMG]] [--inc-smaps] [--inc-empty]\n               [--periodic sleep_ms]\n\nPrints information about how transparent huge pages are mapped, either\nsystem-wide, or for a specified process or cgroup.\n\nWhen run with --pid, the user explicitly specifies the set of pids to\nscan.  e.g.  \"--pid 10 [--pid 134 ...]\".  When run with --cgroup, the user\npasses either a v1 or v2 cgroup and all pids that belong to the cgroup\nsubtree are scanned.  When run with neither --pid nor --cgroup, the full\nset of pids on the system is gathered from /proc and scanned as if the\nuser had provided \"--pid 1 --pid 2 ...\".\n\nA default set of statistics is always generated for THP mappings. \nHowever, it is also possible to generate additional statistics for\n\"contiguous block mappings\" where the block size is user-defined.\n\nStatistics are maintained independently for anonymous and file-backed\n(pagecache) memory and are shown both in kB and as a percentage of either\ntotal anonymous or total file-backed memory as appropriate.\n\nTHP Statistics\n--------------\n\nStatistics are always generated for fully- and contiguously-mapped THPs\nwhose mapping address is aligned to their size, for each <size> supported\nby the system.  Separate counters describe THPs mapped by PTE vs those\nmapped by PMD.  (Although note a THP can only be mapped by PMD if it is\nPMD-sized):\n\n- anon-thp-pte-aligned-<size>kB\n- file-thp-pte-aligned-<size>kB\n- anon-thp-pmd-aligned-<size>kB\n- file-thp-pmd-aligned-<size>kB\n\nSimilarly, statistics are always generated for fully- and contiguously-\nmapped THPs whose mapping address is *not* aligned to their size, for each\n<size> supported by the system.  Due to the unaligned mapping, it is\nimpossible to map by PMD, so there are only PTE counters for this case:\n\n- anon-thp-pte-unaligned-<size>kB\n- file-thp-pte-unaligned-<size>kB\n\nStatistics are also always generated for mapped pages that belong to a THP\nbut where the is THP is *not* fully- and contiguously- mapped.  These\n\"partial\" mappings are all counted in the same counter regardless of the\nsize of the THP that is partially mapped:\n\n- anon-thp-pte-partial\n- file-thp-pte-partial\n\nContiguous Block Statistics\n---------------------------\n\nAn optional, additional set of statistics is generated for every\ncontiguous block size specified with `--cont <size>`.  These statistics\nshow how much memory is mapped in contiguous blocks of <size> and also\naligned to <size>.  A given contiguous block must all belong to the same\nTHP, but there is no requirement for it to be the *whole* THP.  Separate\ncounters describe contiguous blocks mapped by PTE vs those mapped by PMD:\n\n- anon-cont-pte-aligned-<size>kB\n- file-cont-pte-aligned-<size>kB\n- anon-cont-pmd-aligned-<size>kB\n- file-cont-pmd-aligned-<size>kB\n\nAs an example, if monitoring 64K contiguous blocks (--cont 64K), there are\na number of sources that could provide such blocks: a fully- and\ncontiguously-mapped 64K THP that is aligned to a 64K boundary would\nprovide 1 block.  A fully- and contiguously-mapped 128K THP that is\naligned to at least a 64K boundary would provide 2 blocks.  Or a 128K THP\nthat maps its first 100K, but contiguously and starting at a 64K boundary\nwould provide 1 block.  A fully- and contiguously-mapped 2M THP would\nprovide 32 blocks.  There are many other possible permutations.\n\noptions:\n  -h, --help           show this help message and exit\n  --pid pid            Process id of the target process. Maybe issued\n                       multiple times to scan multiple processes. --pid\n                       and --cgroup are mutually exclusive. If neither\n                       are provided, all processes are scanned to\n                       provide system-wide information.\n  --cgroup path        Path to the target cgroup in sysfs. Iterates\n                       over every pid in the cgroup and its children.\n                       --pid and --cgroup are mutually exclusive. If\n                       neither are provided, all processes are scanned\n                       to provide system-wide information.\n  --rollup             Sum the per-vma statistics to provide a summary\n                       over the whole system, process or cgroup.\n  --cont size[KMG]     Adds stats for memory that is mapped in\n                       contiguous blocks of <size> and also aligned to\n                       <size>. May be issued multiple times to track\n                       multiple sized blocks. Useful to infer e.g.\n                       arm64 contpte and hpa mappings. Size must be a\n                       power-of-2 number of pages.\n  --inc-smaps          Include all numerical, additive\n                       /proc/<pid>/smaps stats in the output.\n  --inc-empty          Show all statistics including those whose value\n                       is 0.\n  --periodic sleep_ms  Run in a loop, polling every sleep_ms\n                       milliseconds.\n\nRequires root privilege to access pagemap and kpageflags.\n--8<--\n\nExample command to summarise fully and partially mapped THPs and 64K\ncontiguous blocks over all VMAs in all processes in the system\n(--inc-empty forces printing stats that are 0):\n\n    # ./thpmaps --cont 64K --rollup --inc-empty\n\n--8<--\nanon-thp-pmd-aligned-2048kB:      139264 kB ( 6%)\nfile-thp-pmd-aligned-2048kB:           0 kB ( 0%)\nanon-thp-pte-aligned-16kB:             0 kB ( 0%)\nanon-thp-pte-aligned-32kB:             0 kB ( 0%)\nanon-thp-pte-aligned-64kB:         72256 kB ( 3%)\nanon-thp-pte-aligned-128kB:            0 kB ( 0%)\nanon-thp-pte-aligned-256kB:            0 kB ( 0%)\nanon-thp-pte-aligned-512kB:            0 kB ( 0%)\nanon-thp-pte-aligned-1024kB:           0 kB ( 0%)\nanon-thp-pte-aligned-2048kB:           0 kB ( 0%)\nanon-thp-pte-unaligned-16kB:           0 kB ( 0%)\nanon-thp-pte-unaligned-32kB:           0 kB ( 0%)\nanon-thp-pte-unaligned-64kB:           0 kB ( 0%)\nanon-thp-pte-unaligned-128kB:          0 kB ( 0%)\nanon-thp-pte-unaligned-256kB:          0 kB ( 0%)\nanon-thp-pte-unaligned-512kB:          0 kB ( 0%)\nanon-thp-pte-unaligned-1024kB:         0 kB ( 0%)\nanon-thp-pte-unaligned-2048kB:         0 kB ( 0%)\nanon-thp-pte-partial:              63232 kB ( 3%)\nfile-thp-pte-aligned-16kB:        809024 kB (47%)\nfile-thp-pte-aligned-32kB:         43168 kB ( 3%)\nfile-thp-pte-aligned-64kB:         98496 kB ( 6%)\nfile-thp-pte-aligned-128kB:        17536 kB ( 1%)\nfile-thp-pte-aligned-256kB:            0 kB ( 0%)\nfile-thp-pte-aligned-512kB:            0 kB ( 0%)\nfile-thp-pte-aligned-1024kB:           0 kB ( 0%)\nfile-thp-pte-aligned-2048kB:           0 kB ( 0%)\nfile-thp-pte-unaligned-16kB:       21712 kB ( 1%)\nfile-thp-pte-unaligned-32kB:         704 kB ( 0%)\nfile-thp-pte-unaligned-64kB:         896 kB ( 0%)\nfile-thp-pte-unaligned-128kB:      44928 kB ( 3%)\nfile-thp-pte-unaligned-256kB:          0 kB ( 0%)\nfile-thp-pte-unaligned-512kB:          0 kB ( 0%)\nfile-thp-pte-unaligned-1024kB:         0 kB ( 0%)\nfile-thp-pte-unaligned-2048kB:         0 kB ( 0%)\nfile-thp-pte-partial:               9252 kB ( 1%)\nanon-cont-pmd-aligned-64kB:       139264 kB ( 6%)\nfile-cont-pmd-aligned-64kB:            0 kB ( 0%)\nanon-cont-pte-aligned-64kB:       100672 kB ( 4%)\nfile-cont-pte-aligned-64kB:       161856 kB ( 9%)\n--8<--\n\nLink: https://lkml.kernel.org/r/20240116141235.960842-1-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nTested-by: Barry Song <v-songbaohua@oppo.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: John Hubbard <jhubbard@nvidia.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: William Kucharski <william.kucharski@oracle.com>\nCc: Zenghui Yu <yuzenghui@huawei.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}],
        "proper_nouns": [{"Transparent Huge Pages (THP)": "Transparent Huge Pages (THP) is a feature in the Linux kernel that aims to improve memory performance by using larger memory pages, typically 2MB or 1GB, instead of the standard 4KB pages. THP automatically manages the use of huge pages, making it easier for applications to benefit from them without requiring explicit configuration."}, {"Page Table Entries (PTE)": "Page Table Entries (PTE) are the smallest units of memory management in the Linux kernel. Each PTE corresponds to a 4KB memory page and contains information about the physical address, access permissions, and other metadata."}, {"Page Middle Directory (PMD)": "The Page Middle Directory (PMD) is a level in the multi-level page table hierarchy used by the Linux kernel. PMD entries point to large pages, such as 2MB or 1GB, and are used to reduce the overhead of managing large memory regions."}, {"cgroup": "cgroup (control group) is a Linux kernel feature that allows for the hierarchical grouping of processes and the allocation of resources (such as CPU, memory, and I/O) to these groups. cgroups are used for resource management and isolation."}],
        "background_knowledge": ["Understanding how Transparent Huge Pages (THP) are mapped into processes is crucial for optimizing memory usage and performance. THP can be mapped using either Page Table Entries (PTE) or Page Middle Directory (PMD) entries, depending on the size and alignment of the pages.", "The proliferation of large folios for file-backed memory and the introduction of multi-size THP for anonymous memory have made it necessary to have tools that can provide detailed insights into how these large pages are being used and mapped."],
        "kernel_changes": ["Before the addition of the thpmaps script, there was no straightforward way to visualize and analyze how Transparent Huge Pages (THP) were being used across processes or cgroups. Developers had to rely on manual inspection of /proc/<pid>/smaps, /proc/<pid>/pagemap, and /proc/kpageflags, which was cumbersome and time-consuming.", "With the introduction of the thpmaps script, developers now have a powerful tool that can dump detailed information about THP usage, including how THPs are mapped (aligned or unaligned) and whether they are fully or partially mapped. This tool helps in identifying inefficiencies and optimizing memory usage, especially in scenarios where large memory pages are being used."],
        "comprehensive_understanding": "The thpmaps script is a significant addition to the Linux kernel's toolset, providing developers with a comprehensive view of how Transparent Huge Pages (THP) are being used and mapped across processes or cgroups. By reading data from /proc/<pid>/smaps, /proc/<pid>/pagemap, and /proc/kpageflags, thpmaps generates detailed statistics about THP usage, including alignment, mapping type (PTE or PMD), and whether the pages are fully or partially mapped. This tool is particularly useful for optimizing memory usage and performance, especially in architectures like arm64 where TLB usage can be optimized by ensuring memory is mapped using contpte-sized and -aligned blocks. The thpmaps script is a short-term solution that provides valuable insights without requiring any new kernel ABI, making it a practical tool for developers to debug and optimize their workloads."
    }
}

2024-12-03 22:46:27,254 - extractor - DEBUG - deepseek response:
ChatCompletion(id='9b5d05dd-bf17-42ad-a541-905a90341098', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "concepts": ["Transparent Huge Pages (THP)", "Page Table Entries (PTE)", "Page Middle Directory (PMD)", "cgroup", "memory mapping", "TLB optimization", "memory usage optimization", "memory alignment", "memory performance", "resource management", "memory debugging"]\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733237182, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=72, prompt_tokens=5336, total_tokens=5408, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=1536, prompt_cache_miss_tokens=3800))
2024-12-03 22:46:27,256 - extractor - DEBUG - feature_extracted of feature_id=33213:
{
    "feature_id": 33213,
    "feature_description": "tools/mm: add thpmaps script to dump THP usage info",
    "version": "6.9",
    "commits": [
        {
            "commit_subject": "tools/mm: add thpmaps script to dump THP usage info",
            "commit_message": "With the proliferation of large folios for file-backed memory, and more\nrecently the introduction of multi-size THP for anonymous memory, it is\nbecoming useful to be able to see exactly how large folios are mapped into\nprocesses.  For some architectures (e.g.  arm64), if most memory is mapped\nusing contpte-sized and -aligned blocks, TLB usage can be optimized so\nit's useful to see where these requirements are and are not being met.\n\nthpmaps is a Python utility that reads /proc/<pid>/smaps,\n/proc/<pid>/pagemap and /proc/kpageflags to print information about how\ntransparent huge pages (both file and anon) are mapped to a specified\nprocess or cgroup.  It aims to help users debug and optimize their\nworkloads.  In future we may wish to introduce stats directly into the\nkernel (e.g.  smaps or similar), but for now this provides a short term\nsolution without the need to introduce any new ABI.\n\nRun with help option for a full listing of the arguments:\n\n    # ./thpmaps --help\n\n--8<--\nusage: thpmaps [-h] [--pid pid | --cgroup path] [--rollup]\n               [--cont size[KMG]] [--inc-smaps] [--inc-empty]\n               [--periodic sleep_ms]\n\nPrints information about how transparent huge pages are mapped, either\nsystem-wide, or for a specified process or cgroup.\n\nWhen run with --pid, the user explicitly specifies the set of pids to\nscan.  e.g.  \"--pid 10 [--pid 134 ...]\".  When run with --cgroup, the user\npasses either a v1 or v2 cgroup and all pids that belong to the cgroup\nsubtree are scanned.  When run with neither --pid nor --cgroup, the full\nset of pids on the system is gathered from /proc and scanned as if the\nuser had provided \"--pid 1 --pid 2 ...\".\n\nA default set of statistics is always generated for THP mappings. \nHowever, it is also possible to generate additional statistics for\n\"contiguous block mappings\" where the block size is user-defined.\n\nStatistics are maintained independently for anonymous and file-backed\n(pagecache) memory and are shown both in kB and as a percentage of either\ntotal anonymous or total file-backed memory as appropriate.\n\nTHP Statistics\n--------------\n\nStatistics are always generated for fully- and contiguously-mapped THPs\nwhose mapping address is aligned to their size, for each <size> supported\nby the system.  Separate counters describe THPs mapped by PTE vs those\nmapped by PMD.  (Although note a THP can only be mapped by PMD if it is\nPMD-sized):\n\n- anon-thp-pte-aligned-<size>kB\n- file-thp-pte-aligned-<size>kB\n- anon-thp-pmd-aligned-<size>kB\n- file-thp-pmd-aligned-<size>kB\n\nSimilarly, statistics are always generated for fully- and contiguously-\nmapped THPs whose mapping address is *not* aligned to their size, for each\n<size> supported by the system.  Due to the unaligned mapping, it is\nimpossible to map by PMD, so there are only PTE counters for this case:\n\n- anon-thp-pte-unaligned-<size>kB\n- file-thp-pte-unaligned-<size>kB\n\nStatistics are also always generated for mapped pages that belong to a THP\nbut where the is THP is *not* fully- and contiguously- mapped.  These\n\"partial\" mappings are all counted in the same counter regardless of the\nsize of the THP that is partially mapped:\n\n- anon-thp-pte-partial\n- file-thp-pte-partial\n\nContiguous Block Statistics\n---------------------------\n\nAn optional, additional set of statistics is generated for every\ncontiguous block size specified with `--cont <size>`.  These statistics\nshow how much memory is mapped in contiguous blocks of <size> and also\naligned to <size>.  A given contiguous block must all belong to the same\nTHP, but there is no requirement for it to be the *whole* THP.  Separate\ncounters describe contiguous blocks mapped by PTE vs those mapped by PMD:\n\n- anon-cont-pte-aligned-<size>kB\n- file-cont-pte-aligned-<size>kB\n- anon-cont-pmd-aligned-<size>kB\n- file-cont-pmd-aligned-<size>kB\n\nAs an example, if monitoring 64K contiguous blocks (--cont 64K), there are\na number of sources that could provide such blocks: a fully- and\ncontiguously-mapped 64K THP that is aligned to a 64K boundary would\nprovide 1 block.  A fully- and contiguously-mapped 128K THP that is\naligned to at least a 64K boundary would provide 2 blocks.  Or a 128K THP\nthat maps its first 100K, but contiguously and starting at a 64K boundary\nwould provide 1 block.  A fully- and contiguously-mapped 2M THP would\nprovide 32 blocks.  There are many other possible permutations.\n\noptions:\n  -h, --help           show this help message and exit\n  --pid pid            Process id of the target process. Maybe issued\n                       multiple times to scan multiple processes. --pid\n                       and --cgroup are mutually exclusive. If neither\n                       are provided, all processes are scanned to\n                       provide system-wide information.\n  --cgroup path        Path to the target cgroup in sysfs. Iterates\n                       over every pid in the cgroup and its children.\n                       --pid and --cgroup are mutually exclusive. If\n                       neither are provided, all processes are scanned\n                       to provide system-wide information.\n  --rollup             Sum the per-vma statistics to provide a summary\n                       over the whole system, process or cgroup.\n  --cont size[KMG]     Adds stats for memory that is mapped in\n                       contiguous blocks of <size> and also aligned to\n                       <size>. May be issued multiple times to track\n                       multiple sized blocks. Useful to infer e.g.\n                       arm64 contpte and hpa mappings. Size must be a\n                       power-of-2 number of pages.\n  --inc-smaps          Include all numerical, additive\n                       /proc/<pid>/smaps stats in the output.\n  --inc-empty          Show all statistics including those whose value\n                       is 0.\n  --periodic sleep_ms  Run in a loop, polling every sleep_ms\n                       milliseconds.\n\nRequires root privilege to access pagemap and kpageflags.\n--8<--\n\nExample command to summarise fully and partially mapped THPs and 64K\ncontiguous blocks over all VMAs in all processes in the system\n(--inc-empty forces printing stats that are 0):\n\n    # ./thpmaps --cont 64K --rollup --inc-empty\n\n--8<--\nanon-thp-pmd-aligned-2048kB:      139264 kB ( 6%)\nfile-thp-pmd-aligned-2048kB:           0 kB ( 0%)\nanon-thp-pte-aligned-16kB:             0 kB ( 0%)\nanon-thp-pte-aligned-32kB:             0 kB ( 0%)\nanon-thp-pte-aligned-64kB:         72256 kB ( 3%)\nanon-thp-pte-aligned-128kB:            0 kB ( 0%)\nanon-thp-pte-aligned-256kB:            0 kB ( 0%)\nanon-thp-pte-aligned-512kB:            0 kB ( 0%)\nanon-thp-pte-aligned-1024kB:           0 kB ( 0%)\nanon-thp-pte-aligned-2048kB:           0 kB ( 0%)\nanon-thp-pte-unaligned-16kB:           0 kB ( 0%)\nanon-thp-pte-unaligned-32kB:           0 kB ( 0%)\nanon-thp-pte-unaligned-64kB:           0 kB ( 0%)\nanon-thp-pte-unaligned-128kB:          0 kB ( 0%)\nanon-thp-pte-unaligned-256kB:          0 kB ( 0%)\nanon-thp-pte-unaligned-512kB:          0 kB ( 0%)\nanon-thp-pte-unaligned-1024kB:         0 kB ( 0%)\nanon-thp-pte-unaligned-2048kB:         0 kB ( 0%)\nanon-thp-pte-partial:              63232 kB ( 3%)\nfile-thp-pte-aligned-16kB:        809024 kB (47%)\nfile-thp-pte-aligned-32kB:         43168 kB ( 3%)\nfile-thp-pte-aligned-64kB:         98496 kB ( 6%)\nfile-thp-pte-aligned-128kB:        17536 kB ( 1%)\nfile-thp-pte-aligned-256kB:            0 kB ( 0%)\nfile-thp-pte-aligned-512kB:            0 kB ( 0%)\nfile-thp-pte-aligned-1024kB:           0 kB ( 0%)\nfile-thp-pte-aligned-2048kB:           0 kB ( 0%)\nfile-thp-pte-unaligned-16kB:       21712 kB ( 1%)\nfile-thp-pte-unaligned-32kB:         704 kB ( 0%)\nfile-thp-pte-unaligned-64kB:         896 kB ( 0%)\nfile-thp-pte-unaligned-128kB:      44928 kB ( 3%)\nfile-thp-pte-unaligned-256kB:          0 kB ( 0%)\nfile-thp-pte-unaligned-512kB:          0 kB ( 0%)\nfile-thp-pte-unaligned-1024kB:         0 kB ( 0%)\nfile-thp-pte-unaligned-2048kB:         0 kB ( 0%)\nfile-thp-pte-partial:               9252 kB ( 1%)\nanon-cont-pmd-aligned-64kB:       139264 kB ( 6%)\nfile-cont-pmd-aligned-64kB:            0 kB ( 0%)\nanon-cont-pte-aligned-64kB:       100672 kB ( 4%)\nfile-cont-pte-aligned-64kB:       161856 kB ( 9%)\n--8<--\n\nLink: https://lkml.kernel.org/r/20240116141235.960842-1-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nTested-by: Barry Song <v-songbaohua@oppo.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: John Hubbard <jhubbard@nvidia.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: William Kucharski <william.kucharski@oracle.com>\nCc: Zenghui Yu <yuzenghui@huawei.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        }
    ],
    "proper_nouns": [
        {
            "Transparent Huge Pages (THP)": "Transparent Huge Pages (THP) is a feature in the Linux kernel that aims to improve memory performance by using larger memory pages, typically 2MB or 1GB, instead of the standard 4KB pages. THP automatically manages the use of huge pages, making it easier for applications to benefit from them without requiring explicit configuration."
        },
        {
            "Page Table Entries (PTE)": "Page Table Entries (PTE) are the smallest units of memory management in the Linux kernel. Each PTE corresponds to a 4KB memory page and contains information about the physical address, access permissions, and other metadata."
        },
        {
            "Page Middle Directory (PMD)": "The Page Middle Directory (PMD) is a level in the multi-level page table hierarchy used by the Linux kernel. PMD entries point to large pages, such as 2MB or 1GB, and are used to reduce the overhead of managing large memory regions."
        },
        {
            "cgroup": "cgroup (control group) is a Linux kernel feature that allows for the hierarchical grouping of processes and the allocation of resources (such as CPU, memory, and I/O) to these groups. cgroups are used for resource management and isolation."
        }
    ],
    "background_knowledge": [
        "Understanding how Transparent Huge Pages (THP) are mapped into processes is crucial for optimizing memory usage and performance. THP can be mapped using either Page Table Entries (PTE) or Page Middle Directory (PMD) entries, depending on the size and alignment of the pages.",
        "The proliferation of large folios for file-backed memory and the introduction of multi-size THP for anonymous memory have made it necessary to have tools that can provide detailed insights into how these large pages are being used and mapped."
    ],
    "kernel_changes": [
        "Before the addition of the thpmaps script, there was no straightforward way to visualize and analyze how Transparent Huge Pages (THP) were being used across processes or cgroups. Developers had to rely on manual inspection of /proc/<pid>/smaps, /proc/<pid>/pagemap, and /proc/kpageflags, which was cumbersome and time-consuming.",
        "With the introduction of the thpmaps script, developers now have a powerful tool that can dump detailed information about THP usage, including how THPs are mapped (aligned or unaligned) and whether they are fully or partially mapped. This tool helps in identifying inefficiencies and optimizing memory usage, especially in scenarios where large memory pages are being used."
    ],
    "comprehensive_understanding": "The thpmaps script is a significant addition to the Linux kernel's toolset, providing developers with a comprehensive view of how Transparent Huge Pages (THP) are being used and mapped across processes or cgroups. By reading data from /proc/<pid>/smaps, /proc/<pid>/pagemap, and /proc/kpageflags, thpmaps generates detailed statistics about THP usage, including alignment, mapping type (PTE or PMD), and whether the pages are fully or partially mapped. This tool is particularly useful for optimizing memory usage and performance, especially in architectures like arm64 where TLB usage can be optimized by ensuring memory is mapped using contpte-sized and -aligned blocks. The thpmaps script is a short-term solution that provides valuable insights without requiring any new kernel ABI, making it a practical tool for developers to debug and optimize their workloads.",
    "concepts": [
        "Transparent Huge Pages (THP)",
        "Page Table Entries (PTE)",
        "Page Middle Directory (PMD)",
        "cgroup",
        "memory mapping",
        "TLB optimization",
        "memory usage optimization",
        "memory alignment",
        "memory performance",
        "resource management",
        "memory debugging"
    ]
}
2024-12-03 22:46:27,256 - extractor - INFO - ---------Processing feature 28/815---------
2024-12-03 22:46:31,491 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个 linux 内核的专家。给你一个 linux 内核特性的描述以及其对应的若干 commit 的标题和提交信息，你的任务是为这个特性写一个多方面的扩写版描述。你的扩写需包含以下几个方面：1. 该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)；2. 为了让开发者理解该特性，需要哪些具体的，与这个特性直接相关的背景知识 (background_knowledge)；3. 该特性被实现前后，linux 内核发生了怎样的变化 (kernel_changes)；4. 你对该特性的综合理解 (comprehensive_understanding)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}]
        },
        "output": {
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
        }
    ],
    "input": {
        "feature_description": "huge_memory: enable debugfs to split huge pages to any order",
        "commits": [{"commit_subject": "mm: huge_memory: enable debugfs to split huge pages to any order", "commit_message": "It is used to test split_huge_page_to_list_to_order for pagecache THPs. \nAlso add test cases for split_huge_page_to_list_to_order via both debugfs.\n\n[ziy@nvidia.com: fix issue discovered with NFS]\n  Link: https://lkml.kernel.org/r/262E4DAA-4A78-4328-B745-1355AE356A07@nvidia.com\nLink: https://lkml.kernel.org/r/20240226205534.1603748-9-zi.yan@sent.com\nSigned-off-by: Zi Yan <ziy@nvidia.com>\nTested-by: Aishwarya TCV <aishwarya.tcv@arm.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Hugh Dickins <hughd@google.com>\nCc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>\nCc: Luis Chamberlain <mcgrof@kernel.org>\nCc: \"Matthew Wilcox (Oracle)\" <willy@infradead.org>\nCc: Michal Koutny <mkoutny@suse.com>\nCc: Roman Gushchin <roman.gushchin@linux.dev>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Yu Zhao <yuzhao@google.com>\nCc: Zach O'Keefe <zokeefe@google.com>\nCc: Aishwarya TCV <aishwarya.tcv@arm.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}]
    }
}

2024-12-03 22:47:02,910 - extractor - DEBUG - deepseek response:
ChatCompletion(id='884b6e6a-6383-44d4-8ef9-892e3b0c6989', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "proper_nouns": [\n        {\n            "debugfs": "debugfs is a virtual file system provided by the Linux kernel, primarily used for debugging purposes. It allows kernel developers to expose internal kernel data structures and variables to user space, making it easier to monitor and debug kernel behavior."\n        },\n        {\n            "THP (Transparent Huge Pages)": "Transparent Huge Pages (THP) is a feature in the Linux kernel that automatically manages and uses large memory pages (huge pages) to improve memory performance. THP reduces the overhead associated with managing smaller memory pages, such as 4KB pages, by using larger pages, typically 2MB or 1GB, depending on the architecture."\n        },\n        {\n            "pagecache": "The pagecache is a cache in the Linux kernel that stores file system data in memory. It is used to speed up file access by reducing the need to read data from disk. The pagecache is managed using memory pages, and THP can be used to manage these pages more efficiently."\n        }\n    ],\n    "background_knowledge": [\n        "The ability to split huge pages into smaller pages of any order is crucial for testing and debugging memory management in the Linux kernel. This feature allows developers to simulate various memory fragmentation scenarios and verify the robustness of the memory management subsystem.",\n        "Understanding the behavior of THP in the pagecache is essential for this feature, as it directly impacts how file system data is cached and managed in memory. THP can significantly improve performance by reducing the number of memory page faults, but it also introduces complexities in managing memory fragmentation."\n    ],\n    "kernel_changes": [\n        "Before this feature was implemented, the Linux kernel did not provide a direct mechanism to split huge pages into smaller pages of any order via debugfs. This limitation made it challenging to test and debug scenarios where memory fragmentation occurred due to THP.",\n        "After the implementation, developers can now use debugfs to split huge pages into smaller pages of any order. This capability allows for more granular control over memory management, enabling better testing and debugging of memory fragmentation issues. As a result, the kernel\'s memory management subsystem has become more robust and flexible."\n    ],\n    "comprehensive_understanding": "This feature enhances the Linux kernel\'s memory management capabilities by enabling the splitting of huge pages into smaller pages of any order via debugfs. This functionality is particularly useful for testing and debugging scenarios involving memory fragmentation in the pagecache. By providing developers with more control over memory page management, this feature improves the robustness and flexibility of the kernel\'s memory management subsystem, ultimately leading to better system performance and reliability."\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733237191, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=572, prompt_tokens=2031, total_tokens=2603, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=1472, prompt_cache_miss_tokens=559))
2024-12-03 22:47:02,910 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个命名实体抽取 (NER) 任务的专家，同时也是 linux 内核的专家。给你一个 linux 内核特性的描述 (feature_description)、对应的若干 commit 的标题和提交信息 (commits)、该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)、为了让开发者理解该特性所需的与这个特性直接相关的背景知识 (background_knowledge)、该特性被实现前后，linux 内核发生的变化 (kernel_changes)、对该特性的综合理解 (comprehensive_understanding)。你的任务是：根据这些信息提取出在 linux 内核中涉及到的和这个特性相关的概念实体 (concepts)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}],
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": [
                "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
            ]
        },
        "output": {
            "concepts": ["Copy-on-Write (CoW)", "mTHP", "folio", "memory reclamation", "memory availability", "anti-fragmentation"]
        }
    ],
    "input": {
        "feature_description": "huge_memory: enable debugfs to split huge pages to any order",
        "commits": [{"commit_subject": "mm: huge_memory: enable debugfs to split huge pages to any order", "commit_message": "It is used to test split_huge_page_to_list_to_order for pagecache THPs. \nAlso add test cases for split_huge_page_to_list_to_order via both debugfs.\n\n[ziy@nvidia.com: fix issue discovered with NFS]\n  Link: https://lkml.kernel.org/r/262E4DAA-4A78-4328-B745-1355AE356A07@nvidia.com\nLink: https://lkml.kernel.org/r/20240226205534.1603748-9-zi.yan@sent.com\nSigned-off-by: Zi Yan <ziy@nvidia.com>\nTested-by: Aishwarya TCV <aishwarya.tcv@arm.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Hugh Dickins <hughd@google.com>\nCc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>\nCc: Luis Chamberlain <mcgrof@kernel.org>\nCc: \"Matthew Wilcox (Oracle)\" <willy@infradead.org>\nCc: Michal Koutny <mkoutny@suse.com>\nCc: Roman Gushchin <roman.gushchin@linux.dev>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Yu Zhao <yuzhao@google.com>\nCc: Zach O'Keefe <zokeefe@google.com>\nCc: Aishwarya TCV <aishwarya.tcv@arm.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}],
        "proper_nouns": [{"debugfs": "debugfs is a virtual file system provided by the Linux kernel, primarily used for debugging purposes. It allows kernel developers to expose internal kernel data structures and variables to user space, making it easier to monitor and debug kernel behavior."}, {"THP (Transparent Huge Pages)": "Transparent Huge Pages (THP) is a feature in the Linux kernel that automatically manages and uses large memory pages (huge pages) to improve memory performance. THP reduces the overhead associated with managing smaller memory pages, such as 4KB pages, by using larger pages, typically 2MB or 1GB, depending on the architecture."}, {"pagecache": "The pagecache is a cache in the Linux kernel that stores file system data in memory. It is used to speed up file access by reducing the need to read data from disk. The pagecache is managed using memory pages, and THP can be used to manage these pages more efficiently."}],
        "background_knowledge": ["The ability to split huge pages into smaller pages of any order is crucial for testing and debugging memory management in the Linux kernel. This feature allows developers to simulate various memory fragmentation scenarios and verify the robustness of the memory management subsystem.", "Understanding the behavior of THP in the pagecache is essential for this feature, as it directly impacts how file system data is cached and managed in memory. THP can significantly improve performance by reducing the number of memory page faults, but it also introduces complexities in managing memory fragmentation."],
        "kernel_changes": ["Before this feature was implemented, the Linux kernel did not provide a direct mechanism to split huge pages into smaller pages of any order via debugfs. This limitation made it challenging to test and debug scenarios where memory fragmentation occurred due to THP.", "After the implementation, developers can now use debugfs to split huge pages into smaller pages of any order. This capability allows for more granular control over memory management, enabling better testing and debugging of memory fragmentation issues. As a result, the kernel's memory management subsystem has become more robust and flexible."],
        "comprehensive_understanding": "This feature enhances the Linux kernel's memory management capabilities by enabling the splitting of huge pages into smaller pages of any order via debugfs. This functionality is particularly useful for testing and debugging scenarios involving memory fragmentation in the pagecache. By providing developers with more control over memory page management, this feature improves the robustness and flexibility of the kernel's memory management subsystem, ultimately leading to better system performance and reliability."
    }
}

2024-12-03 22:47:06,090 - extractor - DEBUG - deepseek response:
ChatCompletion(id='810a198b-2ae1-4541-930b-1619889fe1b0', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "concepts": ["debugfs", "THP (Transparent Huge Pages)", "pagecache", "memory fragmentation", "memory management", "huge pages", "memory page management"]\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733237222, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=48, prompt_tokens=2613, total_tokens=2661, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=1536, prompt_cache_miss_tokens=1077))
2024-12-03 22:47:06,090 - extractor - DEBUG - feature_extracted of feature_id=33212:
{
    "feature_id": 33212,
    "feature_description": "huge_memory: enable debugfs to split huge pages to any order",
    "version": "6.9",
    "commits": [
        {
            "commit_subject": "mm: huge_memory: enable debugfs to split huge pages to any order",
            "commit_message": "It is used to test split_huge_page_to_list_to_order for pagecache THPs. \nAlso add test cases for split_huge_page_to_list_to_order via both debugfs.\n\n[ziy@nvidia.com: fix issue discovered with NFS]\n  Link: https://lkml.kernel.org/r/262E4DAA-4A78-4328-B745-1355AE356A07@nvidia.com\nLink: https://lkml.kernel.org/r/20240226205534.1603748-9-zi.yan@sent.com\nSigned-off-by: Zi Yan <ziy@nvidia.com>\nTested-by: Aishwarya TCV <aishwarya.tcv@arm.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: Hugh Dickins <hughd@google.com>\nCc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>\nCc: Luis Chamberlain <mcgrof@kernel.org>\nCc: \"Matthew Wilcox (Oracle)\" <willy@infradead.org>\nCc: Michal Koutny <mkoutny@suse.com>\nCc: Roman Gushchin <roman.gushchin@linux.dev>\nCc: Ryan Roberts <ryan.roberts@arm.com>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Yu Zhao <yuzhao@google.com>\nCc: Zach O'Keefe <zokeefe@google.com>\nCc: Aishwarya TCV <aishwarya.tcv@arm.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        }
    ],
    "proper_nouns": [
        {
            "debugfs": "debugfs is a virtual file system provided by the Linux kernel, primarily used for debugging purposes. It allows kernel developers to expose internal kernel data structures and variables to user space, making it easier to monitor and debug kernel behavior."
        },
        {
            "THP (Transparent Huge Pages)": "Transparent Huge Pages (THP) is a feature in the Linux kernel that automatically manages and uses large memory pages (huge pages) to improve memory performance. THP reduces the overhead associated with managing smaller memory pages, such as 4KB pages, by using larger pages, typically 2MB or 1GB, depending on the architecture."
        },
        {
            "pagecache": "The pagecache is a cache in the Linux kernel that stores file system data in memory. It is used to speed up file access by reducing the need to read data from disk. The pagecache is managed using memory pages, and THP can be used to manage these pages more efficiently."
        }
    ],
    "background_knowledge": [
        "The ability to split huge pages into smaller pages of any order is crucial for testing and debugging memory management in the Linux kernel. This feature allows developers to simulate various memory fragmentation scenarios and verify the robustness of the memory management subsystem.",
        "Understanding the behavior of THP in the pagecache is essential for this feature, as it directly impacts how file system data is cached and managed in memory. THP can significantly improve performance by reducing the number of memory page faults, but it also introduces complexities in managing memory fragmentation."
    ],
    "kernel_changes": [
        "Before this feature was implemented, the Linux kernel did not provide a direct mechanism to split huge pages into smaller pages of any order via debugfs. This limitation made it challenging to test and debug scenarios where memory fragmentation occurred due to THP.",
        "After the implementation, developers can now use debugfs to split huge pages into smaller pages of any order. This capability allows for more granular control over memory management, enabling better testing and debugging of memory fragmentation issues. As a result, the kernel's memory management subsystem has become more robust and flexible."
    ],
    "comprehensive_understanding": "This feature enhances the Linux kernel's memory management capabilities by enabling the splitting of huge pages into smaller pages of any order via debugfs. This functionality is particularly useful for testing and debugging scenarios involving memory fragmentation in the pagecache. By providing developers with more control over memory page management, this feature improves the robustness and flexibility of the kernel's memory management subsystem, ultimately leading to better system performance and reliability.",
    "concepts": [
        "debugfs",
        "THP (Transparent Huge Pages)",
        "pagecache",
        "memory fragmentation",
        "memory management",
        "huge pages",
        "memory page management"
    ]
}
2024-12-03 22:47:06,091 - extractor - INFO - ---------Processing feature 29/815---------
2024-12-03 22:47:10,885 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个 linux 内核的专家。给你一个 linux 内核特性的描述以及其对应的若干 commit 的标题和提交信息，你的任务是为这个特性写一个多方面的扩写版描述。你的扩写需包含以下几个方面：1. 该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)；2. 为了让开发者理解该特性，需要哪些具体的，与这个特性直接相关的背景知识 (background_knowledge)；3. 该特性被实现前后，linux 内核发生了怎样的变化 (kernel_changes)；4. 你对该特性的综合理解 (comprehensive_understanding)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}]
        },
        "output": {
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
        }
    ],
    "input": {
        "feature_description": "increase folio batch size",
        "commits": [{"commit_subject": "mm: increase folio batch size", "commit_message": "On a 104 thread, 2 socket Skylake system, Intel report a 4.7% performance\nreduction with will-it-scale page_fault2.  This was due to reducing the\nsize of the batch from 32 to 15.  Increasing the folio batch size from 15\nto 31 gives a performance increase of 12.5% relative to the original, or\n17.2% relative to the reduced performance commit.\n\nThe penalty of this commit is an additional 128 bytes of stack usage.  Six\nfolio_batches are also allocated from percpu memory in cpu_fbatches so\nthat will be an additional 768 bytes of percpu memory (per CPU).  Tim Chen\noriginally submitted a patch like this in 2020:\nhttps://lore.kernel.org/linux-mm/d1cc9f12a8ad6c2a52cb600d93b06b064f2bbc57.1593205965.git.tim.c.chen@linux.intel.com/\n\nLink: https://lkml.kernel.org/r/20240315140823.2478146-1-willy@infradead.org\nFixes: 99fbb6bfc16f (\"mm: make folios_put() the basis of release_pages()\")\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nTested-by: Yujie Liu <yujie.liu@intel.com>\nReported-by: kernel test robot <oliver.sang@intel.com>\nCloses: https://lore.kernel.org/oe-lkp/202403151058.7048f6a8-oliver.sang@intel.com\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}]
    }
}

2024-12-03 22:47:46,711 - extractor - DEBUG - deepseek response:
ChatCompletion(id='7bc6dfae-f05e-4eca-b796-feb26a93b479', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "proper_nouns": [\n        {\n            "folio": "In the Linux kernel, a \'folio\' is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages."\n        },\n        {\n            "folio batch size": "The \'folio batch size\' refers to the number of folios that are processed together in a single batch operation. This batching mechanism is used to optimize memory management operations, such as page fault handling and memory reclamation, by reducing the overhead associated with individual folio operations."\n        },\n        {\n            "Skylake system": "The \'Skylake system\' refers to a specific generation of Intel processors, known as Skylake, which are used in various computing systems. These processors are characterized by their high performance and efficiency, making them suitable for a wide range of applications, including high-performance computing and data processing."\n        }\n    ],\n    "background_knowledge": [\n        "The performance of the Linux kernel, particularly in high-thread-count scenarios, can be significantly affected by the efficiency of memory management operations. One such operation is the handling of page faults, which occur when a process attempts to access a memory page that is not currently loaded into RAM. The Linux kernel uses a batching mechanism to optimize these operations, processing multiple folios together to reduce overhead. The size of these batches, known as the \'folio batch size,\' directly impacts the performance of memory management operations."\n    ],\n    "kernel_changes": [\n        "Before the implementation of this feature, the folio batch size was set to 15, which was found to cause a performance reduction of 4.7% on a 104-thread, 2-socket Skylake system. This performance degradation was attributed to the reduced efficiency of batch operations in handling page faults. After the implementation of this feature, the folio batch size was increased from 15 to 31, resulting in a performance increase of 12.5% relative to the original configuration, or 17.2% relative to the reduced performance commit. This change not only improved the performance of memory management operations but also increased the efficiency of the Linux kernel in high-thread-count scenarios."\n    ],\n    "comprehensive_understanding": "This feature addresses a performance bottleneck in the Linux kernel\'s memory management system by increasing the folio batch size. The original batch size of 15 was found to cause a significant performance reduction in high-thread-count scenarios, particularly on systems with multiple sockets and high-performance processors like Intel\'s Skylake. By increasing the batch size to 31, the kernel is able to process more folios in a single operation, reducing the overhead associated with individual folio operations and improving overall system performance. This change is particularly beneficial in environments where memory management operations are frequent and resource-intensive, such as in high-performance computing and data processing applications."\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733237230, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=660, prompt_tokens=2048, total_tokens=2708, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=1472, prompt_cache_miss_tokens=576))
2024-12-03 22:47:46,712 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个命名实体抽取 (NER) 任务的专家，同时也是 linux 内核的专家。给你一个 linux 内核特性的描述 (feature_description)、对应的若干 commit 的标题和提交信息 (commits)、该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)、为了让开发者理解该特性所需的与这个特性直接相关的背景知识 (background_knowledge)、该特性被实现前后，linux 内核发生的变化 (kernel_changes)、对该特性的综合理解 (comprehensive_understanding)。你的任务是：根据这些信息提取出在 linux 内核中涉及到的和这个特性相关的概念实体 (concepts)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}],
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": [
                "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
            ]
        },
        "output": {
            "concepts": ["Copy-on-Write (CoW)", "mTHP", "folio", "memory reclamation", "memory availability", "anti-fragmentation"]
        }
    ],
    "input": {
        "feature_description": "increase folio batch size",
        "commits": [{"commit_subject": "mm: increase folio batch size", "commit_message": "On a 104 thread, 2 socket Skylake system, Intel report a 4.7% performance\nreduction with will-it-scale page_fault2.  This was due to reducing the\nsize of the batch from 32 to 15.  Increasing the folio batch size from 15\nto 31 gives a performance increase of 12.5% relative to the original, or\n17.2% relative to the reduced performance commit.\n\nThe penalty of this commit is an additional 128 bytes of stack usage.  Six\nfolio_batches are also allocated from percpu memory in cpu_fbatches so\nthat will be an additional 768 bytes of percpu memory (per CPU).  Tim Chen\noriginally submitted a patch like this in 2020:\nhttps://lore.kernel.org/linux-mm/d1cc9f12a8ad6c2a52cb600d93b06b064f2bbc57.1593205965.git.tim.c.chen@linux.intel.com/\n\nLink: https://lkml.kernel.org/r/20240315140823.2478146-1-willy@infradead.org\nFixes: 99fbb6bfc16f (\"mm: make folios_put() the basis of release_pages()\")\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nTested-by: Yujie Liu <yujie.liu@intel.com>\nReported-by: kernel test robot <oliver.sang@intel.com>\nCloses: https://lore.kernel.org/oe-lkp/202403151058.7048f6a8-oliver.sang@intel.com\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}],
        "proper_nouns": [{"folio": "In the Linux kernel, a 'folio' is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages."}, {"folio batch size": "The 'folio batch size' refers to the number of folios that are processed together in a single batch operation. This batching mechanism is used to optimize memory management operations, such as page fault handling and memory reclamation, by reducing the overhead associated with individual folio operations."}, {"Skylake system": "The 'Skylake system' refers to a specific generation of Intel processors, known as Skylake, which are used in various computing systems. These processors are characterized by their high performance and efficiency, making them suitable for a wide range of applications, including high-performance computing and data processing."}],
        "background_knowledge": ["The performance of the Linux kernel, particularly in high-thread-count scenarios, can be significantly affected by the efficiency of memory management operations. One such operation is the handling of page faults, which occur when a process attempts to access a memory page that is not currently loaded into RAM. The Linux kernel uses a batching mechanism to optimize these operations, processing multiple folios together to reduce overhead. The size of these batches, known as the 'folio batch size,' directly impacts the performance of memory management operations."],
        "kernel_changes": ["Before the implementation of this feature, the folio batch size was set to 15, which was found to cause a performance reduction of 4.7% on a 104-thread, 2-socket Skylake system. This performance degradation was attributed to the reduced efficiency of batch operations in handling page faults. After the implementation of this feature, the folio batch size was increased from 15 to 31, resulting in a performance increase of 12.5% relative to the original configuration, or 17.2% relative to the reduced performance commit. This change not only improved the performance of memory management operations but also increased the efficiency of the Linux kernel in high-thread-count scenarios."],
        "comprehensive_understanding": "This feature addresses a performance bottleneck in the Linux kernel's memory management system by increasing the folio batch size. The original batch size of 15 was found to cause a significant performance reduction in high-thread-count scenarios, particularly on systems with multiple sockets and high-performance processors like Intel's Skylake. By increasing the batch size to 31, the kernel is able to process more folios in a single operation, reducing the overhead associated with individual folio operations and improving overall system performance. This change is particularly beneficial in environments where memory management operations are frequent and resource-intensive, such as in high-performance computing and data processing applications."
    }
}

2024-12-03 22:47:50,552 - extractor - DEBUG - deepseek response:
ChatCompletion(id='b0c1572e-48f3-49ea-a161-4514f91d8603', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "concepts": ["folio", "folio batch size", "Skylake system", "memory management", "page fault handling", "high-thread-count scenarios", "performance optimization", "batching mechanism", "memory reclamation"]\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733237266, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=59, prompt_tokens=2722, total_tokens=2781, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=1536, prompt_cache_miss_tokens=1186))
2024-12-03 22:47:50,552 - extractor - DEBUG - feature_extracted of feature_id=33211:
{
    "feature_id": 33211,
    "feature_description": "increase folio batch size",
    "version": "6.9",
    "commits": [
        {
            "commit_subject": "mm: increase folio batch size",
            "commit_message": "On a 104 thread, 2 socket Skylake system, Intel report a 4.7% performance\nreduction with will-it-scale page_fault2.  This was due to reducing the\nsize of the batch from 32 to 15.  Increasing the folio batch size from 15\nto 31 gives a performance increase of 12.5% relative to the original, or\n17.2% relative to the reduced performance commit.\n\nThe penalty of this commit is an additional 128 bytes of stack usage.  Six\nfolio_batches are also allocated from percpu memory in cpu_fbatches so\nthat will be an additional 768 bytes of percpu memory (per CPU).  Tim Chen\noriginally submitted a patch like this in 2020:\nhttps://lore.kernel.org/linux-mm/d1cc9f12a8ad6c2a52cb600d93b06b064f2bbc57.1593205965.git.tim.c.chen@linux.intel.com/\n\nLink: https://lkml.kernel.org/r/20240315140823.2478146-1-willy@infradead.org\nFixes: 99fbb6bfc16f (\"mm: make folios_put() the basis of release_pages()\")\nSigned-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>\nTested-by: Yujie Liu <yujie.liu@intel.com>\nReported-by: kernel test robot <oliver.sang@intel.com>\nCloses: https://lore.kernel.org/oe-lkp/202403151058.7048f6a8-oliver.sang@intel.com\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        }
    ],
    "proper_nouns": [
        {
            "folio": "In the Linux kernel, a 'folio' is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages."
        },
        {
            "folio batch size": "The 'folio batch size' refers to the number of folios that are processed together in a single batch operation. This batching mechanism is used to optimize memory management operations, such as page fault handling and memory reclamation, by reducing the overhead associated with individual folio operations."
        },
        {
            "Skylake system": "The 'Skylake system' refers to a specific generation of Intel processors, known as Skylake, which are used in various computing systems. These processors are characterized by their high performance and efficiency, making them suitable for a wide range of applications, including high-performance computing and data processing."
        }
    ],
    "background_knowledge": [
        "The performance of the Linux kernel, particularly in high-thread-count scenarios, can be significantly affected by the efficiency of memory management operations. One such operation is the handling of page faults, which occur when a process attempts to access a memory page that is not currently loaded into RAM. The Linux kernel uses a batching mechanism to optimize these operations, processing multiple folios together to reduce overhead. The size of these batches, known as the 'folio batch size,' directly impacts the performance of memory management operations."
    ],
    "kernel_changes": [
        "Before the implementation of this feature, the folio batch size was set to 15, which was found to cause a performance reduction of 4.7% on a 104-thread, 2-socket Skylake system. This performance degradation was attributed to the reduced efficiency of batch operations in handling page faults. After the implementation of this feature, the folio batch size was increased from 15 to 31, resulting in a performance increase of 12.5% relative to the original configuration, or 17.2% relative to the reduced performance commit. This change not only improved the performance of memory management operations but also increased the efficiency of the Linux kernel in high-thread-count scenarios."
    ],
    "comprehensive_understanding": "This feature addresses a performance bottleneck in the Linux kernel's memory management system by increasing the folio batch size. The original batch size of 15 was found to cause a significant performance reduction in high-thread-count scenarios, particularly on systems with multiple sockets and high-performance processors like Intel's Skylake. By increasing the batch size to 31, the kernel is able to process more folios in a single operation, reducing the overhead associated with individual folio operations and improving overall system performance. This change is particularly beneficial in environments where memory management operations are frequent and resource-intensive, such as in high-performance computing and data processing applications.",
    "concepts": [
        "folio",
        "folio batch size",
        "Skylake system",
        "memory management",
        "page fault handling",
        "high-thread-count scenarios",
        "performance optimization",
        "batching mechanism",
        "memory reclamation"
    ]
}
2024-12-03 22:47:50,553 - extractor - INFO - ---------Processing feature 30/815---------
2024-12-03 22:49:05,414 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个 linux 内核的专家。给你一个 linux 内核特性的描述以及其对应的若干 commit 的标题和提交信息，你的任务是为这个特性写一个多方面的扩写版描述。你的扩写需包含以下几个方面：1. 该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)；2. 为了让开发者理解该特性，需要哪些具体的，与这个特性直接相关的背景知识 (background_knowledge)；3. 该特性被实现前后，linux 内核发生了怎样的变化 (kernel_changes)；4. 你对该特性的综合理解 (comprehensive_understanding)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}]
        },
        "output": {
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
        }
    ],
    "input": {
        "feature_description": "Transparent Contiguous PTEs for User Mappings",
        "commits": [{"commit_subject": "arm64/mm: implement new wrprotect_ptes() batch API", "commit_message": "Optimize the contpte implementation to fix some of the fork performance\nregression introduced by the initial contpte commit.  Subsequent patches\nwill solve it entirely.\n\nDuring fork(), any private memory in the parent must be write-protected. \nPreviously this was done 1 PTE at a time.  But the core-mm supports\nbatched wrprotect via the new wrprotect_ptes() API.  So let's implement\nthat API and for fully covered contpte mappings, we no longer need to\nunfold the contpte.  This has 2 benefits:\n\n  - reduced unfolding, reduces the number of tlbis that must be issued.\n  - The memory remains contpte-mapped (\"folded\") in the parent, so it\n    continues to benefit from the more efficient use of the TLB after\n    the fork.\n\nThe optimization to wrprotect a whole contpte block without unfolding is\npossible thanks to the tightening of the Arm ARM in respect to the\ndefinition and behaviour when 'Misprogramming the Contiguous bit'.  See\nsection D21194 at https://developer.arm.com/documentation/102105/ja-07/\n\nLink: https://lkml.kernel.org/r/20240215103205.2607016-14-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nTested-by: John Hubbard <jhubbard@nvidia.com>\nAcked-by: Mark Rutland <mark.rutland@arm.com>\nAcked-by: Catalin Marinas <catalin.marinas@arm.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Ard Biesheuvel <ardb@kernel.org>\nCc: Barry Song <21cnbao@gmail.com>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: James Morse <james.morse@arm.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Marc Zyngier <maz@kernel.org>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Will Deacon <will@kernel.org>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: add pte_batch_hint() to reduce scanning in folio_pte_batch()", "commit_message": "Some architectures (e.g.  arm64) can tell from looking at a pte, if some\nfollow-on ptes also map contiguous physical memory with the same pgprot. \n(for arm64, these are contpte mappings).\n\nTake advantage of this knowledge to optimize folio_pte_batch() so that it\ncan skip these ptes when scanning to create a batch.  By default, if an\narch does not opt-in, folio_pte_batch() returns a compile-time 1, so the\nchanges are optimized out and the behaviour is as before.\n\narm64 will opt-in to providing this hint in the next patch, which will\ngreatly reduce the cost of ptep_get() when scanning a range of contptes.\n\nLink: https://lkml.kernel.org/r/20240215103205.2607016-16-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nAcked-by: David Hildenbrand <david@redhat.com>\nTested-by: John Hubbard <jhubbard@nvidia.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Ard Biesheuvel <ardb@kernel.org>\nCc: Barry Song <21cnbao@gmail.com>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: James Morse <james.morse@arm.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Marc Zyngier <maz@kernel.org>\nCc: Mark Rutland <mark.rutland@arm.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Will Deacon <will@kernel.org>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "arm64/mm: __always_inline to improve fork() perf", "commit_message": "As set_ptes() and wrprotect_ptes() become a bit more complex, the compiler\nmay choose not to inline them.  But this is critical for fork()\nperformance.  So mark the functions, along with contpte_try_unfold() which\nis called by them, as __always_inline.  This is worth ~1% on the fork()\nmicrobenchmark with order-0 folios (the common case).\n\nLink: https://lkml.kernel.org/r/20240215103205.2607016-18-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nAcked-by: Mark Rutland <mark.rutland@arm.com>\nAcked-by: Catalin Marinas <catalin.marinas@arm.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Ard Biesheuvel <ardb@kernel.org>\nCc: Barry Song <21cnbao@gmail.com>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: James Morse <james.morse@arm.com>\nCc: John Hubbard <jhubbard@nvidia.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Marc Zyngier <maz@kernel.org>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Will Deacon <will@kernel.org>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: tidy up pte_next_pfn() definition", "commit_message": "Now that the all architecture overrides of pte_next_pfn() have been\nreplaced with pte_advance_pfn(), we can simplify the definition of the\ngeneric pte_next_pfn() macro so that it is unconditionally defined.\n\nLink: https://lkml.kernel.org/r/20240215103205.2607016-7-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nAcked-by: David Hildenbrand <david@redhat.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Ard Biesheuvel <ardb@kernel.org>\nCc: Barry Song <21cnbao@gmail.com>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: James Morse <james.morse@arm.com>\nCc: John Hubbard <jhubbard@nvidia.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Marc Zyngier <maz@kernel.org>\nCc: Mark Rutland <mark.rutland@arm.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Will Deacon <will@kernel.org>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "arm64/mm: convert READ_ONCE(*ptep) to ptep_get(ptep)", "commit_message": "There are a number of places in the arch code that read a pte by using the\nREAD_ONCE() macro.  Refactor these call sites to instead use the\nptep_get() helper, which itself is a READ_ONCE().  Generated code should\nbe the same.\n\nThis will benefit us when we shortly introduce the transparent contpte\nsupport.  In this case, ptep_get() will become more complex so we now have\nall the code abstracted through it.\n\nLink: https://lkml.kernel.org/r/20240215103205.2607016-8-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nTested-by: John Hubbard <jhubbard@nvidia.com>\nAcked-by: Mark Rutland <mark.rutland@arm.com>\nAcked-by: Catalin Marinas <catalin.marinas@arm.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Ard Biesheuvel <ardb@kernel.org>\nCc: Barry Song <21cnbao@gmail.com>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: James Morse <james.morse@arm.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Marc Zyngier <maz@kernel.org>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Will Deacon <will@kernel.org>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: thp: batch-collapse PMD with set_ptes()", "commit_message": "Refactor __split_huge_pmd_locked() so that a present PMD can be collapsed\nto PTEs in a single batch using set_ptes().\n\nThis should improve performance a little bit, but the real motivation is\nto remove the need for the arm64 backend to have to fold the contpte\nentries.  Instead, since the ptes are set as a batch, the contpte blocks\ncan be initially set up pre-folded (once the arm64 contpte support is\nadded in the next few patches).  This leads to noticeable performance\nimprovement during split.\n\nLink: https://lkml.kernel.org/r/20240215103205.2607016-3-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nAcked-by: David Hildenbrand <david@redhat.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Ard Biesheuvel <ardb@kernel.org>\nCc: Barry Song <21cnbao@gmail.com>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: James Morse <james.morse@arm.com>\nCc: John Hubbard <jhubbard@nvidia.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Marc Zyngier <maz@kernel.org>\nCc: Mark Rutland <mark.rutland@arm.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Will Deacon <will@kernel.org>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "arm64/mm: implement pte_batch_hint()", "commit_message": "When core code iterates over a range of ptes and calls ptep_get() for each\nof them, if the range happens to cover contpte mappings, the number of pte\nreads becomes amplified by a factor of the number of PTEs in a contpte\nblock.  This is because for each call to ptep_get(), the implementation\nmust read all of the ptes in the contpte block to which it belongs to\ngather the access and dirty bits.\n\nThis causes a hotspot for fork(), as well as operations that unmap memory\nsuch as munmap(), exit and madvise(MADV_DONTNEED).  Fortunately we can fix\nthis by implementing pte_batch_hint() which allows their iterators to skip\ngetting the contpte tail ptes when gathering the batch of ptes to operate\non.  This results in the number of PTE reads returning to 1 per pte.\n\nLink: https://lkml.kernel.org/r/20240215103205.2607016-17-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nAcked-by: Mark Rutland <mark.rutland@arm.com>\nReviewed-by: David Hildenbrand <david@redhat.com>\nTested-by: John Hubbard <jhubbard@nvidia.com>\nAcked-by: Catalin Marinas <catalin.marinas@arm.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Ard Biesheuvel <ardb@kernel.org>\nCc: Barry Song <21cnbao@gmail.com>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: James Morse <james.morse@arm.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Marc Zyngier <maz@kernel.org>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Will Deacon <will@kernel.org>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "arm64/mm: wire up PTE_CONT for user mappings", "commit_message": "With the ptep API sufficiently refactored, we can now introduce a new\n\"contpte\" API layer, which transparently manages the PTE_CONT bit for user\nmappings.\n\nIn this initial implementation, only suitable batches of PTEs, set via\nset_ptes(), are mapped with the PTE_CONT bit.  Any subsequent modification\nof individual PTEs will cause an \"unfold\" operation to repaint the contpte\nblock as individual PTEs before performing the requested operation. \nWhile, a modification of a single PTE could cause the block of PTEs to\nwhich it belongs to become eligible for \"folding\" into a contpte entry,\n\"folding\" is not performed in this initial implementation due to the costs\nof checking the requirements are met.  Due to this, contpte mappings will\ndegrade back to normal pte mappings over time if/when protections are\nchanged.  This will be solved in a future patch.\n\nSince a contpte block only has a single access and dirty bit, the semantic\nhere changes slightly; when getting a pte (e.g.  ptep_get()) that is part\nof a contpte mapping, the access and dirty information are pulled from the\nblock (so all ptes in the block return the same access/dirty info).  When\nchanging the access/dirty info on a pte (e.g.  ptep_set_access_flags())\nthat is part of a contpte mapping, this change will affect the whole\ncontpte block.  This is works fine in practice since we guarantee that\nonly a single folio is mapped by a contpte block, and the core-mm tracks\naccess/dirty information per folio.\n\nIn order for the public functions, which used to be pure inline, to\ncontinue to be callable by modules, export all the contpte_* symbols that\nare now called by those public inline functions.\n\nThe feature is enabled/disabled with the ARM64_CONTPTE Kconfig parameter\nat build time.  It defaults to enabled as long as its dependency,\nTRANSPARENT_HUGEPAGE is also enabled.  The core-mm depends upon\nTRANSPARENT_HUGEPAGE to be able to allocate large folios, so if its not\nenabled, then there is no chance of meeting the physical contiguity\nrequirement for contpte mappings.\n\nLink: https://lkml.kernel.org/r/20240215103205.2607016-13-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nAcked-by: Ard Biesheuvel <ardb@kernel.org>\nTested-by: John Hubbard <jhubbard@nvidia.com>\nAcked-by: Mark Rutland <mark.rutland@arm.com>\nReviewed-by: Catalin Marinas <catalin.marinas@arm.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Barry Song <21cnbao@gmail.com>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: James Morse <james.morse@arm.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Marc Zyngier <maz@kernel.org>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Will Deacon <will@kernel.org>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "arm64/mm: implement new [get_and_]clear_full_ptes() batch APIs", "commit_message": "Optimize the contpte implementation to fix some of the\nexit/munmap/dontneed performance regression introduced by the initial\ncontpte commit.  Subsequent patches will solve it entirely.\n\nDuring exit(), munmap() or madvise(MADV_DONTNEED), mappings must be\ncleared.  Previously this was done 1 PTE at a time.  But the core-mm\nsupports batched clear via the new [get_and_]clear_full_ptes() APIs.  So\nlet's implement those APIs and for fully covered contpte mappings, we no\nlonger need to unfold the contpte.  This significantly reduces unfolding\noperations, reducing the number of tlbis that must be issued.\n\nLink: https://lkml.kernel.org/r/20240215103205.2607016-15-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nTested-by: John Hubbard <jhubbard@nvidia.com>\nAcked-by: Mark Rutland <mark.rutland@arm.com>\nAcked-by: Catalin Marinas <catalin.marinas@arm.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Ard Biesheuvel <ardb@kernel.org>\nCc: Barry Song <21cnbao@gmail.com>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: James Morse <james.morse@arm.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Marc Zyngier <maz@kernel.org>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Will Deacon <will@kernel.org>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: introduce pte_advance_pfn() and use for pte_next_pfn()", "commit_message": "The goal is to be able to advance a PTE by an arbitrary number of PFNs. \nSo introduce a new API that takes a nr param.  Define the default\nimplementation here and allow for architectures to override. \npte_next_pfn() becomes a wrapper around pte_advance_pfn().\n\nFollow up commits will convert each overriding architecture's\npte_next_pfn() to pte_advance_pfn().\n\nLink: https://lkml.kernel.org/r/20240215103205.2607016-4-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nAcked-by: David Hildenbrand <david@redhat.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Ard Biesheuvel <ardb@kernel.org>\nCc: Barry Song <21cnbao@gmail.com>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: James Morse <james.morse@arm.com>\nCc: John Hubbard <jhubbard@nvidia.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Marc Zyngier <maz@kernel.org>\nCc: Mark Rutland <mark.rutland@arm.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Will Deacon <will@kernel.org>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "arm64/mm: new ptep layer to manage contig bit", "commit_message": "Create a new layer for the in-table PTE manipulation APIs.  For now, The\nexisting API is prefixed with double underscore to become the arch-private\nAPI and the public API is just a simple wrapper that calls the private\nAPI.\n\nThe public API implementation will subsequently be used to transparently\nmanipulate the contiguous bit where appropriate.  But since there are\nalready some contig-aware users (e.g.  hugetlb, kernel mapper), we must\nfirst ensure those users use the private API directly so that the future\ncontig-bit manipulations in the public API do not interfere with those\nexisting uses.\n\nThe following APIs are treated this way:\n\n - ptep_get\n - set_pte\n - set_ptes\n - pte_clear\n - ptep_get_and_clear\n - ptep_test_and_clear_young\n - ptep_clear_flush_young\n - ptep_set_wrprotect\n - ptep_set_access_flags\n\nLink: https://lkml.kernel.org/r/20240215103205.2607016-11-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nTested-by: John Hubbard <jhubbard@nvidia.com>\nAcked-by: Mark Rutland <mark.rutland@arm.com>\nAcked-by: Catalin Marinas <catalin.marinas@arm.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Ard Biesheuvel <ardb@kernel.org>\nCc: Barry Song <21cnbao@gmail.com>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: James Morse <james.morse@arm.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Marc Zyngier <maz@kernel.org>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Will Deacon <will@kernel.org>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: clarify the spec for set_ptes()", "commit_message": "Patch series \"Transparent Contiguous PTEs for User Mappings\", v6.\n\nThis is a series to opportunistically and transparently use contpte\nmappings (set the contiguous bit in ptes) for user memory when those\nmappings meet the requirements.  The change benefits arm64, but there is\nsome (very) minor refactoring for x86 to enable its integration with\ncore-mm.\n\nIt is part of a wider effort to improve performance by allocating and\nmapping variable-sized blocks of memory (folios).  One aim is for the 4K\nkernel to approach the performance of the 16K kernel, but without breaking\ncompatibility and without the associated increase in memory.  Another aim\nis to benefit the 16K and 64K kernels by enabling 2M THP, since this is\nthe contpte size for those kernels.  We have good performance data that\ndemonstrates both aims are being met (see below).\n\nOf course this is only one half of the change.  We require the mapped\nphysical memory to be the correct size and alignment for this to actually\nbe useful (i.e.  64K for 4K pages, or 2M for 16K/64K pages).  Fortunately\nfolios are solving this problem for us.  Filesystems that support it (XFS,\nAFS, EROFS, tmpfs, ...) will allocate large folios up to the PMD size\ntoday, and more filesystems are coming.  And for anonymous memory,\n\"multi-size THP\" is now upstream.\n\n\nPatch Layout\n============\n\nIn this version, I've split the patches to better show each optimization:\n\n  - 1-2:    mm prep: misc code and docs cleanups\n  - 3-6:    mm,arm64,x86 prep: Add pte_advance_pfn() and make pte_next_pfn() a\n            generic wrapper around it\n  - 7-11:   arm64 prep: Refactor ptep helpers into new layer\n  - 12:     functional contpte implementation\n  - 23-18:  various optimizations on top of the contpte implementation\n\n\nTesting\n=======\n\nI've tested this series on both Ampere Altra (bare metal) and Apple M2 (VM):\n  - mm selftests (inc new tests written for multi-size THP); no regressions\n  - Speedometer Java script benchmark in Chromium web browser; no issues\n  - Kernel compilation; no issues\n  - Various tests under high memory pressure with swap enabled; no issues\n\n\nPerformance\n===========\n\nHigh Level Use Cases\n~~~~~~~~~~~~~~~~~~~~\n\nFirst some high level use cases (kernel compilation and speedometer JavaScript\nbenchmarks). These are running on Ampere Altra (I've seen similar improvements\non Android/Pixel 6).\n\nbaseline:                  mm-unstable (mTHP switched off)\nmTHP:                      + enable 16K, 32K, 64K mTHP sizes \"always\"\nmTHP + contpte:            + this series\nmTHP + contpte + exefolio: + patch at [6], which series supports\n\nKernel Compilation with -j8 (negative is faster):\n\n| kernel                    | real-time | kern-time | user-time |\n|---------------------------|-----------|-----------|-----------|\n| baseline                  |      0.0% |      0.0% |      0.0% |\n| mTHP                      |     -5.0% |    -39.1% |     -0.7% |\n| mTHP + contpte            |     -6.0% |    -41.4% |     -1.5% |\n| mTHP + contpte + exefolio |     -7.8% |    -43.1% |     -3.4% |\n\nKernel Compilation with -j80 (negative is faster):\n\n| kernel                    | real-time | kern-time | user-time |\n|---------------------------|-----------|-----------|-----------|\n| baseline                  |      0.0% |      0.0% |      0.0% |\n| mTHP                      |     -5.0% |    -36.6% |     -0.6% |\n| mTHP + contpte            |     -6.1% |    -38.2% |     -1.6% |\n| mTHP + contpte + exefolio |     -7.4% |    -39.2% |     -3.2% |\n\nSpeedometer (positive is faster):\n\n| kernel                    | runs_per_min |\n|:--------------------------|--------------|\n| baseline                  |         0.0% |\n| mTHP                      |         1.5% |\n| mTHP + contpte            |         3.2% |\n| mTHP + contpte + exefolio |         4.5% |\n\n\nMicro Benchmarks\n~~~~~~~~~~~~~~~~\n\nThe following microbenchmarks are intended to demonstrate the performance of\nfork() and munmap() do not regress. I'm showing results for order-0 (4K)\nmappings, and for order-9 (2M) PTE-mapped THP. Thanks to David for sharing his\nbenchmarks.\n\nbaseline:                  mm-unstable + batch zap [7] series\ncontpte-basic:             + patches 0-19; functional contpte implementation\ncontpte-batch:             + patches 20-23; implement new batched APIs\ncontpte-inline:            + patch 24; __always_inline to help compiler\ncontpte-fold:              + patch 25; fold contpte mapping when sensible\n\nPrimary platform is Ampere Altra bare metal. I'm also showing results for M2 VM\n(on top of MacOS) for reference, although experience suggests this might not be\nthe most reliable for performance numbers of this sort:\n\n| FORK           |         order-0        |         order-9        |\n| Ampere Altra   |------------------------|------------------------|\n| (pte-map)      |       mean |     stdev |       mean |     stdev |\n|----------------|------------|-----------|------------|-----------|\n| baseline       |       0.0% |      2.7% |       0.0% |      0.2% |\n| contpte-basic  |       6.3% |      1.4% |    1948.7% |      0.2% |\n| contpte-batch  |       7.6% |      2.0% |      -1.9% |      0.4% |\n| contpte-inline |       3.6% |      1.5% |      -1.0% |      0.2% |\n| contpte-fold   |       4.6% |      2.1% |      -1.8% |      0.2% |\n\n| MUNMAP         |         order-0        |         order-9        |\n| Ampere Altra   |------------------------|------------------------|\n| (pte-map)      |       mean |     stdev |       mean |     stdev |\n|----------------|------------|-----------|------------|-----------|\n| baseline       |       0.0% |      0.5% |       0.0% |      0.3% |\n| contpte-basic  |       1.8% |      0.3% |    1104.8% |      0.1% |\n| contpte-batch  |      -0.3% |      0.4% |       2.7% |      0.1% |\n| contpte-inline |      -0.1% |      0.6% |       0.9% |      0.1% |\n| contpte-fold   |       0.1% |      0.6% |       0.8% |      0.1% |\n\n| FORK           |         order-0        |         order-9        |\n| Apple M2 VM    |------------------------|------------------------|\n| (pte-map)      |       mean |     stdev |       mean |     stdev |\n|----------------|------------|-----------|------------|-----------|\n| baseline       |       0.0% |      1.4% |       0.0% |      0.8% |\n| contpte-basic  |       6.8% |      1.2% |     469.4% |      1.4% |\n| contpte-batch  |      -7.7% |      2.0% |      -8.9% |      0.7% |\n| contpte-inline |      -6.0% |      2.1% |      -6.0% |      2.0% |\n| contpte-fold   |       5.9% |      1.4% |      -6.4% |      1.4% |\n\n| MUNMAP         |         order-0        |         order-9        |\n| Apple M2 VM    |------------------------|------------------------|\n| (pte-map)      |       mean |     stdev |       mean |     stdev |\n|----------------|------------|-----------|------------|-----------|\n| baseline       |       0.0% |      0.6% |       0.0% |      0.4% |\n| contpte-basic  |       1.6% |      0.6% |     233.6% |      0.7% |\n| contpte-batch  |       1.9% |      0.3% |      -3.9% |      0.4% |\n| contpte-inline |       2.2% |      0.8% |      -1.6% |      0.9% |\n| contpte-fold   |       1.5% |      0.7% |      -1.7% |      0.7% |\n\nMisc\n~~~~\n\nJohn Hubbard at Nvidia has indicated dramatic 10x performance improvements\nfor some workloads at [8], when using 64K base page kernel.\n\n[1] https://lore.kernel.org/linux-arm-kernel/20230622144210.2623299-1-ryan.roberts@arm.com/\n[2] https://lore.kernel.org/linux-arm-kernel/20231115163018.1303287-1-ryan.roberts@arm.com/\n[3] https://lore.kernel.org/linux-arm-kernel/20231204105440.61448-1-ryan.roberts@arm.com/\n[4] https://lore.kernel.org/lkml/20231218105100.172635-1-ryan.roberts@arm.com/\n[5] https://lore.kernel.org/linux-mm/633af0a7-0823-424f-b6ef-374d99483f05@arm.com/\n[6] https://lore.kernel.org/lkml/08c16f7d-f3b3-4f22-9acc-da943f647dc3@arm.com/\n[7] https://lore.kernel.org/linux-mm/20240214204435.167852-1-david@redhat.com/\n[8] https://lore.kernel.org/linux-mm/c507308d-bdd4-5f9e-d4ff-e96e4520be85@nvidia.com/\n[9] https://gitlab.arm.com/linux-arm/linux-rr/-/tree/features/granule_perf/contpte-lkml_v6\n\n\n\n\nThis patch (of 18):\n\nset_ptes() spec implies that it can only be used to set a present pte\nbecause it interprets the PFN field to increment it.  However,\nset_pte_at() has been implemented on top of set_ptes() since set_ptes()\nwas introduced, and set_pte_at() allows setting a pte to a not-present\nstate.  So clarify the spec to state that when nr==1, new state of pte may\nbe present or not present.  When nr>1, new state of all ptes must be\npresent.\n\nWhile we are at it, tighten the spec to set requirements around the\ninitial state of ptes; when nr==1 it may be either present or not-present.\nBut when nr>1 all ptes must initially be not-present.  All set_ptes()\ncallsites already conform to this requirement.  Stating it explicitly is\nuseful because it allows for a simplification to the upcoming arm64\ncontpte implementation.\n\nLink: https://lkml.kernel.org/r/20240215103205.2607016-1-ryan.roberts@arm.com\nLink: https://lkml.kernel.org/r/20240215103205.2607016-2-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nAcked-by: David Hildenbrand <david@redhat.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Ard Biesheuvel <ardb@kernel.org>\nCc: Barry Song <21cnbao@gmail.com>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: James Morse <james.morse@arm.com>\nCc: John Hubbard <jhubbard@nvidia.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Marc Zyngier <maz@kernel.org>\nCc: Mark Rutland <mark.rutland@arm.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Will Deacon <will@kernel.org>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "arm64/mm: convert set_pte_at() to set_ptes(..., 1)", "commit_message": "Since set_ptes() was introduced, set_pte_at() has been implemented as a\ngeneric macro around set_ptes(..., 1).  So this change should continue to\ngenerate the same code.  However, making this change prepares us for the\ntransparent contpte support.  It means we can reroute set_ptes() to\n__set_ptes().  Since set_pte_at() is a generic macro, there will be no\nequivalent __set_pte_at() to reroute to.\n\nNote that a couple of calls to set_pte_at() remain in the arch code.  This\nis intentional, since those call sites are acting on behalf of core-mm and\nshould continue to call into the public set_ptes() rather than the\narch-private __set_ptes().\n\nLink: https://lkml.kernel.org/r/20240215103205.2607016-9-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nTested-by: John Hubbard <jhubbard@nvidia.com>\nAcked-by: Mark Rutland <mark.rutland@arm.com>\nAcked-by: Catalin Marinas <catalin.marinas@arm.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Ard Biesheuvel <ardb@kernel.org>\nCc: Barry Song <21cnbao@gmail.com>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: James Morse <james.morse@arm.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Marc Zyngier <maz@kernel.org>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Will Deacon <will@kernel.org>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "arm64/mm: convert pte_next_pfn() to pte_advance_pfn()", "commit_message": "Core-mm needs to be able to advance the pfn by an arbitrary amount, so\noverride the new pte_advance_pfn() API to do so.\n\nLink: https://lkml.kernel.org/r/20240215103205.2607016-5-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nAcked-by: David Hildenbrand <david@redhat.com>\nAcked-by: Mark Rutland <mark.rutland@arm.com>\nAcked-by: Catalin Marinas <catalin.marinas@arm.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Ard Biesheuvel <ardb@kernel.org>\nCc: Barry Song <21cnbao@gmail.com>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: James Morse <james.morse@arm.com>\nCc: John Hubbard <jhubbard@nvidia.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Marc Zyngier <maz@kernel.org>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Will Deacon <will@kernel.org>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "arm64/mm: automatically fold contpte mappings", "commit_message": "There are situations where a change to a single PTE could cause the\ncontpte block in which it resides to become foldable (i.e.  could be\nrepainted with the contiguous bit).  Such situations arise, for example,\nwhen user space temporarily changes protections, via mprotect, for\nindividual pages, such can be the case for certain garbage collectors.\n\nWe would like to detect when such a PTE change occurs.  However this can\nbe expensive due to the amount of checking required.  Therefore only\nperform the checks when an indiviual PTE is modified via mprotect\n(ptep_modify_prot_commit() -> set_pte_at() -> set_ptes(nr=1)) and only\nwhen we are setting the final PTE in a contpte-aligned block.\n\nLink: https://lkml.kernel.org/r/20240215103205.2607016-19-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nAcked-by: Mark Rutland <mark.rutland@arm.com>\nAcked-by: Catalin Marinas <catalin.marinas@arm.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Ard Biesheuvel <ardb@kernel.org>\nCc: Barry Song <21cnbao@gmail.com>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: James Morse <james.morse@arm.com>\nCc: John Hubbard <jhubbard@nvidia.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Marc Zyngier <maz@kernel.org>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Will Deacon <will@kernel.org>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "arm64/mm: convert ptep_clear() to ptep_get_and_clear()", "commit_message": "ptep_clear() is a generic wrapper around the arch-implemented\nptep_get_and_clear().  We are about to convert ptep_get_and_clear() into a\npublic version and private version (__ptep_get_and_clear()) to support the\ntransparent contpte work.  We won't have a private version of ptep_clear()\nso let's convert it to directly call ptep_get_and_clear().\n\nLink: https://lkml.kernel.org/r/20240215103205.2607016-10-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nTested-by: John Hubbard <jhubbard@nvidia.com>\nAcked-by: Mark Rutland <mark.rutland@arm.com>\nAcked-by: Catalin Marinas <catalin.marinas@arm.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Ard Biesheuvel <ardb@kernel.org>\nCc: Barry Song <21cnbao@gmail.com>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: James Morse <james.morse@arm.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Marc Zyngier <maz@kernel.org>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Will Deacon <will@kernel.org>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}]
    }
}

2024-12-03 22:49:55,521 - extractor - DEBUG - deepseek response:
ChatCompletion(id='2aea965d-6031-4677-91b7-9b62eac87cc7', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "proper_nouns": [\n        {\n            "Copy-on-Write (CoW)": "Copy-on-Write (CoW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind CoW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data."\n        },\n        {\n            "PTE (Page Table Entry)": "A Page Table Entry (PTE) is a data structure used in the Linux kernel\'s memory management system to map virtual memory addresses to physical memory addresses. Each PTE contains information about the permissions, status, and physical location of a memory page."\n        },\n        {\n            "TLB (Translation Lookaside Buffer)": "The Translation Lookaside Buffer (TLB) is a cache used by the CPU to store recently used mappings between virtual and physical memory addresses. This cache helps speed up the process of translating virtual addresses to physical addresses, reducing the overhead of accessing the main memory."\n        },\n        {\n            "mTHP (Memory Transparent Huge Pages)": "Memory Transparent Huge Pages (mTHP) is a feature in the Linux kernel that uses larger memory pages to improve memory performance. Traditional memory pages are typically 4KB, but mTHP allows the use of larger pages, such as 2MB or 1GB, depending on the architecture."\n        },\n        {\n            "folio": "In the Linux kernel, a \'folio\' is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages."\n        }\n    ],\n    "background_knowledge": [\n        "The Linux kernel\'s memory management system relies on efficient handling of memory pages, which are the smallest units of memory that can be allocated and managed. The kernel uses page tables to map virtual memory addresses to physical memory addresses, and the TLB helps speed up this translation process.",\n        "Transparent Contiguous PTEs (Page Table Entries) for User Mappings is a feature designed to improve the efficiency of memory management, especially in scenarios where large contiguous blocks of memory are frequently used. This feature leverages the concept of contiguous PTEs to reduce the overhead associated with managing large memory blocks.",\n        "The implementation of this feature involves optimizing the handling of PTEs during operations like fork(), munmap(), and madvise(MADV_DONTNEED). By batching operations on PTEs, the kernel can reduce the number of TLB invalidations and improve overall performance."\n    ],\n    "kernel_changes": [\n        "Before the implementation of Transparent Contiguous PTEs for User Mappings, the Linux kernel handled PTEs individually during operations like fork(), munmap(), and madvise(MADV_DONTNEED). This approach led to inefficiencies, particularly when dealing with large memory blocks, as each PTE had to be processed separately, resulting in a high number of TLB invalidations.",\n        "After the implementation, the kernel was modified to support batch operations on PTEs. This allowed for the efficient handling of contiguous memory blocks, reducing the number of TLB invalidations and improving performance. The new wrprotect_ptes() and clear_full_ptes() APIs were introduced to facilitate these batch operations.",\n        "Additionally, the kernel was updated to support the automatic folding and unfolding of contiguous PTE blocks. This means that when a contiguous block of memory is modified, the kernel can decide whether to fold the block into a single PTE or unfold it into individual PTEs, depending on the context and requirements."\n    ],\n    "comprehensive_understanding": "The Transparent Contiguous PTEs for User Mappings feature is a significant enhancement to the Linux kernel\'s memory management system. By optimizing the handling of contiguous memory blocks, this feature reduces the overhead associated with operations like fork(), munmap(), and madvise(MADV_DONTNEED). The introduction of batch operations on PTEs and the ability to automatically fold and unfold contiguous blocks of memory significantly improve performance and resource utilization. This feature is particularly beneficial in scenarios where large memory blocks are frequently used, such as in high-performance computing and large-scale data processing applications."\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733237345, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=958, prompt_tokens=14688, total_tokens=15646, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=1472, prompt_cache_miss_tokens=13216))
2024-12-03 22:49:55,521 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个命名实体抽取 (NER) 任务的专家，同时也是 linux 内核的专家。给你一个 linux 内核特性的描述 (feature_description)、对应的若干 commit 的标题和提交信息 (commits)、该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)、为了让开发者理解该特性所需的与这个特性直接相关的背景知识 (background_knowledge)、该特性被实现前后，linux 内核发生的变化 (kernel_changes)、对该特性的综合理解 (comprehensive_understanding)。你的任务是：根据这些信息提取出在 linux 内核中涉及到的和这个特性相关的概念实体 (concepts)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}],
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": [
                "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
            ]
        },
        "output": {
            "concepts": ["Copy-on-Write (CoW)", "mTHP", "folio", "memory reclamation", "memory availability", "anti-fragmentation"]
        }
    ],
    "input": {
        "feature_description": "Transparent Contiguous PTEs for User Mappings",
        "commits": [{"commit_subject": "arm64/mm: implement new wrprotect_ptes() batch API", "commit_message": "Optimize the contpte implementation to fix some of the fork performance\nregression introduced by the initial contpte commit.  Subsequent patches\nwill solve it entirely.\n\nDuring fork(), any private memory in the parent must be write-protected. \nPreviously this was done 1 PTE at a time.  But the core-mm supports\nbatched wrprotect via the new wrprotect_ptes() API.  So let's implement\nthat API and for fully covered contpte mappings, we no longer need to\nunfold the contpte.  This has 2 benefits:\n\n  - reduced unfolding, reduces the number of tlbis that must be issued.\n  - The memory remains contpte-mapped (\"folded\") in the parent, so it\n    continues to benefit from the more efficient use of the TLB after\n    the fork.\n\nThe optimization to wrprotect a whole contpte block without unfolding is\npossible thanks to the tightening of the Arm ARM in respect to the\ndefinition and behaviour when 'Misprogramming the Contiguous bit'.  See\nsection D21194 at https://developer.arm.com/documentation/102105/ja-07/\n\nLink: https://lkml.kernel.org/r/20240215103205.2607016-14-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nTested-by: John Hubbard <jhubbard@nvidia.com>\nAcked-by: Mark Rutland <mark.rutland@arm.com>\nAcked-by: Catalin Marinas <catalin.marinas@arm.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Ard Biesheuvel <ardb@kernel.org>\nCc: Barry Song <21cnbao@gmail.com>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: James Morse <james.morse@arm.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Marc Zyngier <maz@kernel.org>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Will Deacon <will@kernel.org>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: add pte_batch_hint() to reduce scanning in folio_pte_batch()", "commit_message": "Some architectures (e.g.  arm64) can tell from looking at a pte, if some\nfollow-on ptes also map contiguous physical memory with the same pgprot. \n(for arm64, these are contpte mappings).\n\nTake advantage of this knowledge to optimize folio_pte_batch() so that it\ncan skip these ptes when scanning to create a batch.  By default, if an\narch does not opt-in, folio_pte_batch() returns a compile-time 1, so the\nchanges are optimized out and the behaviour is as before.\n\narm64 will opt-in to providing this hint in the next patch, which will\ngreatly reduce the cost of ptep_get() when scanning a range of contptes.\n\nLink: https://lkml.kernel.org/r/20240215103205.2607016-16-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nAcked-by: David Hildenbrand <david@redhat.com>\nTested-by: John Hubbard <jhubbard@nvidia.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Ard Biesheuvel <ardb@kernel.org>\nCc: Barry Song <21cnbao@gmail.com>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: James Morse <james.morse@arm.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Marc Zyngier <maz@kernel.org>\nCc: Mark Rutland <mark.rutland@arm.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Will Deacon <will@kernel.org>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "arm64/mm: __always_inline to improve fork() perf", "commit_message": "As set_ptes() and wrprotect_ptes() become a bit more complex, the compiler\nmay choose not to inline them.  But this is critical for fork()\nperformance.  So mark the functions, along with contpte_try_unfold() which\nis called by them, as __always_inline.  This is worth ~1% on the fork()\nmicrobenchmark with order-0 folios (the common case).\n\nLink: https://lkml.kernel.org/r/20240215103205.2607016-18-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nAcked-by: Mark Rutland <mark.rutland@arm.com>\nAcked-by: Catalin Marinas <catalin.marinas@arm.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Ard Biesheuvel <ardb@kernel.org>\nCc: Barry Song <21cnbao@gmail.com>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: James Morse <james.morse@arm.com>\nCc: John Hubbard <jhubbard@nvidia.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Marc Zyngier <maz@kernel.org>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Will Deacon <will@kernel.org>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: tidy up pte_next_pfn() definition", "commit_message": "Now that the all architecture overrides of pte_next_pfn() have been\nreplaced with pte_advance_pfn(), we can simplify the definition of the\ngeneric pte_next_pfn() macro so that it is unconditionally defined.\n\nLink: https://lkml.kernel.org/r/20240215103205.2607016-7-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nAcked-by: David Hildenbrand <david@redhat.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Ard Biesheuvel <ardb@kernel.org>\nCc: Barry Song <21cnbao@gmail.com>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: James Morse <james.morse@arm.com>\nCc: John Hubbard <jhubbard@nvidia.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Marc Zyngier <maz@kernel.org>\nCc: Mark Rutland <mark.rutland@arm.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Will Deacon <will@kernel.org>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "arm64/mm: convert READ_ONCE(*ptep) to ptep_get(ptep)", "commit_message": "There are a number of places in the arch code that read a pte by using the\nREAD_ONCE() macro.  Refactor these call sites to instead use the\nptep_get() helper, which itself is a READ_ONCE().  Generated code should\nbe the same.\n\nThis will benefit us when we shortly introduce the transparent contpte\nsupport.  In this case, ptep_get() will become more complex so we now have\nall the code abstracted through it.\n\nLink: https://lkml.kernel.org/r/20240215103205.2607016-8-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nTested-by: John Hubbard <jhubbard@nvidia.com>\nAcked-by: Mark Rutland <mark.rutland@arm.com>\nAcked-by: Catalin Marinas <catalin.marinas@arm.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Ard Biesheuvel <ardb@kernel.org>\nCc: Barry Song <21cnbao@gmail.com>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: James Morse <james.morse@arm.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Marc Zyngier <maz@kernel.org>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Will Deacon <will@kernel.org>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: thp: batch-collapse PMD with set_ptes()", "commit_message": "Refactor __split_huge_pmd_locked() so that a present PMD can be collapsed\nto PTEs in a single batch using set_ptes().\n\nThis should improve performance a little bit, but the real motivation is\nto remove the need for the arm64 backend to have to fold the contpte\nentries.  Instead, since the ptes are set as a batch, the contpte blocks\ncan be initially set up pre-folded (once the arm64 contpte support is\nadded in the next few patches).  This leads to noticeable performance\nimprovement during split.\n\nLink: https://lkml.kernel.org/r/20240215103205.2607016-3-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nAcked-by: David Hildenbrand <david@redhat.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Ard Biesheuvel <ardb@kernel.org>\nCc: Barry Song <21cnbao@gmail.com>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: James Morse <james.morse@arm.com>\nCc: John Hubbard <jhubbard@nvidia.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Marc Zyngier <maz@kernel.org>\nCc: Mark Rutland <mark.rutland@arm.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Will Deacon <will@kernel.org>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "arm64/mm: implement pte_batch_hint()", "commit_message": "When core code iterates over a range of ptes and calls ptep_get() for each\nof them, if the range happens to cover contpte mappings, the number of pte\nreads becomes amplified by a factor of the number of PTEs in a contpte\nblock.  This is because for each call to ptep_get(), the implementation\nmust read all of the ptes in the contpte block to which it belongs to\ngather the access and dirty bits.\n\nThis causes a hotspot for fork(), as well as operations that unmap memory\nsuch as munmap(), exit and madvise(MADV_DONTNEED).  Fortunately we can fix\nthis by implementing pte_batch_hint() which allows their iterators to skip\ngetting the contpte tail ptes when gathering the batch of ptes to operate\non.  This results in the number of PTE reads returning to 1 per pte.\n\nLink: https://lkml.kernel.org/r/20240215103205.2607016-17-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nAcked-by: Mark Rutland <mark.rutland@arm.com>\nReviewed-by: David Hildenbrand <david@redhat.com>\nTested-by: John Hubbard <jhubbard@nvidia.com>\nAcked-by: Catalin Marinas <catalin.marinas@arm.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Ard Biesheuvel <ardb@kernel.org>\nCc: Barry Song <21cnbao@gmail.com>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: James Morse <james.morse@arm.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Marc Zyngier <maz@kernel.org>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Will Deacon <will@kernel.org>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "arm64/mm: wire up PTE_CONT for user mappings", "commit_message": "With the ptep API sufficiently refactored, we can now introduce a new\n\"contpte\" API layer, which transparently manages the PTE_CONT bit for user\nmappings.\n\nIn this initial implementation, only suitable batches of PTEs, set via\nset_ptes(), are mapped with the PTE_CONT bit.  Any subsequent modification\nof individual PTEs will cause an \"unfold\" operation to repaint the contpte\nblock as individual PTEs before performing the requested operation. \nWhile, a modification of a single PTE could cause the block of PTEs to\nwhich it belongs to become eligible for \"folding\" into a contpte entry,\n\"folding\" is not performed in this initial implementation due to the costs\nof checking the requirements are met.  Due to this, contpte mappings will\ndegrade back to normal pte mappings over time if/when protections are\nchanged.  This will be solved in a future patch.\n\nSince a contpte block only has a single access and dirty bit, the semantic\nhere changes slightly; when getting a pte (e.g.  ptep_get()) that is part\nof a contpte mapping, the access and dirty information are pulled from the\nblock (so all ptes in the block return the same access/dirty info).  When\nchanging the access/dirty info on a pte (e.g.  ptep_set_access_flags())\nthat is part of a contpte mapping, this change will affect the whole\ncontpte block.  This is works fine in practice since we guarantee that\nonly a single folio is mapped by a contpte block, and the core-mm tracks\naccess/dirty information per folio.\n\nIn order for the public functions, which used to be pure inline, to\ncontinue to be callable by modules, export all the contpte_* symbols that\nare now called by those public inline functions.\n\nThe feature is enabled/disabled with the ARM64_CONTPTE Kconfig parameter\nat build time.  It defaults to enabled as long as its dependency,\nTRANSPARENT_HUGEPAGE is also enabled.  The core-mm depends upon\nTRANSPARENT_HUGEPAGE to be able to allocate large folios, so if its not\nenabled, then there is no chance of meeting the physical contiguity\nrequirement for contpte mappings.\n\nLink: https://lkml.kernel.org/r/20240215103205.2607016-13-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nAcked-by: Ard Biesheuvel <ardb@kernel.org>\nTested-by: John Hubbard <jhubbard@nvidia.com>\nAcked-by: Mark Rutland <mark.rutland@arm.com>\nReviewed-by: Catalin Marinas <catalin.marinas@arm.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Barry Song <21cnbao@gmail.com>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: James Morse <james.morse@arm.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Marc Zyngier <maz@kernel.org>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Will Deacon <will@kernel.org>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "arm64/mm: implement new [get_and_]clear_full_ptes() batch APIs", "commit_message": "Optimize the contpte implementation to fix some of the\nexit/munmap/dontneed performance regression introduced by the initial\ncontpte commit.  Subsequent patches will solve it entirely.\n\nDuring exit(), munmap() or madvise(MADV_DONTNEED), mappings must be\ncleared.  Previously this was done 1 PTE at a time.  But the core-mm\nsupports batched clear via the new [get_and_]clear_full_ptes() APIs.  So\nlet's implement those APIs and for fully covered contpte mappings, we no\nlonger need to unfold the contpte.  This significantly reduces unfolding\noperations, reducing the number of tlbis that must be issued.\n\nLink: https://lkml.kernel.org/r/20240215103205.2607016-15-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nTested-by: John Hubbard <jhubbard@nvidia.com>\nAcked-by: Mark Rutland <mark.rutland@arm.com>\nAcked-by: Catalin Marinas <catalin.marinas@arm.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Ard Biesheuvel <ardb@kernel.org>\nCc: Barry Song <21cnbao@gmail.com>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: James Morse <james.morse@arm.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Marc Zyngier <maz@kernel.org>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Will Deacon <will@kernel.org>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: introduce pte_advance_pfn() and use for pte_next_pfn()", "commit_message": "The goal is to be able to advance a PTE by an arbitrary number of PFNs. \nSo introduce a new API that takes a nr param.  Define the default\nimplementation here and allow for architectures to override. \npte_next_pfn() becomes a wrapper around pte_advance_pfn().\n\nFollow up commits will convert each overriding architecture's\npte_next_pfn() to pte_advance_pfn().\n\nLink: https://lkml.kernel.org/r/20240215103205.2607016-4-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nAcked-by: David Hildenbrand <david@redhat.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Ard Biesheuvel <ardb@kernel.org>\nCc: Barry Song <21cnbao@gmail.com>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: James Morse <james.morse@arm.com>\nCc: John Hubbard <jhubbard@nvidia.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Marc Zyngier <maz@kernel.org>\nCc: Mark Rutland <mark.rutland@arm.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Will Deacon <will@kernel.org>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "arm64/mm: new ptep layer to manage contig bit", "commit_message": "Create a new layer for the in-table PTE manipulation APIs.  For now, The\nexisting API is prefixed with double underscore to become the arch-private\nAPI and the public API is just a simple wrapper that calls the private\nAPI.\n\nThe public API implementation will subsequently be used to transparently\nmanipulate the contiguous bit where appropriate.  But since there are\nalready some contig-aware users (e.g.  hugetlb, kernel mapper), we must\nfirst ensure those users use the private API directly so that the future\ncontig-bit manipulations in the public API do not interfere with those\nexisting uses.\n\nThe following APIs are treated this way:\n\n - ptep_get\n - set_pte\n - set_ptes\n - pte_clear\n - ptep_get_and_clear\n - ptep_test_and_clear_young\n - ptep_clear_flush_young\n - ptep_set_wrprotect\n - ptep_set_access_flags\n\nLink: https://lkml.kernel.org/r/20240215103205.2607016-11-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nTested-by: John Hubbard <jhubbard@nvidia.com>\nAcked-by: Mark Rutland <mark.rutland@arm.com>\nAcked-by: Catalin Marinas <catalin.marinas@arm.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Ard Biesheuvel <ardb@kernel.org>\nCc: Barry Song <21cnbao@gmail.com>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: James Morse <james.morse@arm.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Marc Zyngier <maz@kernel.org>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Will Deacon <will@kernel.org>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm: clarify the spec for set_ptes()", "commit_message": "Patch series \"Transparent Contiguous PTEs for User Mappings\", v6.\n\nThis is a series to opportunistically and transparently use contpte\nmappings (set the contiguous bit in ptes) for user memory when those\nmappings meet the requirements.  The change benefits arm64, but there is\nsome (very) minor refactoring for x86 to enable its integration with\ncore-mm.\n\nIt is part of a wider effort to improve performance by allocating and\nmapping variable-sized blocks of memory (folios).  One aim is for the 4K\nkernel to approach the performance of the 16K kernel, but without breaking\ncompatibility and without the associated increase in memory.  Another aim\nis to benefit the 16K and 64K kernels by enabling 2M THP, since this is\nthe contpte size for those kernels.  We have good performance data that\ndemonstrates both aims are being met (see below).\n\nOf course this is only one half of the change.  We require the mapped\nphysical memory to be the correct size and alignment for this to actually\nbe useful (i.e.  64K for 4K pages, or 2M for 16K/64K pages).  Fortunately\nfolios are solving this problem for us.  Filesystems that support it (XFS,\nAFS, EROFS, tmpfs, ...) will allocate large folios up to the PMD size\ntoday, and more filesystems are coming.  And for anonymous memory,\n\"multi-size THP\" is now upstream.\n\n\nPatch Layout\n============\n\nIn this version, I've split the patches to better show each optimization:\n\n  - 1-2:    mm prep: misc code and docs cleanups\n  - 3-6:    mm,arm64,x86 prep: Add pte_advance_pfn() and make pte_next_pfn() a\n            generic wrapper around it\n  - 7-11:   arm64 prep: Refactor ptep helpers into new layer\n  - 12:     functional contpte implementation\n  - 23-18:  various optimizations on top of the contpte implementation\n\n\nTesting\n=======\n\nI've tested this series on both Ampere Altra (bare metal) and Apple M2 (VM):\n  - mm selftests (inc new tests written for multi-size THP); no regressions\n  - Speedometer Java script benchmark in Chromium web browser; no issues\n  - Kernel compilation; no issues\n  - Various tests under high memory pressure with swap enabled; no issues\n\n\nPerformance\n===========\n\nHigh Level Use Cases\n~~~~~~~~~~~~~~~~~~~~\n\nFirst some high level use cases (kernel compilation and speedometer JavaScript\nbenchmarks). These are running on Ampere Altra (I've seen similar improvements\non Android/Pixel 6).\n\nbaseline:                  mm-unstable (mTHP switched off)\nmTHP:                      + enable 16K, 32K, 64K mTHP sizes \"always\"\nmTHP + contpte:            + this series\nmTHP + contpte + exefolio: + patch at [6], which series supports\n\nKernel Compilation with -j8 (negative is faster):\n\n| kernel                    | real-time | kern-time | user-time |\n|---------------------------|-----------|-----------|-----------|\n| baseline                  |      0.0% |      0.0% |      0.0% |\n| mTHP                      |     -5.0% |    -39.1% |     -0.7% |\n| mTHP + contpte            |     -6.0% |    -41.4% |     -1.5% |\n| mTHP + contpte + exefolio |     -7.8% |    -43.1% |     -3.4% |\n\nKernel Compilation with -j80 (negative is faster):\n\n| kernel                    | real-time | kern-time | user-time |\n|---------------------------|-----------|-----------|-----------|\n| baseline                  |      0.0% |      0.0% |      0.0% |\n| mTHP                      |     -5.0% |    -36.6% |     -0.6% |\n| mTHP + contpte            |     -6.1% |    -38.2% |     -1.6% |\n| mTHP + contpte + exefolio |     -7.4% |    -39.2% |     -3.2% |\n\nSpeedometer (positive is faster):\n\n| kernel                    | runs_per_min |\n|:--------------------------|--------------|\n| baseline                  |         0.0% |\n| mTHP                      |         1.5% |\n| mTHP + contpte            |         3.2% |\n| mTHP + contpte + exefolio |         4.5% |\n\n\nMicro Benchmarks\n~~~~~~~~~~~~~~~~\n\nThe following microbenchmarks are intended to demonstrate the performance of\nfork() and munmap() do not regress. I'm showing results for order-0 (4K)\nmappings, and for order-9 (2M) PTE-mapped THP. Thanks to David for sharing his\nbenchmarks.\n\nbaseline:                  mm-unstable + batch zap [7] series\ncontpte-basic:             + patches 0-19; functional contpte implementation\ncontpte-batch:             + patches 20-23; implement new batched APIs\ncontpte-inline:            + patch 24; __always_inline to help compiler\ncontpte-fold:              + patch 25; fold contpte mapping when sensible\n\nPrimary platform is Ampere Altra bare metal. I'm also showing results for M2 VM\n(on top of MacOS) for reference, although experience suggests this might not be\nthe most reliable for performance numbers of this sort:\n\n| FORK           |         order-0        |         order-9        |\n| Ampere Altra   |------------------------|------------------------|\n| (pte-map)      |       mean |     stdev |       mean |     stdev |\n|----------------|------------|-----------|------------|-----------|\n| baseline       |       0.0% |      2.7% |       0.0% |      0.2% |\n| contpte-basic  |       6.3% |      1.4% |    1948.7% |      0.2% |\n| contpte-batch  |       7.6% |      2.0% |      -1.9% |      0.4% |\n| contpte-inline |       3.6% |      1.5% |      -1.0% |      0.2% |\n| contpte-fold   |       4.6% |      2.1% |      -1.8% |      0.2% |\n\n| MUNMAP         |         order-0        |         order-9        |\n| Ampere Altra   |------------------------|------------------------|\n| (pte-map)      |       mean |     stdev |       mean |     stdev |\n|----------------|------------|-----------|------------|-----------|\n| baseline       |       0.0% |      0.5% |       0.0% |      0.3% |\n| contpte-basic  |       1.8% |      0.3% |    1104.8% |      0.1% |\n| contpte-batch  |      -0.3% |      0.4% |       2.7% |      0.1% |\n| contpte-inline |      -0.1% |      0.6% |       0.9% |      0.1% |\n| contpte-fold   |       0.1% |      0.6% |       0.8% |      0.1% |\n\n| FORK           |         order-0        |         order-9        |\n| Apple M2 VM    |------------------------|------------------------|\n| (pte-map)      |       mean |     stdev |       mean |     stdev |\n|----------------|------------|-----------|------------|-----------|\n| baseline       |       0.0% |      1.4% |       0.0% |      0.8% |\n| contpte-basic  |       6.8% |      1.2% |     469.4% |      1.4% |\n| contpte-batch  |      -7.7% |      2.0% |      -8.9% |      0.7% |\n| contpte-inline |      -6.0% |      2.1% |      -6.0% |      2.0% |\n| contpte-fold   |       5.9% |      1.4% |      -6.4% |      1.4% |\n\n| MUNMAP         |         order-0        |         order-9        |\n| Apple M2 VM    |------------------------|------------------------|\n| (pte-map)      |       mean |     stdev |       mean |     stdev |\n|----------------|------------|-----------|------------|-----------|\n| baseline       |       0.0% |      0.6% |       0.0% |      0.4% |\n| contpte-basic  |       1.6% |      0.6% |     233.6% |      0.7% |\n| contpte-batch  |       1.9% |      0.3% |      -3.9% |      0.4% |\n| contpte-inline |       2.2% |      0.8% |      -1.6% |      0.9% |\n| contpte-fold   |       1.5% |      0.7% |      -1.7% |      0.7% |\n\nMisc\n~~~~\n\nJohn Hubbard at Nvidia has indicated dramatic 10x performance improvements\nfor some workloads at [8], when using 64K base page kernel.\n\n[1] https://lore.kernel.org/linux-arm-kernel/20230622144210.2623299-1-ryan.roberts@arm.com/\n[2] https://lore.kernel.org/linux-arm-kernel/20231115163018.1303287-1-ryan.roberts@arm.com/\n[3] https://lore.kernel.org/linux-arm-kernel/20231204105440.61448-1-ryan.roberts@arm.com/\n[4] https://lore.kernel.org/lkml/20231218105100.172635-1-ryan.roberts@arm.com/\n[5] https://lore.kernel.org/linux-mm/633af0a7-0823-424f-b6ef-374d99483f05@arm.com/\n[6] https://lore.kernel.org/lkml/08c16f7d-f3b3-4f22-9acc-da943f647dc3@arm.com/\n[7] https://lore.kernel.org/linux-mm/20240214204435.167852-1-david@redhat.com/\n[8] https://lore.kernel.org/linux-mm/c507308d-bdd4-5f9e-d4ff-e96e4520be85@nvidia.com/\n[9] https://gitlab.arm.com/linux-arm/linux-rr/-/tree/features/granule_perf/contpte-lkml_v6\n\n\n\n\nThis patch (of 18):\n\nset_ptes() spec implies that it can only be used to set a present pte\nbecause it interprets the PFN field to increment it.  However,\nset_pte_at() has been implemented on top of set_ptes() since set_ptes()\nwas introduced, and set_pte_at() allows setting a pte to a not-present\nstate.  So clarify the spec to state that when nr==1, new state of pte may\nbe present or not present.  When nr>1, new state of all ptes must be\npresent.\n\nWhile we are at it, tighten the spec to set requirements around the\ninitial state of ptes; when nr==1 it may be either present or not-present.\nBut when nr>1 all ptes must initially be not-present.  All set_ptes()\ncallsites already conform to this requirement.  Stating it explicitly is\nuseful because it allows for a simplification to the upcoming arm64\ncontpte implementation.\n\nLink: https://lkml.kernel.org/r/20240215103205.2607016-1-ryan.roberts@arm.com\nLink: https://lkml.kernel.org/r/20240215103205.2607016-2-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nAcked-by: David Hildenbrand <david@redhat.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Ard Biesheuvel <ardb@kernel.org>\nCc: Barry Song <21cnbao@gmail.com>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: James Morse <james.morse@arm.com>\nCc: John Hubbard <jhubbard@nvidia.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Marc Zyngier <maz@kernel.org>\nCc: Mark Rutland <mark.rutland@arm.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Will Deacon <will@kernel.org>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "arm64/mm: convert set_pte_at() to set_ptes(..., 1)", "commit_message": "Since set_ptes() was introduced, set_pte_at() has been implemented as a\ngeneric macro around set_ptes(..., 1).  So this change should continue to\ngenerate the same code.  However, making this change prepares us for the\ntransparent contpte support.  It means we can reroute set_ptes() to\n__set_ptes().  Since set_pte_at() is a generic macro, there will be no\nequivalent __set_pte_at() to reroute to.\n\nNote that a couple of calls to set_pte_at() remain in the arch code.  This\nis intentional, since those call sites are acting on behalf of core-mm and\nshould continue to call into the public set_ptes() rather than the\narch-private __set_ptes().\n\nLink: https://lkml.kernel.org/r/20240215103205.2607016-9-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nTested-by: John Hubbard <jhubbard@nvidia.com>\nAcked-by: Mark Rutland <mark.rutland@arm.com>\nAcked-by: Catalin Marinas <catalin.marinas@arm.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Ard Biesheuvel <ardb@kernel.org>\nCc: Barry Song <21cnbao@gmail.com>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: James Morse <james.morse@arm.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Marc Zyngier <maz@kernel.org>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Will Deacon <will@kernel.org>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "arm64/mm: convert pte_next_pfn() to pte_advance_pfn()", "commit_message": "Core-mm needs to be able to advance the pfn by an arbitrary amount, so\noverride the new pte_advance_pfn() API to do so.\n\nLink: https://lkml.kernel.org/r/20240215103205.2607016-5-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nAcked-by: David Hildenbrand <david@redhat.com>\nAcked-by: Mark Rutland <mark.rutland@arm.com>\nAcked-by: Catalin Marinas <catalin.marinas@arm.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Ard Biesheuvel <ardb@kernel.org>\nCc: Barry Song <21cnbao@gmail.com>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: James Morse <james.morse@arm.com>\nCc: John Hubbard <jhubbard@nvidia.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Marc Zyngier <maz@kernel.org>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Will Deacon <will@kernel.org>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "arm64/mm: automatically fold contpte mappings", "commit_message": "There are situations where a change to a single PTE could cause the\ncontpte block in which it resides to become foldable (i.e.  could be\nrepainted with the contiguous bit).  Such situations arise, for example,\nwhen user space temporarily changes protections, via mprotect, for\nindividual pages, such can be the case for certain garbage collectors.\n\nWe would like to detect when such a PTE change occurs.  However this can\nbe expensive due to the amount of checking required.  Therefore only\nperform the checks when an indiviual PTE is modified via mprotect\n(ptep_modify_prot_commit() -> set_pte_at() -> set_ptes(nr=1)) and only\nwhen we are setting the final PTE in a contpte-aligned block.\n\nLink: https://lkml.kernel.org/r/20240215103205.2607016-19-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nAcked-by: Mark Rutland <mark.rutland@arm.com>\nAcked-by: Catalin Marinas <catalin.marinas@arm.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Ard Biesheuvel <ardb@kernel.org>\nCc: Barry Song <21cnbao@gmail.com>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: James Morse <james.morse@arm.com>\nCc: John Hubbard <jhubbard@nvidia.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Marc Zyngier <maz@kernel.org>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Will Deacon <will@kernel.org>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "arm64/mm: convert ptep_clear() to ptep_get_and_clear()", "commit_message": "ptep_clear() is a generic wrapper around the arch-implemented\nptep_get_and_clear().  We are about to convert ptep_get_and_clear() into a\npublic version and private version (__ptep_get_and_clear()) to support the\ntransparent contpte work.  We won't have a private version of ptep_clear()\nso let's convert it to directly call ptep_get_and_clear().\n\nLink: https://lkml.kernel.org/r/20240215103205.2607016-10-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nTested-by: John Hubbard <jhubbard@nvidia.com>\nAcked-by: Mark Rutland <mark.rutland@arm.com>\nAcked-by: Catalin Marinas <catalin.marinas@arm.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Ard Biesheuvel <ardb@kernel.org>\nCc: Barry Song <21cnbao@gmail.com>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: James Morse <james.morse@arm.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Marc Zyngier <maz@kernel.org>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Will Deacon <will@kernel.org>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}],
        "proper_nouns": [{"Copy-on-Write (CoW)": "Copy-on-Write (CoW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind CoW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data."}, {"PTE (Page Table Entry)": "A Page Table Entry (PTE) is a data structure used in the Linux kernel's memory management system to map virtual memory addresses to physical memory addresses. Each PTE contains information about the permissions, status, and physical location of a memory page."}, {"TLB (Translation Lookaside Buffer)": "The Translation Lookaside Buffer (TLB) is a cache used by the CPU to store recently used mappings between virtual and physical memory addresses. This cache helps speed up the process of translating virtual addresses to physical addresses, reducing the overhead of accessing the main memory."}, {"mTHP (Memory Transparent Huge Pages)": "Memory Transparent Huge Pages (mTHP) is a feature in the Linux kernel that uses larger memory pages to improve memory performance. Traditional memory pages are typically 4KB, but mTHP allows the use of larger pages, such as 2MB or 1GB, depending on the architecture."}, {"folio": "In the Linux kernel, a 'folio' is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages."}],
        "background_knowledge": ["The Linux kernel's memory management system relies on efficient handling of memory pages, which are the smallest units of memory that can be allocated and managed. The kernel uses page tables to map virtual memory addresses to physical memory addresses, and the TLB helps speed up this translation process.", "Transparent Contiguous PTEs (Page Table Entries) for User Mappings is a feature designed to improve the efficiency of memory management, especially in scenarios where large contiguous blocks of memory are frequently used. This feature leverages the concept of contiguous PTEs to reduce the overhead associated with managing large memory blocks.", "The implementation of this feature involves optimizing the handling of PTEs during operations like fork(), munmap(), and madvise(MADV_DONTNEED). By batching operations on PTEs, the kernel can reduce the number of TLB invalidations and improve overall performance."],
        "kernel_changes": ["Before the implementation of Transparent Contiguous PTEs for User Mappings, the Linux kernel handled PTEs individually during operations like fork(), munmap(), and madvise(MADV_DONTNEED). This approach led to inefficiencies, particularly when dealing with large memory blocks, as each PTE had to be processed separately, resulting in a high number of TLB invalidations.", "After the implementation, the kernel was modified to support batch operations on PTEs. This allowed for the efficient handling of contiguous memory blocks, reducing the number of TLB invalidations and improving performance. The new wrprotect_ptes() and clear_full_ptes() APIs were introduced to facilitate these batch operations.", "Additionally, the kernel was updated to support the automatic folding and unfolding of contiguous PTE blocks. This means that when a contiguous block of memory is modified, the kernel can decide whether to fold the block into a single PTE or unfold it into individual PTEs, depending on the context and requirements."],
        "comprehensive_understanding": "The Transparent Contiguous PTEs for User Mappings feature is a significant enhancement to the Linux kernel's memory management system. By optimizing the handling of contiguous memory blocks, this feature reduces the overhead associated with operations like fork(), munmap(), and madvise(MADV_DONTNEED). The introduction of batch operations on PTEs and the ability to automatically fold and unfold contiguous blocks of memory significantly improve performance and resource utilization. This feature is particularly beneficial in scenarios where large memory blocks are frequently used, such as in high-performance computing and large-scale data processing applications."
    }
}

2024-12-03 22:50:04,259 - extractor - DEBUG - deepseek response:
ChatCompletion(id='c6f61bfd-dd41-417c-9318-066dd2729e86', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "concepts": ["Copy-on-Write (CoW)", "PTE (Page Table Entry)", "TLB (Translation Lookaside Buffer)", "mTHP (Memory Transparent Huge Pages)", "folio", "memory management", "batch operations", "TLB invalidations", "fork()", "munmap()", "madvise(MADV_DONTNEED)", "wrprotect_ptes()", "clear_full_ptes()", "contiguous PTEs", "memory performance", "memory blocks", "virtual memory", "physical memory", "memory pages", "page tables", "memory reclamation", "memory fragmentation", "memory availability", "anti-fragmentation"]\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733237395, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=156, prompt_tokens=15638, total_tokens=15794, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=1536, prompt_cache_miss_tokens=14102))
2024-12-03 22:50:04,260 - extractor - DEBUG - feature_extracted of feature_id=33210:
{
    "feature_id": 33210,
    "feature_description": "Transparent Contiguous PTEs for User Mappings",
    "version": "6.9",
    "commits": [
        {
            "commit_subject": "arm64/mm: implement new wrprotect_ptes() batch API",
            "commit_message": "Optimize the contpte implementation to fix some of the fork performance\nregression introduced by the initial contpte commit.  Subsequent patches\nwill solve it entirely.\n\nDuring fork(), any private memory in the parent must be write-protected. \nPreviously this was done 1 PTE at a time.  But the core-mm supports\nbatched wrprotect via the new wrprotect_ptes() API.  So let's implement\nthat API and for fully covered contpte mappings, we no longer need to\nunfold the contpte.  This has 2 benefits:\n\n  - reduced unfolding, reduces the number of tlbis that must be issued.\n  - The memory remains contpte-mapped (\"folded\") in the parent, so it\n    continues to benefit from the more efficient use of the TLB after\n    the fork.\n\nThe optimization to wrprotect a whole contpte block without unfolding is\npossible thanks to the tightening of the Arm ARM in respect to the\ndefinition and behaviour when 'Misprogramming the Contiguous bit'.  See\nsection D21194 at https://developer.arm.com/documentation/102105/ja-07/\n\nLink: https://lkml.kernel.org/r/20240215103205.2607016-14-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nTested-by: John Hubbard <jhubbard@nvidia.com>\nAcked-by: Mark Rutland <mark.rutland@arm.com>\nAcked-by: Catalin Marinas <catalin.marinas@arm.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Ard Biesheuvel <ardb@kernel.org>\nCc: Barry Song <21cnbao@gmail.com>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: James Morse <james.morse@arm.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Marc Zyngier <maz@kernel.org>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Will Deacon <will@kernel.org>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm: add pte_batch_hint() to reduce scanning in folio_pte_batch()",
            "commit_message": "Some architectures (e.g.  arm64) can tell from looking at a pte, if some\nfollow-on ptes also map contiguous physical memory with the same pgprot. \n(for arm64, these are contpte mappings).\n\nTake advantage of this knowledge to optimize folio_pte_batch() so that it\ncan skip these ptes when scanning to create a batch.  By default, if an\narch does not opt-in, folio_pte_batch() returns a compile-time 1, so the\nchanges are optimized out and the behaviour is as before.\n\narm64 will opt-in to providing this hint in the next patch, which will\ngreatly reduce the cost of ptep_get() when scanning a range of contptes.\n\nLink: https://lkml.kernel.org/r/20240215103205.2607016-16-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nAcked-by: David Hildenbrand <david@redhat.com>\nTested-by: John Hubbard <jhubbard@nvidia.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Ard Biesheuvel <ardb@kernel.org>\nCc: Barry Song <21cnbao@gmail.com>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: James Morse <james.morse@arm.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Marc Zyngier <maz@kernel.org>\nCc: Mark Rutland <mark.rutland@arm.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Will Deacon <will@kernel.org>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "arm64/mm: __always_inline to improve fork() perf",
            "commit_message": "As set_ptes() and wrprotect_ptes() become a bit more complex, the compiler\nmay choose not to inline them.  But this is critical for fork()\nperformance.  So mark the functions, along with contpte_try_unfold() which\nis called by them, as __always_inline.  This is worth ~1% on the fork()\nmicrobenchmark with order-0 folios (the common case).\n\nLink: https://lkml.kernel.org/r/20240215103205.2607016-18-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nAcked-by: Mark Rutland <mark.rutland@arm.com>\nAcked-by: Catalin Marinas <catalin.marinas@arm.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Ard Biesheuvel <ardb@kernel.org>\nCc: Barry Song <21cnbao@gmail.com>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: James Morse <james.morse@arm.com>\nCc: John Hubbard <jhubbard@nvidia.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Marc Zyngier <maz@kernel.org>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Will Deacon <will@kernel.org>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm: tidy up pte_next_pfn() definition",
            "commit_message": "Now that the all architecture overrides of pte_next_pfn() have been\nreplaced with pte_advance_pfn(), we can simplify the definition of the\ngeneric pte_next_pfn() macro so that it is unconditionally defined.\n\nLink: https://lkml.kernel.org/r/20240215103205.2607016-7-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nAcked-by: David Hildenbrand <david@redhat.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Ard Biesheuvel <ardb@kernel.org>\nCc: Barry Song <21cnbao@gmail.com>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: James Morse <james.morse@arm.com>\nCc: John Hubbard <jhubbard@nvidia.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Marc Zyngier <maz@kernel.org>\nCc: Mark Rutland <mark.rutland@arm.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Will Deacon <will@kernel.org>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "arm64/mm: convert READ_ONCE(*ptep) to ptep_get(ptep)",
            "commit_message": "There are a number of places in the arch code that read a pte by using the\nREAD_ONCE() macro.  Refactor these call sites to instead use the\nptep_get() helper, which itself is a READ_ONCE().  Generated code should\nbe the same.\n\nThis will benefit us when we shortly introduce the transparent contpte\nsupport.  In this case, ptep_get() will become more complex so we now have\nall the code abstracted through it.\n\nLink: https://lkml.kernel.org/r/20240215103205.2607016-8-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nTested-by: John Hubbard <jhubbard@nvidia.com>\nAcked-by: Mark Rutland <mark.rutland@arm.com>\nAcked-by: Catalin Marinas <catalin.marinas@arm.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Ard Biesheuvel <ardb@kernel.org>\nCc: Barry Song <21cnbao@gmail.com>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: James Morse <james.morse@arm.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Marc Zyngier <maz@kernel.org>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Will Deacon <will@kernel.org>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm: thp: batch-collapse PMD with set_ptes()",
            "commit_message": "Refactor __split_huge_pmd_locked() so that a present PMD can be collapsed\nto PTEs in a single batch using set_ptes().\n\nThis should improve performance a little bit, but the real motivation is\nto remove the need for the arm64 backend to have to fold the contpte\nentries.  Instead, since the ptes are set as a batch, the contpte blocks\ncan be initially set up pre-folded (once the arm64 contpte support is\nadded in the next few patches).  This leads to noticeable performance\nimprovement during split.\n\nLink: https://lkml.kernel.org/r/20240215103205.2607016-3-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nAcked-by: David Hildenbrand <david@redhat.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Ard Biesheuvel <ardb@kernel.org>\nCc: Barry Song <21cnbao@gmail.com>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: James Morse <james.morse@arm.com>\nCc: John Hubbard <jhubbard@nvidia.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Marc Zyngier <maz@kernel.org>\nCc: Mark Rutland <mark.rutland@arm.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Will Deacon <will@kernel.org>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "arm64/mm: implement pte_batch_hint()",
            "commit_message": "When core code iterates over a range of ptes and calls ptep_get() for each\nof them, if the range happens to cover contpte mappings, the number of pte\nreads becomes amplified by a factor of the number of PTEs in a contpte\nblock.  This is because for each call to ptep_get(), the implementation\nmust read all of the ptes in the contpte block to which it belongs to\ngather the access and dirty bits.\n\nThis causes a hotspot for fork(), as well as operations that unmap memory\nsuch as munmap(), exit and madvise(MADV_DONTNEED).  Fortunately we can fix\nthis by implementing pte_batch_hint() which allows their iterators to skip\ngetting the contpte tail ptes when gathering the batch of ptes to operate\non.  This results in the number of PTE reads returning to 1 per pte.\n\nLink: https://lkml.kernel.org/r/20240215103205.2607016-17-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nAcked-by: Mark Rutland <mark.rutland@arm.com>\nReviewed-by: David Hildenbrand <david@redhat.com>\nTested-by: John Hubbard <jhubbard@nvidia.com>\nAcked-by: Catalin Marinas <catalin.marinas@arm.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Ard Biesheuvel <ardb@kernel.org>\nCc: Barry Song <21cnbao@gmail.com>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: James Morse <james.morse@arm.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Marc Zyngier <maz@kernel.org>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Will Deacon <will@kernel.org>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "arm64/mm: wire up PTE_CONT for user mappings",
            "commit_message": "With the ptep API sufficiently refactored, we can now introduce a new\n\"contpte\" API layer, which transparently manages the PTE_CONT bit for user\nmappings.\n\nIn this initial implementation, only suitable batches of PTEs, set via\nset_ptes(), are mapped with the PTE_CONT bit.  Any subsequent modification\nof individual PTEs will cause an \"unfold\" operation to repaint the contpte\nblock as individual PTEs before performing the requested operation. \nWhile, a modification of a single PTE could cause the block of PTEs to\nwhich it belongs to become eligible for \"folding\" into a contpte entry,\n\"folding\" is not performed in this initial implementation due to the costs\nof checking the requirements are met.  Due to this, contpte mappings will\ndegrade back to normal pte mappings over time if/when protections are\nchanged.  This will be solved in a future patch.\n\nSince a contpte block only has a single access and dirty bit, the semantic\nhere changes slightly; when getting a pte (e.g.  ptep_get()) that is part\nof a contpte mapping, the access and dirty information are pulled from the\nblock (so all ptes in the block return the same access/dirty info).  When\nchanging the access/dirty info on a pte (e.g.  ptep_set_access_flags())\nthat is part of a contpte mapping, this change will affect the whole\ncontpte block.  This is works fine in practice since we guarantee that\nonly a single folio is mapped by a contpte block, and the core-mm tracks\naccess/dirty information per folio.\n\nIn order for the public functions, which used to be pure inline, to\ncontinue to be callable by modules, export all the contpte_* symbols that\nare now called by those public inline functions.\n\nThe feature is enabled/disabled with the ARM64_CONTPTE Kconfig parameter\nat build time.  It defaults to enabled as long as its dependency,\nTRANSPARENT_HUGEPAGE is also enabled.  The core-mm depends upon\nTRANSPARENT_HUGEPAGE to be able to allocate large folios, so if its not\nenabled, then there is no chance of meeting the physical contiguity\nrequirement for contpte mappings.\n\nLink: https://lkml.kernel.org/r/20240215103205.2607016-13-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nAcked-by: Ard Biesheuvel <ardb@kernel.org>\nTested-by: John Hubbard <jhubbard@nvidia.com>\nAcked-by: Mark Rutland <mark.rutland@arm.com>\nReviewed-by: Catalin Marinas <catalin.marinas@arm.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Barry Song <21cnbao@gmail.com>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: James Morse <james.morse@arm.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Marc Zyngier <maz@kernel.org>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Will Deacon <will@kernel.org>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "arm64/mm: implement new [get_and_]clear_full_ptes() batch APIs",
            "commit_message": "Optimize the contpte implementation to fix some of the\nexit/munmap/dontneed performance regression introduced by the initial\ncontpte commit.  Subsequent patches will solve it entirely.\n\nDuring exit(), munmap() or madvise(MADV_DONTNEED), mappings must be\ncleared.  Previously this was done 1 PTE at a time.  But the core-mm\nsupports batched clear via the new [get_and_]clear_full_ptes() APIs.  So\nlet's implement those APIs and for fully covered contpte mappings, we no\nlonger need to unfold the contpte.  This significantly reduces unfolding\noperations, reducing the number of tlbis that must be issued.\n\nLink: https://lkml.kernel.org/r/20240215103205.2607016-15-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nTested-by: John Hubbard <jhubbard@nvidia.com>\nAcked-by: Mark Rutland <mark.rutland@arm.com>\nAcked-by: Catalin Marinas <catalin.marinas@arm.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Ard Biesheuvel <ardb@kernel.org>\nCc: Barry Song <21cnbao@gmail.com>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: James Morse <james.morse@arm.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Marc Zyngier <maz@kernel.org>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Will Deacon <will@kernel.org>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm: introduce pte_advance_pfn() and use for pte_next_pfn()",
            "commit_message": "The goal is to be able to advance a PTE by an arbitrary number of PFNs. \nSo introduce a new API that takes a nr param.  Define the default\nimplementation here and allow for architectures to override. \npte_next_pfn() becomes a wrapper around pte_advance_pfn().\n\nFollow up commits will convert each overriding architecture's\npte_next_pfn() to pte_advance_pfn().\n\nLink: https://lkml.kernel.org/r/20240215103205.2607016-4-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nAcked-by: David Hildenbrand <david@redhat.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Ard Biesheuvel <ardb@kernel.org>\nCc: Barry Song <21cnbao@gmail.com>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: James Morse <james.morse@arm.com>\nCc: John Hubbard <jhubbard@nvidia.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Marc Zyngier <maz@kernel.org>\nCc: Mark Rutland <mark.rutland@arm.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Will Deacon <will@kernel.org>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "arm64/mm: new ptep layer to manage contig bit",
            "commit_message": "Create a new layer for the in-table PTE manipulation APIs.  For now, The\nexisting API is prefixed with double underscore to become the arch-private\nAPI and the public API is just a simple wrapper that calls the private\nAPI.\n\nThe public API implementation will subsequently be used to transparently\nmanipulate the contiguous bit where appropriate.  But since there are\nalready some contig-aware users (e.g.  hugetlb, kernel mapper), we must\nfirst ensure those users use the private API directly so that the future\ncontig-bit manipulations in the public API do not interfere with those\nexisting uses.\n\nThe following APIs are treated this way:\n\n - ptep_get\n - set_pte\n - set_ptes\n - pte_clear\n - ptep_get_and_clear\n - ptep_test_and_clear_young\n - ptep_clear_flush_young\n - ptep_set_wrprotect\n - ptep_set_access_flags\n\nLink: https://lkml.kernel.org/r/20240215103205.2607016-11-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nTested-by: John Hubbard <jhubbard@nvidia.com>\nAcked-by: Mark Rutland <mark.rutland@arm.com>\nAcked-by: Catalin Marinas <catalin.marinas@arm.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Ard Biesheuvel <ardb@kernel.org>\nCc: Barry Song <21cnbao@gmail.com>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: James Morse <james.morse@arm.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Marc Zyngier <maz@kernel.org>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Will Deacon <will@kernel.org>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm: clarify the spec for set_ptes()",
            "commit_message": "Patch series \"Transparent Contiguous PTEs for User Mappings\", v6.\n\nThis is a series to opportunistically and transparently use contpte\nmappings (set the contiguous bit in ptes) for user memory when those\nmappings meet the requirements.  The change benefits arm64, but there is\nsome (very) minor refactoring for x86 to enable its integration with\ncore-mm.\n\nIt is part of a wider effort to improve performance by allocating and\nmapping variable-sized blocks of memory (folios).  One aim is for the 4K\nkernel to approach the performance of the 16K kernel, but without breaking\ncompatibility and without the associated increase in memory.  Another aim\nis to benefit the 16K and 64K kernels by enabling 2M THP, since this is\nthe contpte size for those kernels.  We have good performance data that\ndemonstrates both aims are being met (see below).\n\nOf course this is only one half of the change.  We require the mapped\nphysical memory to be the correct size and alignment for this to actually\nbe useful (i.e.  64K for 4K pages, or 2M for 16K/64K pages).  Fortunately\nfolios are solving this problem for us.  Filesystems that support it (XFS,\nAFS, EROFS, tmpfs, ...) will allocate large folios up to the PMD size\ntoday, and more filesystems are coming.  And for anonymous memory,\n\"multi-size THP\" is now upstream.\n\n\nPatch Layout\n============\n\nIn this version, I've split the patches to better show each optimization:\n\n  - 1-2:    mm prep: misc code and docs cleanups\n  - 3-6:    mm,arm64,x86 prep: Add pte_advance_pfn() and make pte_next_pfn() a\n            generic wrapper around it\n  - 7-11:   arm64 prep: Refactor ptep helpers into new layer\n  - 12:     functional contpte implementation\n  - 23-18:  various optimizations on top of the contpte implementation\n\n\nTesting\n=======\n\nI've tested this series on both Ampere Altra (bare metal) and Apple M2 (VM):\n  - mm selftests (inc new tests written for multi-size THP); no regressions\n  - Speedometer Java script benchmark in Chromium web browser; no issues\n  - Kernel compilation; no issues\n  - Various tests under high memory pressure with swap enabled; no issues\n\n\nPerformance\n===========\n\nHigh Level Use Cases\n~~~~~~~~~~~~~~~~~~~~\n\nFirst some high level use cases (kernel compilation and speedometer JavaScript\nbenchmarks). These are running on Ampere Altra (I've seen similar improvements\non Android/Pixel 6).\n\nbaseline:                  mm-unstable (mTHP switched off)\nmTHP:                      + enable 16K, 32K, 64K mTHP sizes \"always\"\nmTHP + contpte:            + this series\nmTHP + contpte + exefolio: + patch at [6], which series supports\n\nKernel Compilation with -j8 (negative is faster):\n\n| kernel                    | real-time | kern-time | user-time |\n|---------------------------|-----------|-----------|-----------|\n| baseline                  |      0.0% |      0.0% |      0.0% |\n| mTHP                      |     -5.0% |    -39.1% |     -0.7% |\n| mTHP + contpte            |     -6.0% |    -41.4% |     -1.5% |\n| mTHP + contpte + exefolio |     -7.8% |    -43.1% |     -3.4% |\n\nKernel Compilation with -j80 (negative is faster):\n\n| kernel                    | real-time | kern-time | user-time |\n|---------------------------|-----------|-----------|-----------|\n| baseline                  |      0.0% |      0.0% |      0.0% |\n| mTHP                      |     -5.0% |    -36.6% |     -0.6% |\n| mTHP + contpte            |     -6.1% |    -38.2% |     -1.6% |\n| mTHP + contpte + exefolio |     -7.4% |    -39.2% |     -3.2% |\n\nSpeedometer (positive is faster):\n\n| kernel                    | runs_per_min |\n|:--------------------------|--------------|\n| baseline                  |         0.0% |\n| mTHP                      |         1.5% |\n| mTHP + contpte            |         3.2% |\n| mTHP + contpte + exefolio |         4.5% |\n\n\nMicro Benchmarks\n~~~~~~~~~~~~~~~~\n\nThe following microbenchmarks are intended to demonstrate the performance of\nfork() and munmap() do not regress. I'm showing results for order-0 (4K)\nmappings, and for order-9 (2M) PTE-mapped THP. Thanks to David for sharing his\nbenchmarks.\n\nbaseline:                  mm-unstable + batch zap [7] series\ncontpte-basic:             + patches 0-19; functional contpte implementation\ncontpte-batch:             + patches 20-23; implement new batched APIs\ncontpte-inline:            + patch 24; __always_inline to help compiler\ncontpte-fold:              + patch 25; fold contpte mapping when sensible\n\nPrimary platform is Ampere Altra bare metal. I'm also showing results for M2 VM\n(on top of MacOS) for reference, although experience suggests this might not be\nthe most reliable for performance numbers of this sort:\n\n| FORK           |         order-0        |         order-9        |\n| Ampere Altra   |------------------------|------------------------|\n| (pte-map)      |       mean |     stdev |       mean |     stdev |\n|----------------|------------|-----------|------------|-----------|\n| baseline       |       0.0% |      2.7% |       0.0% |      0.2% |\n| contpte-basic  |       6.3% |      1.4% |    1948.7% |      0.2% |\n| contpte-batch  |       7.6% |      2.0% |      -1.9% |      0.4% |\n| contpte-inline |       3.6% |      1.5% |      -1.0% |      0.2% |\n| contpte-fold   |       4.6% |      2.1% |      -1.8% |      0.2% |\n\n| MUNMAP         |         order-0        |         order-9        |\n| Ampere Altra   |------------------------|------------------------|\n| (pte-map)      |       mean |     stdev |       mean |     stdev |\n|----------------|------------|-----------|------------|-----------|\n| baseline       |       0.0% |      0.5% |       0.0% |      0.3% |\n| contpte-basic  |       1.8% |      0.3% |    1104.8% |      0.1% |\n| contpte-batch  |      -0.3% |      0.4% |       2.7% |      0.1% |\n| contpte-inline |      -0.1% |      0.6% |       0.9% |      0.1% |\n| contpte-fold   |       0.1% |      0.6% |       0.8% |      0.1% |\n\n| FORK           |         order-0        |         order-9        |\n| Apple M2 VM    |------------------------|------------------------|\n| (pte-map)      |       mean |     stdev |       mean |     stdev |\n|----------------|------------|-----------|------------|-----------|\n| baseline       |       0.0% |      1.4% |       0.0% |      0.8% |\n| contpte-basic  |       6.8% |      1.2% |     469.4% |      1.4% |\n| contpte-batch  |      -7.7% |      2.0% |      -8.9% |      0.7% |\n| contpte-inline |      -6.0% |      2.1% |      -6.0% |      2.0% |\n| contpte-fold   |       5.9% |      1.4% |      -6.4% |      1.4% |\n\n| MUNMAP         |         order-0        |         order-9        |\n| Apple M2 VM    |------------------------|------------------------|\n| (pte-map)      |       mean |     stdev |       mean |     stdev |\n|----------------|------------|-----------|------------|-----------|\n| baseline       |       0.0% |      0.6% |       0.0% |      0.4% |\n| contpte-basic  |       1.6% |      0.6% |     233.6% |      0.7% |\n| contpte-batch  |       1.9% |      0.3% |      -3.9% |      0.4% |\n| contpte-inline |       2.2% |      0.8% |      -1.6% |      0.9% |\n| contpte-fold   |       1.5% |      0.7% |      -1.7% |      0.7% |\n\nMisc\n~~~~\n\nJohn Hubbard at Nvidia has indicated dramatic 10x performance improvements\nfor some workloads at [8], when using 64K base page kernel.\n\n[1] https://lore.kernel.org/linux-arm-kernel/20230622144210.2623299-1-ryan.roberts@arm.com/\n[2] https://lore.kernel.org/linux-arm-kernel/20231115163018.1303287-1-ryan.roberts@arm.com/\n[3] https://lore.kernel.org/linux-arm-kernel/20231204105440.61448-1-ryan.roberts@arm.com/\n[4] https://lore.kernel.org/lkml/20231218105100.172635-1-ryan.roberts@arm.com/\n[5] https://lore.kernel.org/linux-mm/633af0a7-0823-424f-b6ef-374d99483f05@arm.com/\n[6] https://lore.kernel.org/lkml/08c16f7d-f3b3-4f22-9acc-da943f647dc3@arm.com/\n[7] https://lore.kernel.org/linux-mm/20240214204435.167852-1-david@redhat.com/\n[8] https://lore.kernel.org/linux-mm/c507308d-bdd4-5f9e-d4ff-e96e4520be85@nvidia.com/\n[9] https://gitlab.arm.com/linux-arm/linux-rr/-/tree/features/granule_perf/contpte-lkml_v6\n\n\n\n\nThis patch (of 18):\n\nset_ptes() spec implies that it can only be used to set a present pte\nbecause it interprets the PFN field to increment it.  However,\nset_pte_at() has been implemented on top of set_ptes() since set_ptes()\nwas introduced, and set_pte_at() allows setting a pte to a not-present\nstate.  So clarify the spec to state that when nr==1, new state of pte may\nbe present or not present.  When nr>1, new state of all ptes must be\npresent.\n\nWhile we are at it, tighten the spec to set requirements around the\ninitial state of ptes; when nr==1 it may be either present or not-present.\nBut when nr>1 all ptes must initially be not-present.  All set_ptes()\ncallsites already conform to this requirement.  Stating it explicitly is\nuseful because it allows for a simplification to the upcoming arm64\ncontpte implementation.\n\nLink: https://lkml.kernel.org/r/20240215103205.2607016-1-ryan.roberts@arm.com\nLink: https://lkml.kernel.org/r/20240215103205.2607016-2-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nAcked-by: David Hildenbrand <david@redhat.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Ard Biesheuvel <ardb@kernel.org>\nCc: Barry Song <21cnbao@gmail.com>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: James Morse <james.morse@arm.com>\nCc: John Hubbard <jhubbard@nvidia.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Marc Zyngier <maz@kernel.org>\nCc: Mark Rutland <mark.rutland@arm.com>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Will Deacon <will@kernel.org>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "arm64/mm: convert set_pte_at() to set_ptes(..., 1)",
            "commit_message": "Since set_ptes() was introduced, set_pte_at() has been implemented as a\ngeneric macro around set_ptes(..., 1).  So this change should continue to\ngenerate the same code.  However, making this change prepares us for the\ntransparent contpte support.  It means we can reroute set_ptes() to\n__set_ptes().  Since set_pte_at() is a generic macro, there will be no\nequivalent __set_pte_at() to reroute to.\n\nNote that a couple of calls to set_pte_at() remain in the arch code.  This\nis intentional, since those call sites are acting on behalf of core-mm and\nshould continue to call into the public set_ptes() rather than the\narch-private __set_ptes().\n\nLink: https://lkml.kernel.org/r/20240215103205.2607016-9-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nTested-by: John Hubbard <jhubbard@nvidia.com>\nAcked-by: Mark Rutland <mark.rutland@arm.com>\nAcked-by: Catalin Marinas <catalin.marinas@arm.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Ard Biesheuvel <ardb@kernel.org>\nCc: Barry Song <21cnbao@gmail.com>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: James Morse <james.morse@arm.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Marc Zyngier <maz@kernel.org>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Will Deacon <will@kernel.org>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "arm64/mm: convert pte_next_pfn() to pte_advance_pfn()",
            "commit_message": "Core-mm needs to be able to advance the pfn by an arbitrary amount, so\noverride the new pte_advance_pfn() API to do so.\n\nLink: https://lkml.kernel.org/r/20240215103205.2607016-5-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nAcked-by: David Hildenbrand <david@redhat.com>\nAcked-by: Mark Rutland <mark.rutland@arm.com>\nAcked-by: Catalin Marinas <catalin.marinas@arm.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Ard Biesheuvel <ardb@kernel.org>\nCc: Barry Song <21cnbao@gmail.com>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: James Morse <james.morse@arm.com>\nCc: John Hubbard <jhubbard@nvidia.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Marc Zyngier <maz@kernel.org>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Will Deacon <will@kernel.org>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "arm64/mm: automatically fold contpte mappings",
            "commit_message": "There are situations where a change to a single PTE could cause the\ncontpte block in which it resides to become foldable (i.e.  could be\nrepainted with the contiguous bit).  Such situations arise, for example,\nwhen user space temporarily changes protections, via mprotect, for\nindividual pages, such can be the case for certain garbage collectors.\n\nWe would like to detect when such a PTE change occurs.  However this can\nbe expensive due to the amount of checking required.  Therefore only\nperform the checks when an indiviual PTE is modified via mprotect\n(ptep_modify_prot_commit() -> set_pte_at() -> set_ptes(nr=1)) and only\nwhen we are setting the final PTE in a contpte-aligned block.\n\nLink: https://lkml.kernel.org/r/20240215103205.2607016-19-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nAcked-by: Mark Rutland <mark.rutland@arm.com>\nAcked-by: Catalin Marinas <catalin.marinas@arm.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Ard Biesheuvel <ardb@kernel.org>\nCc: Barry Song <21cnbao@gmail.com>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: James Morse <james.morse@arm.com>\nCc: John Hubbard <jhubbard@nvidia.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Marc Zyngier <maz@kernel.org>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Will Deacon <will@kernel.org>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "arm64/mm: convert ptep_clear() to ptep_get_and_clear()",
            "commit_message": "ptep_clear() is a generic wrapper around the arch-implemented\nptep_get_and_clear().  We are about to convert ptep_get_and_clear() into a\npublic version and private version (__ptep_get_and_clear()) to support the\ntransparent contpte work.  We won't have a private version of ptep_clear()\nso let's convert it to directly call ptep_get_and_clear().\n\nLink: https://lkml.kernel.org/r/20240215103205.2607016-10-ryan.roberts@arm.com\nSigned-off-by: Ryan Roberts <ryan.roberts@arm.com>\nTested-by: John Hubbard <jhubbard@nvidia.com>\nAcked-by: Mark Rutland <mark.rutland@arm.com>\nAcked-by: Catalin Marinas <catalin.marinas@arm.com>\nCc: Alistair Popple <apopple@nvidia.com>\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Ard Biesheuvel <ardb@kernel.org>\nCc: Barry Song <21cnbao@gmail.com>\nCc: Borislav Petkov (AMD) <bp@alien8.de>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: David Hildenbrand <david@redhat.com>\nCc: \"H. Peter Anvin\" <hpa@zytor.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: James Morse <james.morse@arm.com>\nCc: Kefeng Wang <wangkefeng.wang@huawei.com>\nCc: Marc Zyngier <maz@kernel.org>\nCc: Matthew Wilcox (Oracle) <willy@infradead.org>\nCc: Thomas Gleixner <tglx@linutronix.de>\nCc: Will Deacon <will@kernel.org>\nCc: Yang Shi <shy828301@gmail.com>\nCc: Zi Yan <ziy@nvidia.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        }
    ],
    "proper_nouns": [
        {
            "Copy-on-Write (CoW)": "Copy-on-Write (CoW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind CoW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data."
        },
        {
            "PTE (Page Table Entry)": "A Page Table Entry (PTE) is a data structure used in the Linux kernel's memory management system to map virtual memory addresses to physical memory addresses. Each PTE contains information about the permissions, status, and physical location of a memory page."
        },
        {
            "TLB (Translation Lookaside Buffer)": "The Translation Lookaside Buffer (TLB) is a cache used by the CPU to store recently used mappings between virtual and physical memory addresses. This cache helps speed up the process of translating virtual addresses to physical addresses, reducing the overhead of accessing the main memory."
        },
        {
            "mTHP (Memory Transparent Huge Pages)": "Memory Transparent Huge Pages (mTHP) is a feature in the Linux kernel that uses larger memory pages to improve memory performance. Traditional memory pages are typically 4KB, but mTHP allows the use of larger pages, such as 2MB or 1GB, depending on the architecture."
        },
        {
            "folio": "In the Linux kernel, a 'folio' is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages."
        }
    ],
    "background_knowledge": [
        "The Linux kernel's memory management system relies on efficient handling of memory pages, which are the smallest units of memory that can be allocated and managed. The kernel uses page tables to map virtual memory addresses to physical memory addresses, and the TLB helps speed up this translation process.",
        "Transparent Contiguous PTEs (Page Table Entries) for User Mappings is a feature designed to improve the efficiency of memory management, especially in scenarios where large contiguous blocks of memory are frequently used. This feature leverages the concept of contiguous PTEs to reduce the overhead associated with managing large memory blocks.",
        "The implementation of this feature involves optimizing the handling of PTEs during operations like fork(), munmap(), and madvise(MADV_DONTNEED). By batching operations on PTEs, the kernel can reduce the number of TLB invalidations and improve overall performance."
    ],
    "kernel_changes": [
        "Before the implementation of Transparent Contiguous PTEs for User Mappings, the Linux kernel handled PTEs individually during operations like fork(), munmap(), and madvise(MADV_DONTNEED). This approach led to inefficiencies, particularly when dealing with large memory blocks, as each PTE had to be processed separately, resulting in a high number of TLB invalidations.",
        "After the implementation, the kernel was modified to support batch operations on PTEs. This allowed for the efficient handling of contiguous memory blocks, reducing the number of TLB invalidations and improving performance. The new wrprotect_ptes() and clear_full_ptes() APIs were introduced to facilitate these batch operations.",
        "Additionally, the kernel was updated to support the automatic folding and unfolding of contiguous PTE blocks. This means that when a contiguous block of memory is modified, the kernel can decide whether to fold the block into a single PTE or unfold it into individual PTEs, depending on the context and requirements."
    ],
    "comprehensive_understanding": "The Transparent Contiguous PTEs for User Mappings feature is a significant enhancement to the Linux kernel's memory management system. By optimizing the handling of contiguous memory blocks, this feature reduces the overhead associated with operations like fork(), munmap(), and madvise(MADV_DONTNEED). The introduction of batch operations on PTEs and the ability to automatically fold and unfold contiguous blocks of memory significantly improve performance and resource utilization. This feature is particularly beneficial in scenarios where large memory blocks are frequently used, such as in high-performance computing and large-scale data processing applications.",
    "concepts": [
        "Copy-on-Write (CoW)",
        "PTE (Page Table Entry)",
        "TLB (Translation Lookaside Buffer)",
        "mTHP (Memory Transparent Huge Pages)",
        "folio",
        "memory management",
        "batch operations",
        "TLB invalidations",
        "fork()",
        "munmap()",
        "madvise(MADV_DONTNEED)",
        "wrprotect_ptes()",
        "clear_full_ptes()",
        "contiguous PTEs",
        "memory performance",
        "memory blocks",
        "virtual memory",
        "physical memory",
        "memory pages",
        "page tables",
        "memory reclamation",
        "memory fragmentation",
        "memory availability",
        "anti-fragmentation"
    ]
}
2024-12-03 22:50:04,261 - extractor - INFO - ---------Processing feature 31/815---------
2024-12-03 22:50:42,042 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个 linux 内核的专家。给你一个 linux 内核特性的描述以及其对应的若干 commit 的标题和提交信息，你的任务是为这个特性写一个多方面的扩写版描述。你的扩写需包含以下几个方面：1. 该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)；2. 为了让开发者理解该特性，需要哪些具体的，与这个特性直接相关的背景知识 (background_knowledge)；3. 该特性被实现前后，linux 内核发生了怎样的变化 (kernel_changes)；4. 你对该特性的综合理解 (comprehensive_understanding)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}]
        },
        "output": {
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
        }
    ],
    "input": {
        "feature_description": "(FEATURED) weighted interleave mempolicy and sysfs extension",
        "commits": [{"commit_subject": "mm/mempolicy: refactor a read-once mechanism into a function for re-use", "commit_message": "Move the use of barrier() to force policy->nodemask onto the stack into a\nfunction `read_once_policy_nodemask` so that it may be re-used.\n\nLink: https://lkml.kernel.org/r/20240202170238.90004-3-gregory.price@memverge.com\nSigned-off-by: Gregory Price <gregory.price@memverge.com>\nSuggested-by: \"Huang, Ying\" <ying.huang@intel.com>\nReviewed-by: \"Huang, Ying\" <ying.huang@intel.com>\nCc: Dan Williams <dan.j.williams@intel.com>\nCc: Hasan Al Maruf <Hasan.Maruf@amd.com>\nCc: Honggyu Kim <honggyu.kim@sk.com>\nCc: Hyeongtak Ji <hyeongtak.ji@sk.com>\nCc: Johannes Weiner <hannes@cmpxchg.org>\nCc: Jonathan Corbet <corbet@lwn.net>\nCc: Michal Hocko <mhocko@kernel.org>\nCc: Rakie Kim <rakie.kim@sk.com>\nCc: Ravi Jonnalagadda <ravis.opensrc@micron.com>\nCc: Srinivasulu Thanneeru <sthanneeru.opensrc@micron.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/mempolicy: introduce MPOL_WEIGHTED_INTERLEAVE for weighted interleaving", "commit_message": "When a system has multiple NUMA nodes and it becomes bandwidth hungry,\nusing the current MPOL_INTERLEAVE could be an wise option.\n\nHowever, if those NUMA nodes consist of different types of memory such as\nsocket-attached DRAM and CXL/PCIe attached DRAM, the round-robin based\ninterleave policy does not optimally distribute data to make use of their\ndifferent bandwidth characteristics.\n\nInstead, interleave is more effective when the allocation policy follows\neach NUMA nodes' bandwidth weight rather than a simple 1:1 distribution.\n\nThis patch introduces a new memory policy, MPOL_WEIGHTED_INTERLEAVE,\nenabling weighted interleave between NUMA nodes.  Weighted interleave\nallows for proportional distribution of memory across multiple numa nodes,\npreferably apportioned to match the bandwidth of each node.\n\nFor example, if a system has 1 CPU node (0), and 2 memory nodes (0,1),\nwith bandwidth of (100GB/s, 50GB/s) respectively, the appropriate weight\ndistribution is (2:1).\n\nWeights for each node can be assigned via the new sysfs extension:\n/sys/kernel/mm/mempolicy/weighted_interleave/\n\nFor now, the default value of all nodes will be `1`, which matches the\nbehavior of standard 1:1 round-robin interleave.  An extension will be\nadded in the future to allow default values to be registered at kernel and\ndevice bringup time.\n\nThe policy allocates a number of pages equal to the set weights.  For\nexample, if the weights are (2,1), then 2 pages will be allocated on node0\nfor every 1 page allocated on node1.\n\nThe new flag MPOL_WEIGHTED_INTERLEAVE can be used in set_mempolicy(2)\nand mbind(2).\n\nSome high level notes about the pieces of weighted interleave:\n\ncurrent->il_prev:\n    Tracks the node previously allocated from.\n\ncurrent->il_weight:\n    The active weight of the current node (current->il_prev)\n    When this reaches 0, current->il_prev is set to the next node\n    and current->il_weight is set to the next weight.\n\nweighted_interleave_nodes:\n    Counts the number of allocations as they occur, and applies the\n    weight for the current node.  When the weight reaches 0, switch\n    to the next node.  Operates only on task->mempolicy.\n\nweighted_interleave_nid:\n    Gets the total weight of the nodemask as well as each individual\n    node weight, then calculates the node based on the given index.\n    Operates on VMA policies.\n\nbulk_array_weighted_interleave:\n    Gets the total weight of the nodemask as well as each individual\n    node weight, then calculates the number of \"interleave rounds\" as\n    well as any delta (\"partial round\").  Calculates the number of\n    pages for each node and allocates them.\n\n    If a node was scheduled for interleave via interleave_nodes, the\n    current weight will be allocated first.\n\n    Operates only on the task->mempolicy.\n\nOne piece of complexity is the interaction between a recent refactor which\nsplit the logic to acquire the \"ilx\" (interleave index) of an allocation\nand the actually application of the interleave.  If a call to\nalloc_pages_mpol() were made with a weighted-interleave policy and ilx set\nto NO_INTERLEAVE_INDEX, weighted_interleave_nodes() would operate on a VMA\npolicy - violating the description above.\n\nAn inspection of all callers of alloc_pages_mpol() shows that all external\ncallers set ilx to `0`, an index value, or will call get_vma_policy() to\nacquire the ilx.\n\nFor example, mm/shmem.c may call into alloc_pages_mpol.  The call stacks\nall set (pgoff_t ilx) or end up in `get_vma_policy()`.  This enforces the\n`weighted_interleave_nodes()` and `weighted_interleave_nid()` policy\nrequirements (task/vma respectively).\n\nLink: https://lkml.kernel.org/r/20240202170238.90004-4-gregory.price@memverge.com\nSuggested-by: Hasan Al Maruf <Hasan.Maruf@amd.com>\nSigned-off-by: Gregory Price <gregory.price@memverge.com>\nCo-developed-by: Rakie Kim <rakie.kim@sk.com>\nSigned-off-by: Rakie Kim <rakie.kim@sk.com>\nCo-developed-by: Honggyu Kim <honggyu.kim@sk.com>\nSigned-off-by: Honggyu Kim <honggyu.kim@sk.com>\nCo-developed-by: Hyeongtak Ji <hyeongtak.ji@sk.com>\nSigned-off-by: Hyeongtak Ji <hyeongtak.ji@sk.com>\nCo-developed-by: Srinivasulu Thanneeru <sthanneeru.opensrc@micron.com>\nSigned-off-by: Srinivasulu Thanneeru <sthanneeru.opensrc@micron.com>\nCo-developed-by: Ravi Jonnalagadda <ravis.opensrc@micron.com>\nSigned-off-by: Ravi Jonnalagadda <ravis.opensrc@micron.com>\nReviewed-by: \"Huang, Ying\" <ying.huang@intel.com>\nCc: Dan Williams <dan.j.williams@intel.com>\nCc: Johannes Weiner <hannes@cmpxchg.org>\nCc: Jonathan Corbet <corbet@lwn.net>\nCc: Michal Hocko <mhocko@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/mempolicy: implement the sysfs-based weighted_interleave interface", "commit_message": "Patch series \"mm/mempolicy: weighted interleave mempolicy and sysfs\nextension\", v5.\n\nWeighted interleave is a new interleave policy intended to make use of\nheterogeneous memory environments appearing with CXL.\n\nThe existing interleave mechanism does an even round-robin distribution of\nmemory across all nodes in a nodemask, while weighted interleave\ndistributes memory across nodes according to a provided weight.  (Weight =\n# of page allocations per round)\n\nWeighted interleave is intended to reduce average latency when bandwidth\nis pressured - therefore increasing total throughput.\n\nIn other words: It allows greater use of the total available bandwidth in\na heterogeneous hardware environment (different hardware provides\ndifferent bandwidth capacity).\n\nAs bandwidth is pressured, latency increases - first linearly and then\nexponentially.  By keeping bandwidth usage distributed according to\navailable bandwidth, we therefore can reduce the average latency of a\ncacheline fetch.\n\nA good explanation of the bandwidth vs latency response curve:\nhttps://mahmoudhatem.wordpress.com/2017/11/07/memory-bandwidth-vs-latency-response-curve/\n\nFrom the article:\n```\nConstant region:\n    The latency response is fairly constant for the first 40%\n    of the sustained bandwidth.\nLinear region:\n    In between 40% to 80% of the sustained bandwidth, the\n    latency response increases almost linearly with the bandwidth\n    demand of the system due to contention overhead by numerous\n    memory requests.\nExponential region:\n    Between 80% to 100% of the sustained bandwidth, the memory\n    latency is dominated by the contention latency which can be\n    as much as twice the idle latency or more.\nMaximum sustained bandwidth :\n    Is 65% to 75% of the theoretical maximum bandwidth.\n```\n\nAs a general rule of thumb:\n* If bandwidth usage is low, latency does not increase. It is\n  optimal to place data in the nearest (lowest latency) device.\n* If bandwidth usage is high, latency increases. It is optimal\n  to place data such that bandwidth use is optimized per-device.\n\nThis is the top line goal: Provide a user a mechanism to target using the\n\"maximum sustained bandwidth\" of each hardware component in a heterogenous\nmemory system.\n\n\nFor example, the stream benchmark demonstrates that 1:1 (default)\ninterleave is actively harmful, while weighted interleave can be\nbeneficial.  Default interleave distributes data such that too much\npressure is placed on devices with lower available bandwidth.\n\nStream Benchmark (vs DRAM, 1 Socket + 1 CXL Device)\nDefault interleave : -78% (slower than DRAM)\nGlobal weighting   : -6% to +4% (workload dependant)\nTargeted weights   : +2.5% to +4% (consistently better than DRAM)\n\nGlobal means the task-policy was set (set_mempolicy), while targeted means\nVMA policies were set (mbind2).  We see weighted interleave is not always\nbeneficial when applied globally, but is always beneficial when applied to\nbandwidth-driving memory regions.\n\n\nThere are 4 patches in this set:\n1) Implement system-global interleave weights as sysfs extension\n   in mm/mempolicy.c.  These weights are RCU protected, and a\n   default weight set is provided (all weights are 1 by default).\n\n   In future work, we intend to expose an interface for HMAT/CDAT\n   code to set reasonable default values based on the memory\n   configuration of the system discovered at boot/hotplug.\n\n2) A mild refactor of some interleave-logic for re-use in the\n   new weighted interleave logic.\n\n3) MPOL_WEIGHTED_INTERLEAVE extension for set_mempolicy/mbind\n\n4) Protect interleave logic (weighted and normal) with the\n   mems_allowed seq cookie.  If the nodemask changes while\n   accessing it during a rebind, just retry the access.\n\nIncluded below are some performance and LTP test information,\nand a sample numactl branch which can be used for testing.\n\n= Performance summary =\n(tests may have different configurations, see extended info below)\n1) MLC (W2) : +38% over DRAM. +264% over default interleave.\n   MLC (W5) : +40% over DRAM. +226% over default interleave.\n2) Stream   : -6% to +4% over DRAM, +430% over default interleave.\n3) XSBench  : +19% over DRAM. +47% over default interleave.\n\n= LTP Testing Summary =\nexisting mempolicy & mbind tests: pass\nmempolicy & mbind + weighted interleave (global weights): pass\n\n= version history\nv5:\n- style fixes\n- mems_allowed cookie protection to detect rebind issues,\n  prevents spurious allocation failures and/or mis-allocations\n- sparse warning fixes related to __rcu on local variables\n\n=====================================================================\nPerformance tests - MLC\nFrom - Ravi Jonnalagadda <ravis.opensrc@micron.com>\n\nHardware: Single-socket, multiple CXL memory expanders.\n\nWorkload:                               W2\nData Signature:                         2:1 read:write\nDRAM only bandwidth (GBps):             298.8\nDRAM + CXL (default interleave) (GBps): 113.04\nDRAM + CXL (weighted interleave)(GBps): 412.5\nGain over DRAM only:                    1.38x\nGain over default interleave:           2.64x\n\nWorkload:                               W5\nData Signature:                         1:1 read:write\nDRAM only bandwidth (GBps):             273.2\nDRAM + CXL (default interleave) (GBps): 117.23\nDRAM + CXL (weighted interleave)(GBps): 382.7\nGain over DRAM only:                    1.4x\nGain over default interleave:           2.26x\n\n=====================================================================\nPerformance test - Stream\nFrom - Gregory Price <gregory.price@memverge.com>\n\nHardware: Single socket, single CXL expander\nnumactl extension: https://github.com/gmprice/numactl/tree/weighted_interleave_master\n\nSummary: 64 threads, ~18GB workload, 3GB per array, executed 100 times\nDefault interleave : -78% (slower than DRAM)\nGlobal weighting   : -6% to +4% (workload dependant)\nmbind2 weights     : +2.5% to +4% (consistently better than DRAM)\n\ndram only:\nnumactl --cpunodebind=1 --membind=1 ./stream_c.exe --ntimes 100 --array-size 400M --malloc\nFunction     Direction    BestRateMBs     AvgTime      MinTime      MaxTime\nCopy:        0->0            200923.2     0.032662     0.031853     0.033301\nScale:       0->0            202123.0     0.032526     0.031664     0.032970\nAdd:         0->0            208873.2     0.047322     0.045961     0.047884\nTriad:       0->0            208523.8     0.047262     0.046038     0.048414\n\nCXL-only:\nnumactl --cpunodebind=1 -w --membind=2 ./stream_c.exe --ntimes 100 --array-size 400M --malloc\nCopy:        0->0             22209.7     0.288661     0.288162     0.289342\nScale:       0->0             22288.2     0.287549     0.287147     0.288291\nAdd:         0->0             24419.1     0.393372     0.393135     0.393735\nTriad:       0->0             24484.6     0.392337     0.392083     0.394331\n\nBased on the above, the optimal weights are ~9:1\necho 9 > /sys/kernel/mm/mempolicy/weighted_interleave/node1\necho 1 > /sys/kernel/mm/mempolicy/weighted_interleave/node2\n\ndefault interleave:\nnumactl --cpunodebind=1 --interleave=1,2 ./stream_c.exe --ntimes 100 --array-size 400M --malloc\nCopy:        0->0             44666.2     0.143671     0.143285     0.144174\nScale:       0->0             44781.6     0.143256     0.142916     0.143713\nAdd:         0->0             48600.7     0.197719     0.197528     0.197858\nTriad:       0->0             48727.5     0.197204     0.197014     0.197439\n\nglobal weighted interleave:\nnumactl --cpunodebind=1 -w --interleave=1,2 ./stream_c.exe --ntimes 100 --array-size 400M --malloc\nCopy:        0->0            190085.9     0.034289     0.033669     0.034645\nScale:       0->0            207677.4     0.031909     0.030817     0.033061\nAdd:         0->0            202036.8     0.048737     0.047516     0.053409\nTriad:       0->0            217671.5     0.045819     0.044103     0.046755\n\ntargted regions w/ global weights (modified stream to mbind2 malloc'd regions))\nnumactl --cpunodebind=1 --membind=1 ./stream_c.exe -b --ntimes 100 --array-size 400M --malloc\nCopy:        0->0            205827.0     0.031445     0.031094     0.031984\nScale:       0->0            208171.8     0.031320     0.030744     0.032505\nAdd:         0->0            217352.0     0.045087     0.044168     0.046515\nTriad:       0->0            216884.8     0.045062     0.044263     0.046982\n\n=====================================================================\nPerformance tests - XSBench\nFrom - Hyeongtak Ji <hyeongtak.ji@sk.com>\n\nHardware: Single socket, Single CXL memory Expander\n\nNUMA node 0: 56 logical cores, 128 GB memory\nNUMA node 2: 96 GB CXL memory\nThreads:     56\nLookups:     170,000,000\n\nSummary: +19% over DRAM. +47% over default interleave.\n\nPerformance tests - XSBench\n1. dram only\n$ numactl -m 0 ./XSBench -s XL –p 5000000\nRuntime:     36.235 seconds\nLookups/s:   4,691,618\n\n2. default interleave\n$ numactl –i 0,2 ./XSBench –s XL –p 5000000\nRuntime:     55.243 seconds\nLookups/s:   3,077,293\n\n3. weighted interleave\nnumactl –w –i 0,2 ./XSBench –s XL –p 5000000\nRuntime:     29.262 seconds\nLookups/s:   5,809,513\n\n=====================================================================\nLTP Tests: https://github.com/gmprice/ltp/tree/mempolicy2\n\n= Existing tests\nset_mempolicy, get_mempolicy, mbind\n\nMPOL_WEIGHTED_INTERLEAVE added manually to test basic functionality but\ndid not adjust tests for weighting.  Basically the weights were set to 1,\nwhich is the default, and it should behave the same as MPOL_INTERLEAVE if\nlogic is correct.\n\n== set_mempolicy01 : passed   18, failed   0\n== set_mempolicy02 : passed   10, failed   0\n== set_mempolicy03 : passed   64, failed   0\n== set_mempolicy04 : passed   32, failed   0\n== set_mempolicy05 - n/a on non-x86\n== set_mempolicy06 : passed   10, failed   0\n   this is set_mempolicy02 + MPOL_WEIGHTED_INTERLEAVE\n== set_mempolicy07 : passed   32, failed   0\n   set_mempolicy04 + MPOL_WEIGHTED_INTERLEAVE\n== get_mempolicy01 : passed   12, failed   0\n   change: added MPOL_WEIGHTED_INTERLEAVE\n== get_mempolicy02 : passed   2, failed   0\n== mbind01 : passed   15, failed   0\n   added MPOL_WEIGHTED_INTERLEAVE\n== mbind02 : passed   4, failed   0\n   added MPOL_WEIGHTED_INTERLEAVE\n== mbind03 : passed   16, failed   0\n   added MPOL_WEIGHTED_INTERLEAVE\n== mbind04 : passed   48, failed   0\n   added MPOL_WEIGHTED_INTERLEAVE\n\n=====================================================================\nnumactl (set_mempolicy) w/ global weighting test\nnumactl fork: https://github.com/gmprice/numactl/tree/weighted_interleave_master\n\ncommand: numactl -w --interleave=0,1 ./eatmem\n\nresult (weights 1:1):\n0176a000 weighted interleave:0-1 heap anon=65793 dirty=65793 active=0 N0=32897 N1=32896 kernelpagesize_kB=4\n7fceeb9ff000 weighted interleave:0-1 anon=65537 dirty=65537 active=0 N0=32768 N1=32769 kernelpagesize_kB=4\n50% distribution is correct\n\nresult (weights 5:1):\n01b14000 weighted interleave:0-1 heap anon=65793 dirty=65793 active=0 N0=54828 N1=10965 kernelpagesize_kB=4\n7f47a1dff000 weighted interleave:0-1 anon=65537 dirty=65537 active=0 N0=54614 N1=10923 kernelpagesize_kB=4\n16.666% distribution is correct\n\nresult (weights 1:5):\n01f07000 weighted interleave:0-1 heap anon=65793 dirty=65793 active=0 N0=10966 N1=54827 kernelpagesize_kB=4\n7f17b1dff000 weighted interleave:0-1 anon=65537 dirty=65537 active=0 N0=10923 N1=54614 kernelpagesize_kB=4\n16.666% distribution is correct\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\nint main (void)\n{\n        char* mem = malloc(1024*1024*256);\n        memset(mem, 1, 1024*1024*256);\n        for (int i = 0; i  < ((1024*1024*256)/4096); i++)\n        {\n                mem = malloc(4096);\n                mem[0] = 1;\n        }\n        printf(\"done\\n\");\n        getchar();\n        return 0;\n}\n\n\nThis patch (of 4):\n\nThis patch provides a way to set interleave weight information under sysfs\nat /sys/kernel/mm/mempolicy/weighted_interleave/nodeN\n\nThe sysfs structure is designed as follows.\n\n  $ tree /sys/kernel/mm/mempolicy/\n  /sys/kernel/mm/mempolicy/ [1]\n  └── weighted_interleave [2]\n      ├── node0 [3]\n      └── node1\n\nEach file above can be explained as follows.\n\n[1] mm/mempolicy: configuration interface for mempolicy subsystem\n\n[2] weighted_interleave/: config interface for weighted interleave policy\n\n[3] weighted_interleave/nodeN: weight for nodeN\n\nIf a node value is set to `0`, the system-default value will be used.\nAs of this patch, the system-default for all nodes is always 1.\n\nLink: https://lkml.kernel.org/r/20240202170238.90004-1-gregory.price@memverge.com\nLink: https://lkml.kernel.org/r/20240202170238.90004-2-gregory.price@memverge.com\nSuggested-by: \"Huang, Ying\" <ying.huang@intel.com>\nSigned-off-by: Rakie Kim <rakie.kim@sk.com>\nSigned-off-by: Honggyu Kim <honggyu.kim@sk.com>\nCo-developed-by: Gregory Price <gregory.price@memverge.com>\nSigned-off-by: Gregory Price <gregory.price@memverge.com>\nCo-developed-by: Hyeongtak Ji <hyeongtak.ji@sk.com>\nSigned-off-by: Hyeongtak Ji <hyeongtak.ji@sk.com>\nReviewed-by: \"Huang, Ying\" <ying.huang@intel.com>\nCc: Dan Williams <dan.j.williams@intel.com>\nCc: Gregory Price <gourry.memverge@gmail.com>\nCc: Hasan Al Maruf <Hasan.Maruf@amd.com>\nCc: Johannes Weiner <hannes@cmpxchg.org>\nCc: Jonathan Corbet <corbet@lwn.net>\nCc: Michal Hocko <mhocko@kernel.org>\nCc: Srinivasulu Thanneeru <sthanneeru.opensrc@micron.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}]
    }
}

2024-12-03 22:51:20,268 - extractor - DEBUG - deepseek response:
ChatCompletion(id='83ade2bf-0871-45ca-a721-dbfb05bd2af7', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "proper_nouns": [\n        {\n            "MPOL_WEIGHTED_INTERLEAVE": "MPOL_WEIGHTED_INTERLEAVE is a new memory policy introduced in the Linux kernel to optimize memory allocation across NUMA (Non-Uniform Memory Access) nodes. Unlike the traditional MPOL_INTERLEAVE policy, which distributes memory evenly across all nodes, MPOL_WEIGHTED_INTERLEAVE allows for proportional distribution based on specified weights, thereby optimizing for bandwidth availability."\n        },\n        {\n            "NUMA (Non-Uniform Memory Access)": "NUMA is a memory architecture where the memory access time depends on the memory location relative to the processor. In a NUMA system, the memory is divided into multiple nodes, and each processor has faster access to memory in its local node compared to memory in other nodes."\n        },\n        {\n            "sysfs": "sysfs is a virtual filesystem provided by the Linux kernel that exports information about various kernel subsystems, hardware devices, and associated device drivers from the kernel\'s device model to user space. It allows for dynamic configuration and monitoring of kernel parameters."\n        },\n        {\n            "CXL (Compute Express Link)": "CXL is an open industry standard interconnect that enables high-speed communication between a processor and devices like accelerators, memory buffers, and smart I/O devices. It is designed to improve performance and efficiency in data-intensive applications."\n        }\n    ],\n    "background_knowledge": [\n        "The traditional MPOL_INTERLEAVE policy distributes memory evenly across all NUMA nodes, which can lead to suboptimal performance in systems with heterogeneous memory types (e.g., DRAM and CXL-attached memory) due to different bandwidth characteristics.",\n        "Weighted interleaving allows for a more balanced use of available bandwidth by distributing memory allocations according to the bandwidth capacity of each NUMA node, as specified by the weights."\n    ],\n    "kernel_changes": [\n        "Before the introduction of MPOL_WEIGHTED_INTERLEAVE, the Linux kernel used a simple round-robin approach for memory allocation across NUMA nodes, which could lead to inefficient use of memory bandwidth in heterogeneous environments.",\n        "With the new MPOL_WEIGHTED_INTERLEAVE policy, the kernel now supports weighted distribution of memory across NUMA nodes, allowing for better utilization of available bandwidth. This is achieved by assigning weights to each node, which determine the proportion of memory allocations that should be directed to that node.",\n        "Additionally, a new sysfs interface has been implemented to allow users to set and adjust these weights dynamically, providing greater flexibility in memory policy management."\n    ],\n    "comprehensive_understanding": "The introduction of MPOL_WEIGHTED_INTERLEAVE and the associated sysfs extension represent a significant enhancement to the Linux kernel\'s memory management capabilities, particularly in environments with heterogeneous memory types. By allowing for weighted interleaving of memory allocations, the kernel can better optimize for bandwidth availability, leading to improved performance and reduced latency in bandwidth-intensive workloads. This feature is particularly beneficial in systems with CXL-attached memory, where the ability to distribute memory according to bandwidth capacity can lead to substantial performance gains. The new sysfs interface further enhances the usability of this feature by providing a straightforward way to configure and manage memory policy weights."\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733237442, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=717, prompt_tokens=8408, total_tokens=9125, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=1472, prompt_cache_miss_tokens=6936))
2024-12-03 22:51:20,269 - extractor - DEBUG - invoking deepseek with prompt:
{
    "instructions": "你是一个命名实体抽取 (NER) 任务的专家，同时也是 linux 内核的专家。给你一个 linux 内核特性的描述 (feature_description)、对应的若干 commit 的标题和提交信息 (commits)、该特性涉及到的一些 linux 内核中的专有名词及其解释 (proper_nouns)、为了让开发者理解该特性所需的与这个特性直接相关的背景知识 (background_knowledge)、该特性被实现前后，linux 内核发生的变化 (kernel_changes)、对该特性的综合理解 (comprehensive_understanding)。你的任务是：根据这些信息提取出在 linux 内核中涉及到的和这个特性相关的概念实体 (concepts)。用英语回答。用json格式字符串返回你的回答。你可以参考 examples 字段给出的例子。",
    "examples": [
        "input": {
            "feature_description": "Prohibit the last subpage from reusing the entire large folio",
            "commits": [{"commit_subject": "mm: prohibit the last subpage from reusing the entire large folio", "commit_message": "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire
large folio, resulting in the waste of (nr_pages - 1) pages.  This wasted
memory remains allocated until it is either unmapped or memory reclamation
occurs.

The following small program can serve as evidence of this behavior

 main()
 {
 #define SIZE 1024 * 1024 * 1024UL
         void *p = malloc(SIZE);
         memset(p, 0x11, SIZE);
         if (fork() == 0)
                 _exit(0);
         memset(p, 0x12, SIZE);
         printf("done\n");
         while(1);
 }

For example, using a 1024KiB mTHP by:
 echo always > /sys/kernel/mm/transparent_hugepage/hugepages-1024kB/enabled

(1) w/o the patch, it takes 2GiB,

Before running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754          84        5692           0          17        5669
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754        2149        3627           0          19        3605
 Swap:              0           0           0

(2) w/ the patch, it takes 1GiB only,

Before running the test program,
 / # free -m
                 total        used        free      shared  buff/cache   available
 Mem:            5754          89        5687           0          17        5664
 Swap:              0           0           0

 / # /a.out &
 / # done

After running the test program,
 / # free -m
                total        used        free      shared  buff/cache   available
 Mem:            5754        1122        4655           0          17        4632
 Swap:              0           0           0

This patch migrates the last subpage to a small folio and immediately
returns the large folio to the system. It benefits both memory availability
and anti-fragmentation.

Link: https://lkml.kernel.org/r/20240308092721.144735-1-21cnbao@gmail.com
Signed-off-by: Barry Song <v-songbaohua@oppo.com>
Acked-by: David Hildenbrand <david@redhat.com>
Cc: Ryan Roberts <ryan.roberts@arm.com>
Cc: Lance Yang <ioworker0@gmail.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

"}],
            "proper_nouns": [
                {"Copy-on-Write (CoW)": Copy-on-Write (COW) is a memory management technique used in the Linux kernel to optimize the process of creating new processes or modifying shared resources. The core idea behind COW is to defer the actual copying of data until it is necessary, thereby reducing the overhead associated with creating new processes or modifying shared data.},
                {"mTHP": mTHP, which stands for "Memory Transparent Huge Pages," is a feature in the Linux kernel designed to improve memory performance by using larger memory pages, known as huge pages. Traditional memory pages in Linux are typically 4KB in size, but huge pages can be much larger, such as 2MB or even 1GB, depending on the architecture.},
                {"folio": In the Linux kernel, a "folio" is a higher-level abstraction introduced to manage memory pages more efficiently, especially in the context of file system operations. The concept of a folio aims to address some of the inefficiencies associated with traditional page management, particularly when dealing with large files or when performing operations that span multiple pages.},
            ],
            "background_knowledge": [
                "In a Copy-on-Write (CoW) scenario, the last subpage will reuse the entire large folio, resulting in the waste of (nr_pages - 1) pages. This wasted memory remains allocated until it is either unmapped or memory reclamation occurs."
            ],
            "kernel_changes": [
                "Before the implementation of this feature, the Linux kernel allowed the last subpage to reuse the entire large folio in a Copy-on-Write (CoW) scenario, leading to the waste of (nr_pages - 1) pages. This wasted memory remained allocated until it was either unmapped or memory reclamation occurred. After the implementation, the kernel was modified to migrate the last subpage to a small folio and immediately return the large folio to the system. This change reduced memory waste, improved memory availability, and reduced fragmentation. As a result, memory usage in CoW scenarios was optimized, leading to better system performance and resource utilization."
            ],
            "comprehensive_understanding": [
                "This feature optimizes memory management in Copy-on-Write (CoW) scenarios by preventing the last subpage from reusing the entire large folio. Instead, it migrates the last subpage to a small folio and immediately returns the large folio to the system. This reduces memory waste, improves memory availability, and mitigates fragmentation, ultimately enhancing system performance and resource utilization."
            ]
        },
        "output": {
            "concepts": ["Copy-on-Write (CoW)", "mTHP", "folio", "memory reclamation", "memory availability", "anti-fragmentation"]
        }
    ],
    "input": {
        "feature_description": "(FEATURED) weighted interleave mempolicy and sysfs extension",
        "commits": [{"commit_subject": "mm/mempolicy: refactor a read-once mechanism into a function for re-use", "commit_message": "Move the use of barrier() to force policy->nodemask onto the stack into a\nfunction `read_once_policy_nodemask` so that it may be re-used.\n\nLink: https://lkml.kernel.org/r/20240202170238.90004-3-gregory.price@memverge.com\nSigned-off-by: Gregory Price <gregory.price@memverge.com>\nSuggested-by: \"Huang, Ying\" <ying.huang@intel.com>\nReviewed-by: \"Huang, Ying\" <ying.huang@intel.com>\nCc: Dan Williams <dan.j.williams@intel.com>\nCc: Hasan Al Maruf <Hasan.Maruf@amd.com>\nCc: Honggyu Kim <honggyu.kim@sk.com>\nCc: Hyeongtak Ji <hyeongtak.ji@sk.com>\nCc: Johannes Weiner <hannes@cmpxchg.org>\nCc: Jonathan Corbet <corbet@lwn.net>\nCc: Michal Hocko <mhocko@kernel.org>\nCc: Rakie Kim <rakie.kim@sk.com>\nCc: Ravi Jonnalagadda <ravis.opensrc@micron.com>\nCc: Srinivasulu Thanneeru <sthanneeru.opensrc@micron.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/mempolicy: introduce MPOL_WEIGHTED_INTERLEAVE for weighted interleaving", "commit_message": "When a system has multiple NUMA nodes and it becomes bandwidth hungry,\nusing the current MPOL_INTERLEAVE could be an wise option.\n\nHowever, if those NUMA nodes consist of different types of memory such as\nsocket-attached DRAM and CXL/PCIe attached DRAM, the round-robin based\ninterleave policy does not optimally distribute data to make use of their\ndifferent bandwidth characteristics.\n\nInstead, interleave is more effective when the allocation policy follows\neach NUMA nodes' bandwidth weight rather than a simple 1:1 distribution.\n\nThis patch introduces a new memory policy, MPOL_WEIGHTED_INTERLEAVE,\nenabling weighted interleave between NUMA nodes.  Weighted interleave\nallows for proportional distribution of memory across multiple numa nodes,\npreferably apportioned to match the bandwidth of each node.\n\nFor example, if a system has 1 CPU node (0), and 2 memory nodes (0,1),\nwith bandwidth of (100GB/s, 50GB/s) respectively, the appropriate weight\ndistribution is (2:1).\n\nWeights for each node can be assigned via the new sysfs extension:\n/sys/kernel/mm/mempolicy/weighted_interleave/\n\nFor now, the default value of all nodes will be `1`, which matches the\nbehavior of standard 1:1 round-robin interleave.  An extension will be\nadded in the future to allow default values to be registered at kernel and\ndevice bringup time.\n\nThe policy allocates a number of pages equal to the set weights.  For\nexample, if the weights are (2,1), then 2 pages will be allocated on node0\nfor every 1 page allocated on node1.\n\nThe new flag MPOL_WEIGHTED_INTERLEAVE can be used in set_mempolicy(2)\nand mbind(2).\n\nSome high level notes about the pieces of weighted interleave:\n\ncurrent->il_prev:\n    Tracks the node previously allocated from.\n\ncurrent->il_weight:\n    The active weight of the current node (current->il_prev)\n    When this reaches 0, current->il_prev is set to the next node\n    and current->il_weight is set to the next weight.\n\nweighted_interleave_nodes:\n    Counts the number of allocations as they occur, and applies the\n    weight for the current node.  When the weight reaches 0, switch\n    to the next node.  Operates only on task->mempolicy.\n\nweighted_interleave_nid:\n    Gets the total weight of the nodemask as well as each individual\n    node weight, then calculates the node based on the given index.\n    Operates on VMA policies.\n\nbulk_array_weighted_interleave:\n    Gets the total weight of the nodemask as well as each individual\n    node weight, then calculates the number of \"interleave rounds\" as\n    well as any delta (\"partial round\").  Calculates the number of\n    pages for each node and allocates them.\n\n    If a node was scheduled for interleave via interleave_nodes, the\n    current weight will be allocated first.\n\n    Operates only on the task->mempolicy.\n\nOne piece of complexity is the interaction between a recent refactor which\nsplit the logic to acquire the \"ilx\" (interleave index) of an allocation\nand the actually application of the interleave.  If a call to\nalloc_pages_mpol() were made with a weighted-interleave policy and ilx set\nto NO_INTERLEAVE_INDEX, weighted_interleave_nodes() would operate on a VMA\npolicy - violating the description above.\n\nAn inspection of all callers of alloc_pages_mpol() shows that all external\ncallers set ilx to `0`, an index value, or will call get_vma_policy() to\nacquire the ilx.\n\nFor example, mm/shmem.c may call into alloc_pages_mpol.  The call stacks\nall set (pgoff_t ilx) or end up in `get_vma_policy()`.  This enforces the\n`weighted_interleave_nodes()` and `weighted_interleave_nid()` policy\nrequirements (task/vma respectively).\n\nLink: https://lkml.kernel.org/r/20240202170238.90004-4-gregory.price@memverge.com\nSuggested-by: Hasan Al Maruf <Hasan.Maruf@amd.com>\nSigned-off-by: Gregory Price <gregory.price@memverge.com>\nCo-developed-by: Rakie Kim <rakie.kim@sk.com>\nSigned-off-by: Rakie Kim <rakie.kim@sk.com>\nCo-developed-by: Honggyu Kim <honggyu.kim@sk.com>\nSigned-off-by: Honggyu Kim <honggyu.kim@sk.com>\nCo-developed-by: Hyeongtak Ji <hyeongtak.ji@sk.com>\nSigned-off-by: Hyeongtak Ji <hyeongtak.ji@sk.com>\nCo-developed-by: Srinivasulu Thanneeru <sthanneeru.opensrc@micron.com>\nSigned-off-by: Srinivasulu Thanneeru <sthanneeru.opensrc@micron.com>\nCo-developed-by: Ravi Jonnalagadda <ravis.opensrc@micron.com>\nSigned-off-by: Ravi Jonnalagadda <ravis.opensrc@micron.com>\nReviewed-by: \"Huang, Ying\" <ying.huang@intel.com>\nCc: Dan Williams <dan.j.williams@intel.com>\nCc: Johannes Weiner <hannes@cmpxchg.org>\nCc: Jonathan Corbet <corbet@lwn.net>\nCc: Michal Hocko <mhocko@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}, {"commit_subject": "mm/mempolicy: implement the sysfs-based weighted_interleave interface", "commit_message": "Patch series \"mm/mempolicy: weighted interleave mempolicy and sysfs\nextension\", v5.\n\nWeighted interleave is a new interleave policy intended to make use of\nheterogeneous memory environments appearing with CXL.\n\nThe existing interleave mechanism does an even round-robin distribution of\nmemory across all nodes in a nodemask, while weighted interleave\ndistributes memory across nodes according to a provided weight.  (Weight =\n# of page allocations per round)\n\nWeighted interleave is intended to reduce average latency when bandwidth\nis pressured - therefore increasing total throughput.\n\nIn other words: It allows greater use of the total available bandwidth in\na heterogeneous hardware environment (different hardware provides\ndifferent bandwidth capacity).\n\nAs bandwidth is pressured, latency increases - first linearly and then\nexponentially.  By keeping bandwidth usage distributed according to\navailable bandwidth, we therefore can reduce the average latency of a\ncacheline fetch.\n\nA good explanation of the bandwidth vs latency response curve:\nhttps://mahmoudhatem.wordpress.com/2017/11/07/memory-bandwidth-vs-latency-response-curve/\n\nFrom the article:\n```\nConstant region:\n    The latency response is fairly constant for the first 40%\n    of the sustained bandwidth.\nLinear region:\n    In between 40% to 80% of the sustained bandwidth, the\n    latency response increases almost linearly with the bandwidth\n    demand of the system due to contention overhead by numerous\n    memory requests.\nExponential region:\n    Between 80% to 100% of the sustained bandwidth, the memory\n    latency is dominated by the contention latency which can be\n    as much as twice the idle latency or more.\nMaximum sustained bandwidth :\n    Is 65% to 75% of the theoretical maximum bandwidth.\n```\n\nAs a general rule of thumb:\n* If bandwidth usage is low, latency does not increase. It is\n  optimal to place data in the nearest (lowest latency) device.\n* If bandwidth usage is high, latency increases. It is optimal\n  to place data such that bandwidth use is optimized per-device.\n\nThis is the top line goal: Provide a user a mechanism to target using the\n\"maximum sustained bandwidth\" of each hardware component in a heterogenous\nmemory system.\n\n\nFor example, the stream benchmark demonstrates that 1:1 (default)\ninterleave is actively harmful, while weighted interleave can be\nbeneficial.  Default interleave distributes data such that too much\npressure is placed on devices with lower available bandwidth.\n\nStream Benchmark (vs DRAM, 1 Socket + 1 CXL Device)\nDefault interleave : -78% (slower than DRAM)\nGlobal weighting   : -6% to +4% (workload dependant)\nTargeted weights   : +2.5% to +4% (consistently better than DRAM)\n\nGlobal means the task-policy was set (set_mempolicy), while targeted means\nVMA policies were set (mbind2).  We see weighted interleave is not always\nbeneficial when applied globally, but is always beneficial when applied to\nbandwidth-driving memory regions.\n\n\nThere are 4 patches in this set:\n1) Implement system-global interleave weights as sysfs extension\n   in mm/mempolicy.c.  These weights are RCU protected, and a\n   default weight set is provided (all weights are 1 by default).\n\n   In future work, we intend to expose an interface for HMAT/CDAT\n   code to set reasonable default values based on the memory\n   configuration of the system discovered at boot/hotplug.\n\n2) A mild refactor of some interleave-logic for re-use in the\n   new weighted interleave logic.\n\n3) MPOL_WEIGHTED_INTERLEAVE extension for set_mempolicy/mbind\n\n4) Protect interleave logic (weighted and normal) with the\n   mems_allowed seq cookie.  If the nodemask changes while\n   accessing it during a rebind, just retry the access.\n\nIncluded below are some performance and LTP test information,\nand a sample numactl branch which can be used for testing.\n\n= Performance summary =\n(tests may have different configurations, see extended info below)\n1) MLC (W2) : +38% over DRAM. +264% over default interleave.\n   MLC (W5) : +40% over DRAM. +226% over default interleave.\n2) Stream   : -6% to +4% over DRAM, +430% over default interleave.\n3) XSBench  : +19% over DRAM. +47% over default interleave.\n\n= LTP Testing Summary =\nexisting mempolicy & mbind tests: pass\nmempolicy & mbind + weighted interleave (global weights): pass\n\n= version history\nv5:\n- style fixes\n- mems_allowed cookie protection to detect rebind issues,\n  prevents spurious allocation failures and/or mis-allocations\n- sparse warning fixes related to __rcu on local variables\n\n=====================================================================\nPerformance tests - MLC\nFrom - Ravi Jonnalagadda <ravis.opensrc@micron.com>\n\nHardware: Single-socket, multiple CXL memory expanders.\n\nWorkload:                               W2\nData Signature:                         2:1 read:write\nDRAM only bandwidth (GBps):             298.8\nDRAM + CXL (default interleave) (GBps): 113.04\nDRAM + CXL (weighted interleave)(GBps): 412.5\nGain over DRAM only:                    1.38x\nGain over default interleave:           2.64x\n\nWorkload:                               W5\nData Signature:                         1:1 read:write\nDRAM only bandwidth (GBps):             273.2\nDRAM + CXL (default interleave) (GBps): 117.23\nDRAM + CXL (weighted interleave)(GBps): 382.7\nGain over DRAM only:                    1.4x\nGain over default interleave:           2.26x\n\n=====================================================================\nPerformance test - Stream\nFrom - Gregory Price <gregory.price@memverge.com>\n\nHardware: Single socket, single CXL expander\nnumactl extension: https://github.com/gmprice/numactl/tree/weighted_interleave_master\n\nSummary: 64 threads, ~18GB workload, 3GB per array, executed 100 times\nDefault interleave : -78% (slower than DRAM)\nGlobal weighting   : -6% to +4% (workload dependant)\nmbind2 weights     : +2.5% to +4% (consistently better than DRAM)\n\ndram only:\nnumactl --cpunodebind=1 --membind=1 ./stream_c.exe --ntimes 100 --array-size 400M --malloc\nFunction     Direction    BestRateMBs     AvgTime      MinTime      MaxTime\nCopy:        0->0            200923.2     0.032662     0.031853     0.033301\nScale:       0->0            202123.0     0.032526     0.031664     0.032970\nAdd:         0->0            208873.2     0.047322     0.045961     0.047884\nTriad:       0->0            208523.8     0.047262     0.046038     0.048414\n\nCXL-only:\nnumactl --cpunodebind=1 -w --membind=2 ./stream_c.exe --ntimes 100 --array-size 400M --malloc\nCopy:        0->0             22209.7     0.288661     0.288162     0.289342\nScale:       0->0             22288.2     0.287549     0.287147     0.288291\nAdd:         0->0             24419.1     0.393372     0.393135     0.393735\nTriad:       0->0             24484.6     0.392337     0.392083     0.394331\n\nBased on the above, the optimal weights are ~9:1\necho 9 > /sys/kernel/mm/mempolicy/weighted_interleave/node1\necho 1 > /sys/kernel/mm/mempolicy/weighted_interleave/node2\n\ndefault interleave:\nnumactl --cpunodebind=1 --interleave=1,2 ./stream_c.exe --ntimes 100 --array-size 400M --malloc\nCopy:        0->0             44666.2     0.143671     0.143285     0.144174\nScale:       0->0             44781.6     0.143256     0.142916     0.143713\nAdd:         0->0             48600.7     0.197719     0.197528     0.197858\nTriad:       0->0             48727.5     0.197204     0.197014     0.197439\n\nglobal weighted interleave:\nnumactl --cpunodebind=1 -w --interleave=1,2 ./stream_c.exe --ntimes 100 --array-size 400M --malloc\nCopy:        0->0            190085.9     0.034289     0.033669     0.034645\nScale:       0->0            207677.4     0.031909     0.030817     0.033061\nAdd:         0->0            202036.8     0.048737     0.047516     0.053409\nTriad:       0->0            217671.5     0.045819     0.044103     0.046755\n\ntargted regions w/ global weights (modified stream to mbind2 malloc'd regions))\nnumactl --cpunodebind=1 --membind=1 ./stream_c.exe -b --ntimes 100 --array-size 400M --malloc\nCopy:        0->0            205827.0     0.031445     0.031094     0.031984\nScale:       0->0            208171.8     0.031320     0.030744     0.032505\nAdd:         0->0            217352.0     0.045087     0.044168     0.046515\nTriad:       0->0            216884.8     0.045062     0.044263     0.046982\n\n=====================================================================\nPerformance tests - XSBench\nFrom - Hyeongtak Ji <hyeongtak.ji@sk.com>\n\nHardware: Single socket, Single CXL memory Expander\n\nNUMA node 0: 56 logical cores, 128 GB memory\nNUMA node 2: 96 GB CXL memory\nThreads:     56\nLookups:     170,000,000\n\nSummary: +19% over DRAM. +47% over default interleave.\n\nPerformance tests - XSBench\n1. dram only\n$ numactl -m 0 ./XSBench -s XL –p 5000000\nRuntime:     36.235 seconds\nLookups/s:   4,691,618\n\n2. default interleave\n$ numactl –i 0,2 ./XSBench –s XL –p 5000000\nRuntime:     55.243 seconds\nLookups/s:   3,077,293\n\n3. weighted interleave\nnumactl –w –i 0,2 ./XSBench –s XL –p 5000000\nRuntime:     29.262 seconds\nLookups/s:   5,809,513\n\n=====================================================================\nLTP Tests: https://github.com/gmprice/ltp/tree/mempolicy2\n\n= Existing tests\nset_mempolicy, get_mempolicy, mbind\n\nMPOL_WEIGHTED_INTERLEAVE added manually to test basic functionality but\ndid not adjust tests for weighting.  Basically the weights were set to 1,\nwhich is the default, and it should behave the same as MPOL_INTERLEAVE if\nlogic is correct.\n\n== set_mempolicy01 : passed   18, failed   0\n== set_mempolicy02 : passed   10, failed   0\n== set_mempolicy03 : passed   64, failed   0\n== set_mempolicy04 : passed   32, failed   0\n== set_mempolicy05 - n/a on non-x86\n== set_mempolicy06 : passed   10, failed   0\n   this is set_mempolicy02 + MPOL_WEIGHTED_INTERLEAVE\n== set_mempolicy07 : passed   32, failed   0\n   set_mempolicy04 + MPOL_WEIGHTED_INTERLEAVE\n== get_mempolicy01 : passed   12, failed   0\n   change: added MPOL_WEIGHTED_INTERLEAVE\n== get_mempolicy02 : passed   2, failed   0\n== mbind01 : passed   15, failed   0\n   added MPOL_WEIGHTED_INTERLEAVE\n== mbind02 : passed   4, failed   0\n   added MPOL_WEIGHTED_INTERLEAVE\n== mbind03 : passed   16, failed   0\n   added MPOL_WEIGHTED_INTERLEAVE\n== mbind04 : passed   48, failed   0\n   added MPOL_WEIGHTED_INTERLEAVE\n\n=====================================================================\nnumactl (set_mempolicy) w/ global weighting test\nnumactl fork: https://github.com/gmprice/numactl/tree/weighted_interleave_master\n\ncommand: numactl -w --interleave=0,1 ./eatmem\n\nresult (weights 1:1):\n0176a000 weighted interleave:0-1 heap anon=65793 dirty=65793 active=0 N0=32897 N1=32896 kernelpagesize_kB=4\n7fceeb9ff000 weighted interleave:0-1 anon=65537 dirty=65537 active=0 N0=32768 N1=32769 kernelpagesize_kB=4\n50% distribution is correct\n\nresult (weights 5:1):\n01b14000 weighted interleave:0-1 heap anon=65793 dirty=65793 active=0 N0=54828 N1=10965 kernelpagesize_kB=4\n7f47a1dff000 weighted interleave:0-1 anon=65537 dirty=65537 active=0 N0=54614 N1=10923 kernelpagesize_kB=4\n16.666% distribution is correct\n\nresult (weights 1:5):\n01f07000 weighted interleave:0-1 heap anon=65793 dirty=65793 active=0 N0=10966 N1=54827 kernelpagesize_kB=4\n7f17b1dff000 weighted interleave:0-1 anon=65537 dirty=65537 active=0 N0=10923 N1=54614 kernelpagesize_kB=4\n16.666% distribution is correct\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\nint main (void)\n{\n        char* mem = malloc(1024*1024*256);\n        memset(mem, 1, 1024*1024*256);\n        for (int i = 0; i  < ((1024*1024*256)/4096); i++)\n        {\n                mem = malloc(4096);\n                mem[0] = 1;\n        }\n        printf(\"done\\n\");\n        getchar();\n        return 0;\n}\n\n\nThis patch (of 4):\n\nThis patch provides a way to set interleave weight information under sysfs\nat /sys/kernel/mm/mempolicy/weighted_interleave/nodeN\n\nThe sysfs structure is designed as follows.\n\n  $ tree /sys/kernel/mm/mempolicy/\n  /sys/kernel/mm/mempolicy/ [1]\n  └── weighted_interleave [2]\n      ├── node0 [3]\n      └── node1\n\nEach file above can be explained as follows.\n\n[1] mm/mempolicy: configuration interface for mempolicy subsystem\n\n[2] weighted_interleave/: config interface for weighted interleave policy\n\n[3] weighted_interleave/nodeN: weight for nodeN\n\nIf a node value is set to `0`, the system-default value will be used.\nAs of this patch, the system-default for all nodes is always 1.\n\nLink: https://lkml.kernel.org/r/20240202170238.90004-1-gregory.price@memverge.com\nLink: https://lkml.kernel.org/r/20240202170238.90004-2-gregory.price@memverge.com\nSuggested-by: \"Huang, Ying\" <ying.huang@intel.com>\nSigned-off-by: Rakie Kim <rakie.kim@sk.com>\nSigned-off-by: Honggyu Kim <honggyu.kim@sk.com>\nCo-developed-by: Gregory Price <gregory.price@memverge.com>\nSigned-off-by: Gregory Price <gregory.price@memverge.com>\nCo-developed-by: Hyeongtak Ji <hyeongtak.ji@sk.com>\nSigned-off-by: Hyeongtak Ji <hyeongtak.ji@sk.com>\nReviewed-by: \"Huang, Ying\" <ying.huang@intel.com>\nCc: Dan Williams <dan.j.williams@intel.com>\nCc: Gregory Price <gourry.memverge@gmail.com>\nCc: Hasan Al Maruf <Hasan.Maruf@amd.com>\nCc: Johannes Weiner <hannes@cmpxchg.org>\nCc: Jonathan Corbet <corbet@lwn.net>\nCc: Michal Hocko <mhocko@kernel.org>\nCc: Srinivasulu Thanneeru <sthanneeru.opensrc@micron.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"}],
        "proper_nouns": [{"MPOL_WEIGHTED_INTERLEAVE": "MPOL_WEIGHTED_INTERLEAVE is a new memory policy introduced in the Linux kernel to optimize memory allocation across NUMA (Non-Uniform Memory Access) nodes. Unlike the traditional MPOL_INTERLEAVE policy, which distributes memory evenly across all nodes, MPOL_WEIGHTED_INTERLEAVE allows for proportional distribution based on specified weights, thereby optimizing for bandwidth availability."}, {"NUMA (Non-Uniform Memory Access)": "NUMA is a memory architecture where the memory access time depends on the memory location relative to the processor. In a NUMA system, the memory is divided into multiple nodes, and each processor has faster access to memory in its local node compared to memory in other nodes."}, {"sysfs": "sysfs is a virtual filesystem provided by the Linux kernel that exports information about various kernel subsystems, hardware devices, and associated device drivers from the kernel's device model to user space. It allows for dynamic configuration and monitoring of kernel parameters."}, {"CXL (Compute Express Link)": "CXL is an open industry standard interconnect that enables high-speed communication between a processor and devices like accelerators, memory buffers, and smart I/O devices. It is designed to improve performance and efficiency in data-intensive applications."}],
        "background_knowledge": ["The traditional MPOL_INTERLEAVE policy distributes memory evenly across all NUMA nodes, which can lead to suboptimal performance in systems with heterogeneous memory types (e.g., DRAM and CXL-attached memory) due to different bandwidth characteristics.", "Weighted interleaving allows for a more balanced use of available bandwidth by distributing memory allocations according to the bandwidth capacity of each NUMA node, as specified by the weights."],
        "kernel_changes": ["Before the introduction of MPOL_WEIGHTED_INTERLEAVE, the Linux kernel used a simple round-robin approach for memory allocation across NUMA nodes, which could lead to inefficient use of memory bandwidth in heterogeneous environments.", "With the new MPOL_WEIGHTED_INTERLEAVE policy, the kernel now supports weighted distribution of memory across NUMA nodes, allowing for better utilization of available bandwidth. This is achieved by assigning weights to each node, which determine the proportion of memory allocations that should be directed to that node.", "Additionally, a new sysfs interface has been implemented to allow users to set and adjust these weights dynamically, providing greater flexibility in memory policy management."],
        "comprehensive_understanding": "The introduction of MPOL_WEIGHTED_INTERLEAVE and the associated sysfs extension represent a significant enhancement to the Linux kernel's memory management capabilities, particularly in environments with heterogeneous memory types. By allowing for weighted interleaving of memory allocations, the kernel can better optimize for bandwidth availability, leading to improved performance and reduced latency in bandwidth-intensive workloads. This feature is particularly beneficial in systems with CXL-attached memory, where the ability to distribute memory according to bandwidth capacity can lead to substantial performance gains. The new sysfs interface further enhances the usability of this feature by providing a straightforward way to configure and manage memory policy weights."
    }
}

2024-12-03 22:51:26,295 - extractor - DEBUG - deepseek response:
ChatCompletion(id='2fe5d0ba-fd9c-4bbb-a1f1-fb05979d13ea', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\n{\n    "concepts": ["MPOL_WEIGHTED_INTERLEAVE", "NUMA (Non-Uniform Memory Access)", "sysfs", "CXL (Compute Express Link)", "memory allocation", "bandwidth optimization", "latency reduction", "heterogeneous memory", "memory policy", "round-robin distribution", "weighted distribution", "sysfs interface", "memory bandwidth", "performance improvement", "bandwidth-intensive workloads"]\n}\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733237480, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_1c141eb703', usage=CompletionUsage(completion_tokens=99, prompt_tokens=9126, total_tokens=9225, completion_tokens_details=None, prompt_tokens_details=None, prompt_cache_hit_tokens=1536, prompt_cache_miss_tokens=7590))
2024-12-03 22:51:26,295 - extractor - DEBUG - feature_extracted of feature_id=33209:
{
    "feature_id": 33209,
    "feature_description": "(FEATURED) weighted interleave mempolicy and sysfs extension",
    "version": "6.9",
    "commits": [
        {
            "commit_subject": "mm/mempolicy: refactor a read-once mechanism into a function for re-use",
            "commit_message": "Move the use of barrier() to force policy->nodemask onto the stack into a\nfunction `read_once_policy_nodemask` so that it may be re-used.\n\nLink: https://lkml.kernel.org/r/20240202170238.90004-3-gregory.price@memverge.com\nSigned-off-by: Gregory Price <gregory.price@memverge.com>\nSuggested-by: \"Huang, Ying\" <ying.huang@intel.com>\nReviewed-by: \"Huang, Ying\" <ying.huang@intel.com>\nCc: Dan Williams <dan.j.williams@intel.com>\nCc: Hasan Al Maruf <Hasan.Maruf@amd.com>\nCc: Honggyu Kim <honggyu.kim@sk.com>\nCc: Hyeongtak Ji <hyeongtak.ji@sk.com>\nCc: Johannes Weiner <hannes@cmpxchg.org>\nCc: Jonathan Corbet <corbet@lwn.net>\nCc: Michal Hocko <mhocko@kernel.org>\nCc: Rakie Kim <rakie.kim@sk.com>\nCc: Ravi Jonnalagadda <ravis.opensrc@micron.com>\nCc: Srinivasulu Thanneeru <sthanneeru.opensrc@micron.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm/mempolicy: introduce MPOL_WEIGHTED_INTERLEAVE for weighted interleaving",
            "commit_message": "When a system has multiple NUMA nodes and it becomes bandwidth hungry,\nusing the current MPOL_INTERLEAVE could be an wise option.\n\nHowever, if those NUMA nodes consist of different types of memory such as\nsocket-attached DRAM and CXL/PCIe attached DRAM, the round-robin based\ninterleave policy does not optimally distribute data to make use of their\ndifferent bandwidth characteristics.\n\nInstead, interleave is more effective when the allocation policy follows\neach NUMA nodes' bandwidth weight rather than a simple 1:1 distribution.\n\nThis patch introduces a new memory policy, MPOL_WEIGHTED_INTERLEAVE,\nenabling weighted interleave between NUMA nodes.  Weighted interleave\nallows for proportional distribution of memory across multiple numa nodes,\npreferably apportioned to match the bandwidth of each node.\n\nFor example, if a system has 1 CPU node (0), and 2 memory nodes (0,1),\nwith bandwidth of (100GB/s, 50GB/s) respectively, the appropriate weight\ndistribution is (2:1).\n\nWeights for each node can be assigned via the new sysfs extension:\n/sys/kernel/mm/mempolicy/weighted_interleave/\n\nFor now, the default value of all nodes will be `1`, which matches the\nbehavior of standard 1:1 round-robin interleave.  An extension will be\nadded in the future to allow default values to be registered at kernel and\ndevice bringup time.\n\nThe policy allocates a number of pages equal to the set weights.  For\nexample, if the weights are (2,1), then 2 pages will be allocated on node0\nfor every 1 page allocated on node1.\n\nThe new flag MPOL_WEIGHTED_INTERLEAVE can be used in set_mempolicy(2)\nand mbind(2).\n\nSome high level notes about the pieces of weighted interleave:\n\ncurrent->il_prev:\n    Tracks the node previously allocated from.\n\ncurrent->il_weight:\n    The active weight of the current node (current->il_prev)\n    When this reaches 0, current->il_prev is set to the next node\n    and current->il_weight is set to the next weight.\n\nweighted_interleave_nodes:\n    Counts the number of allocations as they occur, and applies the\n    weight for the current node.  When the weight reaches 0, switch\n    to the next node.  Operates only on task->mempolicy.\n\nweighted_interleave_nid:\n    Gets the total weight of the nodemask as well as each individual\n    node weight, then calculates the node based on the given index.\n    Operates on VMA policies.\n\nbulk_array_weighted_interleave:\n    Gets the total weight of the nodemask as well as each individual\n    node weight, then calculates the number of \"interleave rounds\" as\n    well as any delta (\"partial round\").  Calculates the number of\n    pages for each node and allocates them.\n\n    If a node was scheduled for interleave via interleave_nodes, the\n    current weight will be allocated first.\n\n    Operates only on the task->mempolicy.\n\nOne piece of complexity is the interaction between a recent refactor which\nsplit the logic to acquire the \"ilx\" (interleave index) of an allocation\nand the actually application of the interleave.  If a call to\nalloc_pages_mpol() were made with a weighted-interleave policy and ilx set\nto NO_INTERLEAVE_INDEX, weighted_interleave_nodes() would operate on a VMA\npolicy - violating the description above.\n\nAn inspection of all callers of alloc_pages_mpol() shows that all external\ncallers set ilx to `0`, an index value, or will call get_vma_policy() to\nacquire the ilx.\n\nFor example, mm/shmem.c may call into alloc_pages_mpol.  The call stacks\nall set (pgoff_t ilx) or end up in `get_vma_policy()`.  This enforces the\n`weighted_interleave_nodes()` and `weighted_interleave_nid()` policy\nrequirements (task/vma respectively).\n\nLink: https://lkml.kernel.org/r/20240202170238.90004-4-gregory.price@memverge.com\nSuggested-by: Hasan Al Maruf <Hasan.Maruf@amd.com>\nSigned-off-by: Gregory Price <gregory.price@memverge.com>\nCo-developed-by: Rakie Kim <rakie.kim@sk.com>\nSigned-off-by: Rakie Kim <rakie.kim@sk.com>\nCo-developed-by: Honggyu Kim <honggyu.kim@sk.com>\nSigned-off-by: Honggyu Kim <honggyu.kim@sk.com>\nCo-developed-by: Hyeongtak Ji <hyeongtak.ji@sk.com>\nSigned-off-by: Hyeongtak Ji <hyeongtak.ji@sk.com>\nCo-developed-by: Srinivasulu Thanneeru <sthanneeru.opensrc@micron.com>\nSigned-off-by: Srinivasulu Thanneeru <sthanneeru.opensrc@micron.com>\nCo-developed-by: Ravi Jonnalagadda <ravis.opensrc@micron.com>\nSigned-off-by: Ravi Jonnalagadda <ravis.opensrc@micron.com>\nReviewed-by: \"Huang, Ying\" <ying.huang@intel.com>\nCc: Dan Williams <dan.j.williams@intel.com>\nCc: Johannes Weiner <hannes@cmpxchg.org>\nCc: Jonathan Corbet <corbet@lwn.net>\nCc: Michal Hocko <mhocko@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        },
        {
            "commit_subject": "mm/mempolicy: implement the sysfs-based weighted_interleave interface",
            "commit_message": "Patch series \"mm/mempolicy: weighted interleave mempolicy and sysfs\nextension\", v5.\n\nWeighted interleave is a new interleave policy intended to make use of\nheterogeneous memory environments appearing with CXL.\n\nThe existing interleave mechanism does an even round-robin distribution of\nmemory across all nodes in a nodemask, while weighted interleave\ndistributes memory across nodes according to a provided weight.  (Weight =\n# of page allocations per round)\n\nWeighted interleave is intended to reduce average latency when bandwidth\nis pressured - therefore increasing total throughput.\n\nIn other words: It allows greater use of the total available bandwidth in\na heterogeneous hardware environment (different hardware provides\ndifferent bandwidth capacity).\n\nAs bandwidth is pressured, latency increases - first linearly and then\nexponentially.  By keeping bandwidth usage distributed according to\navailable bandwidth, we therefore can reduce the average latency of a\ncacheline fetch.\n\nA good explanation of the bandwidth vs latency response curve:\nhttps://mahmoudhatem.wordpress.com/2017/11/07/memory-bandwidth-vs-latency-response-curve/\n\nFrom the article:\n```\nConstant region:\n    The latency response is fairly constant for the first 40%\n    of the sustained bandwidth.\nLinear region:\n    In between 40% to 80% of the sustained bandwidth, the\n    latency response increases almost linearly with the bandwidth\n    demand of the system due to contention overhead by numerous\n    memory requests.\nExponential region:\n    Between 80% to 100% of the sustained bandwidth, the memory\n    latency is dominated by the contention latency which can be\n    as much as twice the idle latency or more.\nMaximum sustained bandwidth :\n    Is 65% to 75% of the theoretical maximum bandwidth.\n```\n\nAs a general rule of thumb:\n* If bandwidth usage is low, latency does not increase. It is\n  optimal to place data in the nearest (lowest latency) device.\n* If bandwidth usage is high, latency increases. It is optimal\n  to place data such that bandwidth use is optimized per-device.\n\nThis is the top line goal: Provide a user a mechanism to target using the\n\"maximum sustained bandwidth\" of each hardware component in a heterogenous\nmemory system.\n\n\nFor example, the stream benchmark demonstrates that 1:1 (default)\ninterleave is actively harmful, while weighted interleave can be\nbeneficial.  Default interleave distributes data such that too much\npressure is placed on devices with lower available bandwidth.\n\nStream Benchmark (vs DRAM, 1 Socket + 1 CXL Device)\nDefault interleave : -78% (slower than DRAM)\nGlobal weighting   : -6% to +4% (workload dependant)\nTargeted weights   : +2.5% to +4% (consistently better than DRAM)\n\nGlobal means the task-policy was set (set_mempolicy), while targeted means\nVMA policies were set (mbind2).  We see weighted interleave is not always\nbeneficial when applied globally, but is always beneficial when applied to\nbandwidth-driving memory regions.\n\n\nThere are 4 patches in this set:\n1) Implement system-global interleave weights as sysfs extension\n   in mm/mempolicy.c.  These weights are RCU protected, and a\n   default weight set is provided (all weights are 1 by default).\n\n   In future work, we intend to expose an interface for HMAT/CDAT\n   code to set reasonable default values based on the memory\n   configuration of the system discovered at boot/hotplug.\n\n2) A mild refactor of some interleave-logic for re-use in the\n   new weighted interleave logic.\n\n3) MPOL_WEIGHTED_INTERLEAVE extension for set_mempolicy/mbind\n\n4) Protect interleave logic (weighted and normal) with the\n   mems_allowed seq cookie.  If the nodemask changes while\n   accessing it during a rebind, just retry the access.\n\nIncluded below are some performance and LTP test information,\nand a sample numactl branch which can be used for testing.\n\n= Performance summary =\n(tests may have different configurations, see extended info below)\n1) MLC (W2) : +38% over DRAM. +264% over default interleave.\n   MLC (W5) : +40% over DRAM. +226% over default interleave.\n2) Stream   : -6% to +4% over DRAM, +430% over default interleave.\n3) XSBench  : +19% over DRAM. +47% over default interleave.\n\n= LTP Testing Summary =\nexisting mempolicy & mbind tests: pass\nmempolicy & mbind + weighted interleave (global weights): pass\n\n= version history\nv5:\n- style fixes\n- mems_allowed cookie protection to detect rebind issues,\n  prevents spurious allocation failures and/or mis-allocations\n- sparse warning fixes related to __rcu on local variables\n\n=====================================================================\nPerformance tests - MLC\nFrom - Ravi Jonnalagadda <ravis.opensrc@micron.com>\n\nHardware: Single-socket, multiple CXL memory expanders.\n\nWorkload:                               W2\nData Signature:                         2:1 read:write\nDRAM only bandwidth (GBps):             298.8\nDRAM + CXL (default interleave) (GBps): 113.04\nDRAM + CXL (weighted interleave)(GBps): 412.5\nGain over DRAM only:                    1.38x\nGain over default interleave:           2.64x\n\nWorkload:                               W5\nData Signature:                         1:1 read:write\nDRAM only bandwidth (GBps):             273.2\nDRAM + CXL (default interleave) (GBps): 117.23\nDRAM + CXL (weighted interleave)(GBps): 382.7\nGain over DRAM only:                    1.4x\nGain over default interleave:           2.26x\n\n=====================================================================\nPerformance test - Stream\nFrom - Gregory Price <gregory.price@memverge.com>\n\nHardware: Single socket, single CXL expander\nnumactl extension: https://github.com/gmprice/numactl/tree/weighted_interleave_master\n\nSummary: 64 threads, ~18GB workload, 3GB per array, executed 100 times\nDefault interleave : -78% (slower than DRAM)\nGlobal weighting   : -6% to +4% (workload dependant)\nmbind2 weights     : +2.5% to +4% (consistently better than DRAM)\n\ndram only:\nnumactl --cpunodebind=1 --membind=1 ./stream_c.exe --ntimes 100 --array-size 400M --malloc\nFunction     Direction    BestRateMBs     AvgTime      MinTime      MaxTime\nCopy:        0->0            200923.2     0.032662     0.031853     0.033301\nScale:       0->0            202123.0     0.032526     0.031664     0.032970\nAdd:         0->0            208873.2     0.047322     0.045961     0.047884\nTriad:       0->0            208523.8     0.047262     0.046038     0.048414\n\nCXL-only:\nnumactl --cpunodebind=1 -w --membind=2 ./stream_c.exe --ntimes 100 --array-size 400M --malloc\nCopy:        0->0             22209.7     0.288661     0.288162     0.289342\nScale:       0->0             22288.2     0.287549     0.287147     0.288291\nAdd:         0->0             24419.1     0.393372     0.393135     0.393735\nTriad:       0->0             24484.6     0.392337     0.392083     0.394331\n\nBased on the above, the optimal weights are ~9:1\necho 9 > /sys/kernel/mm/mempolicy/weighted_interleave/node1\necho 1 > /sys/kernel/mm/mempolicy/weighted_interleave/node2\n\ndefault interleave:\nnumactl --cpunodebind=1 --interleave=1,2 ./stream_c.exe --ntimes 100 --array-size 400M --malloc\nCopy:        0->0             44666.2     0.143671     0.143285     0.144174\nScale:       0->0             44781.6     0.143256     0.142916     0.143713\nAdd:         0->0             48600.7     0.197719     0.197528     0.197858\nTriad:       0->0             48727.5     0.197204     0.197014     0.197439\n\nglobal weighted interleave:\nnumactl --cpunodebind=1 -w --interleave=1,2 ./stream_c.exe --ntimes 100 --array-size 400M --malloc\nCopy:        0->0            190085.9     0.034289     0.033669     0.034645\nScale:       0->0            207677.4     0.031909     0.030817     0.033061\nAdd:         0->0            202036.8     0.048737     0.047516     0.053409\nTriad:       0->0            217671.5     0.045819     0.044103     0.046755\n\ntargted regions w/ global weights (modified stream to mbind2 malloc'd regions))\nnumactl --cpunodebind=1 --membind=1 ./stream_c.exe -b --ntimes 100 --array-size 400M --malloc\nCopy:        0->0            205827.0     0.031445     0.031094     0.031984\nScale:       0->0            208171.8     0.031320     0.030744     0.032505\nAdd:         0->0            217352.0     0.045087     0.044168     0.046515\nTriad:       0->0            216884.8     0.045062     0.044263     0.046982\n\n=====================================================================\nPerformance tests - XSBench\nFrom - Hyeongtak Ji <hyeongtak.ji@sk.com>\n\nHardware: Single socket, Single CXL memory Expander\n\nNUMA node 0: 56 logical cores, 128 GB memory\nNUMA node 2: 96 GB CXL memory\nThreads:     56\nLookups:     170,000,000\n\nSummary: +19% over DRAM. +47% over default interleave.\n\nPerformance tests - XSBench\n1. dram only\n$ numactl -m 0 ./XSBench -s XL –p 5000000\nRuntime:     36.235 seconds\nLookups/s:   4,691,618\n\n2. default interleave\n$ numactl –i 0,2 ./XSBench –s XL –p 5000000\nRuntime:     55.243 seconds\nLookups/s:   3,077,293\n\n3. weighted interleave\nnumactl –w –i 0,2 ./XSBench –s XL –p 5000000\nRuntime:     29.262 seconds\nLookups/s:   5,809,513\n\n=====================================================================\nLTP Tests: https://github.com/gmprice/ltp/tree/mempolicy2\n\n= Existing tests\nset_mempolicy, get_mempolicy, mbind\n\nMPOL_WEIGHTED_INTERLEAVE added manually to test basic functionality but\ndid not adjust tests for weighting.  Basically the weights were set to 1,\nwhich is the default, and it should behave the same as MPOL_INTERLEAVE if\nlogic is correct.\n\n== set_mempolicy01 : passed   18, failed   0\n== set_mempolicy02 : passed   10, failed   0\n== set_mempolicy03 : passed   64, failed   0\n== set_mempolicy04 : passed   32, failed   0\n== set_mempolicy05 - n/a on non-x86\n== set_mempolicy06 : passed   10, failed   0\n   this is set_mempolicy02 + MPOL_WEIGHTED_INTERLEAVE\n== set_mempolicy07 : passed   32, failed   0\n   set_mempolicy04 + MPOL_WEIGHTED_INTERLEAVE\n== get_mempolicy01 : passed   12, failed   0\n   change: added MPOL_WEIGHTED_INTERLEAVE\n== get_mempolicy02 : passed   2, failed   0\n== mbind01 : passed   15, failed   0\n   added MPOL_WEIGHTED_INTERLEAVE\n== mbind02 : passed   4, failed   0\n   added MPOL_WEIGHTED_INTERLEAVE\n== mbind03 : passed   16, failed   0\n   added MPOL_WEIGHTED_INTERLEAVE\n== mbind04 : passed   48, failed   0\n   added MPOL_WEIGHTED_INTERLEAVE\n\n=====================================================================\nnumactl (set_mempolicy) w/ global weighting test\nnumactl fork: https://github.com/gmprice/numactl/tree/weighted_interleave_master\n\ncommand: numactl -w --interleave=0,1 ./eatmem\n\nresult (weights 1:1):\n0176a000 weighted interleave:0-1 heap anon=65793 dirty=65793 active=0 N0=32897 N1=32896 kernelpagesize_kB=4\n7fceeb9ff000 weighted interleave:0-1 anon=65537 dirty=65537 active=0 N0=32768 N1=32769 kernelpagesize_kB=4\n50% distribution is correct\n\nresult (weights 5:1):\n01b14000 weighted interleave:0-1 heap anon=65793 dirty=65793 active=0 N0=54828 N1=10965 kernelpagesize_kB=4\n7f47a1dff000 weighted interleave:0-1 anon=65537 dirty=65537 active=0 N0=54614 N1=10923 kernelpagesize_kB=4\n16.666% distribution is correct\n\nresult (weights 1:5):\n01f07000 weighted interleave:0-1 heap anon=65793 dirty=65793 active=0 N0=10966 N1=54827 kernelpagesize_kB=4\n7f17b1dff000 weighted interleave:0-1 anon=65537 dirty=65537 active=0 N0=10923 N1=54614 kernelpagesize_kB=4\n16.666% distribution is correct\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\nint main (void)\n{\n        char* mem = malloc(1024*1024*256);\n        memset(mem, 1, 1024*1024*256);\n        for (int i = 0; i  < ((1024*1024*256)/4096); i++)\n        {\n                mem = malloc(4096);\n                mem[0] = 1;\n        }\n        printf(\"done\\n\");\n        getchar();\n        return 0;\n}\n\n\nThis patch (of 4):\n\nThis patch provides a way to set interleave weight information under sysfs\nat /sys/kernel/mm/mempolicy/weighted_interleave/nodeN\n\nThe sysfs structure is designed as follows.\n\n  $ tree /sys/kernel/mm/mempolicy/\n  /sys/kernel/mm/mempolicy/ [1]\n  └── weighted_interleave [2]\n      ├── node0 [3]\n      └── node1\n\nEach file above can be explained as follows.\n\n[1] mm/mempolicy: configuration interface for mempolicy subsystem\n\n[2] weighted_interleave/: config interface for weighted interleave policy\n\n[3] weighted_interleave/nodeN: weight for nodeN\n\nIf a node value is set to `0`, the system-default value will be used.\nAs of this patch, the system-default for all nodes is always 1.\n\nLink: https://lkml.kernel.org/r/20240202170238.90004-1-gregory.price@memverge.com\nLink: https://lkml.kernel.org/r/20240202170238.90004-2-gregory.price@memverge.com\nSuggested-by: \"Huang, Ying\" <ying.huang@intel.com>\nSigned-off-by: Rakie Kim <rakie.kim@sk.com>\nSigned-off-by: Honggyu Kim <honggyu.kim@sk.com>\nCo-developed-by: Gregory Price <gregory.price@memverge.com>\nSigned-off-by: Gregory Price <gregory.price@memverge.com>\nCo-developed-by: Hyeongtak Ji <hyeongtak.ji@sk.com>\nSigned-off-by: Hyeongtak Ji <hyeongtak.ji@sk.com>\nReviewed-by: \"Huang, Ying\" <ying.huang@intel.com>\nCc: Dan Williams <dan.j.williams@intel.com>\nCc: Gregory Price <gourry.memverge@gmail.com>\nCc: Hasan Al Maruf <Hasan.Maruf@amd.com>\nCc: Johannes Weiner <hannes@cmpxchg.org>\nCc: Jonathan Corbet <corbet@lwn.net>\nCc: Michal Hocko <mhocko@kernel.org>\nCc: Srinivasulu Thanneeru <sthanneeru.opensrc@micron.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\n"
        }
    ],
    "proper_nouns": [
        {
            "MPOL_WEIGHTED_INTERLEAVE": "MPOL_WEIGHTED_INTERLEAVE is a new memory policy introduced in the Linux kernel to optimize memory allocation across NUMA (Non-Uniform Memory Access) nodes. Unlike the traditional MPOL_INTERLEAVE policy, which distributes memory evenly across all nodes, MPOL_WEIGHTED_INTERLEAVE allows for proportional distribution based on specified weights, thereby optimizing for bandwidth availability."
        },
        {
            "NUMA (Non-Uniform Memory Access)": "NUMA is a memory architecture where the memory access time depends on the memory location relative to the processor. In a NUMA system, the memory is divided into multiple nodes, and each processor has faster access to memory in its local node compared to memory in other nodes."
        },
        {
            "sysfs": "sysfs is a virtual filesystem provided by the Linux kernel that exports information about various kernel subsystems, hardware devices, and associated device drivers from the kernel's device model to user space. It allows for dynamic configuration and monitoring of kernel parameters."
        },
        {
            "CXL (Compute Express Link)": "CXL is an open industry standard interconnect that enables high-speed communication between a processor and devices like accelerators, memory buffers, and smart I/O devices. It is designed to improve performance and efficiency in data-intensive applications."
        }
    ],
    "background_knowledge": [
        "The traditional MPOL_INTERLEAVE policy distributes memory evenly across all NUMA nodes, which can lead to suboptimal performance in systems with heterogeneous memory types (e.g., DRAM and CXL-attached memory) due to different bandwidth characteristics.",
        "Weighted interleaving allows for a more balanced use of available bandwidth by distributing memory allocations according to the bandwidth capacity of each NUMA node, as specified by the weights."
    ],
    "kernel_changes": [
        "Before the introduction of MPOL_WEIGHTED_INTERLEAVE, the Linux kernel used a simple round-robin approach for memory allocation across NUMA nodes, which could lead to inefficient use of memory bandwidth in heterogeneous environments.",
        "With the new MPOL_WEIGHTED_INTERLEAVE policy, the kernel now supports weighted distribution of memory across NUMA nodes, allowing for better utilization of available bandwidth. This is achieved by assigning weights to each node, which determine the proportion of memory allocations that should be directed to that node.",
        "Additionally, a new sysfs interface has been implemented to allow users to set and adjust these weights dynamically, providing greater flexibility in memory policy management."
    ],
    "comprehensive_understanding": "The introduction of MPOL_WEIGHTED_INTERLEAVE and the associated sysfs extension represent a significant enhancement to the Linux kernel's memory management capabilities, particularly in environments with heterogeneous memory types. By allowing for weighted interleaving of memory allocations, the kernel can better optimize for bandwidth availability, leading to improved performance and reduced latency in bandwidth-intensive workloads. This feature is particularly beneficial in systems with CXL-attached memory, where the ability to distribute memory according to bandwidth capacity can lead to substantial performance gains. The new sysfs interface further enhances the usability of this feature by providing a straightforward way to configure and manage memory policy weights.",
    "concepts": [
        "MPOL_WEIGHTED_INTERLEAVE",
        "NUMA (Non-Uniform Memory Access)",
        "sysfs",
        "CXL (Compute Express Link)",
        "memory allocation",
        "bandwidth optimization",
        "latency reduction",
        "heterogeneous memory",
        "memory policy",
        "round-robin distribution",
        "weighted distribution",
        "sysfs interface",
        "memory bandwidth",
        "performance improvement",
        "bandwidth-intensive workloads"
    ]
}
2024-12-03 22:51:26,296 - extractor - INFO - ---------Processing feature 32/815---------
